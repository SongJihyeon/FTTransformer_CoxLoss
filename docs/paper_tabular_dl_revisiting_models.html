<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.0.0"/>
    <title>paper_tabular_dl_revisiting_models API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>


            <h2>Contents</h2>
            <ul>
  <li><a href="#span-stylecolorbrownwhat-to-expect-from-this-packagespan"><span style="color:brown">What to expect from this package</span></a></li>
  <li><a href="#how-to-tune-hyperparameters">How to tune hyperparameters</a></li>
  <li><a href="#api">API</a></li>
</ul>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#MLP">MLP</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MLP.__init__">MLP</a>
                        </li>
                        <li>
                                <a class="variable" href="#MLP.blocks">blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#MLP.output">output</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResNet">ResNet</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResNet.__init__">ResNet</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNet.input_projection">input_projection</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNet.blocks">blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#ResNet.output">output</a>
                        </li>
                        <li>
                                <a class="function" href="#ResNet.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#LinearEmbeddings">LinearEmbeddings</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LinearEmbeddings.__init__">LinearEmbeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#LinearEmbeddings.weight">weight</a>
                        </li>
                        <li>
                                <a class="variable" href="#LinearEmbeddings.bias">bias</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearEmbeddings.reset_parameters">reset_parameters</a>
                        </li>
                        <li>
                                <a class="variable" href="#LinearEmbeddings.n_features">n_features</a>
                        </li>
                        <li>
                                <a class="variable" href="#LinearEmbeddings.d_embedding">d_embedding</a>
                        </li>
                        <li>
                                <a class="function" href="#LinearEmbeddings.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#CategoricalFeatureEmbeddings">CategoricalFeatureEmbeddings</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CategoricalFeatureEmbeddings.__init__">CategoricalFeatureEmbeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#CategoricalFeatureEmbeddings.embeddings">embeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#CategoricalFeatureEmbeddings.bias">bias</a>
                        </li>
                        <li>
                                <a class="function" href="#CategoricalFeatureEmbeddings.reset_parameters">reset_parameters</a>
                        </li>
                        <li>
                                <a class="variable" href="#CategoricalFeatureEmbeddings.n_features">n_features</a>
                        </li>
                        <li>
                                <a class="variable" href="#CategoricalFeatureEmbeddings.d_embedding">d_embedding</a>
                        </li>
                        <li>
                                <a class="function" href="#CategoricalFeatureEmbeddings.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#CLSEmbedding">CLSEmbedding</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CLSEmbedding.__init__">CLSEmbedding</a>
                        </li>
                        <li>
                                <a class="variable" href="#CLSEmbedding.weight">weight</a>
                        </li>
                        <li>
                                <a class="variable" href="#CLSEmbedding.d_embedding">d_embedding</a>
                        </li>
                        <li>
                                <a class="function" href="#CLSEmbedding.reset_parameters">reset_parameters</a>
                        </li>
                        <li>
                                <a class="function" href="#CLSEmbedding.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MultiheadAttention">MultiheadAttention</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MultiheadAttention.__init__">MultiheadAttention</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiheadAttention.W_q">W_q</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiheadAttention.W_k">W_k</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiheadAttention.W_v">W_v</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiheadAttention.W_out">W_out</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiheadAttention.dropout">dropout</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiheadAttention.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#FTTransformerBackbone">FTTransformerBackbone</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#FTTransformerBackbone.__init__">FTTransformerBackbone</a>
                        </li>
                        <li>
                                <a class="variable" href="#FTTransformerBackbone.cls_embedding">cls_embedding</a>
                        </li>
                        <li>
                                <a class="variable" href="#FTTransformerBackbone.blocks">blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#FTTransformerBackbone.output">output</a>
                        </li>
                        <li>
                                <a class="function" href="#FTTransformerBackbone.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#FTTransformer">FTTransformer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#FTTransformer.__init__">FTTransformer</a>
                        </li>
                        <li>
                                <a class="variable" href="#FTTransformer.cont_embeddings">cont_embeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#FTTransformer.cat_embeddings">cat_embeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#FTTransformer.backbone">backbone</a>
                        </li>
                        <li>
                                <a class="function" href="#FTTransformer.get_default_kwargs">get_default_kwargs</a>
                        </li>
                        <li>
                                <a class="function" href="#FTTransformer.make_parameter_groups">make_parameter_groups</a>
                        </li>
                        <li>
                                <a class="function" href="#FTTransformer.make_default_optimizer">make_default_optimizer</a>
                        </li>
                        <li>
                                <a class="function" href="#FTTransformer.forward">forward</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
paper_tabular_dl_revisiting_models    </h1>

                        <div class="docstring"><p>The official package &amp; illustrations for the NeurIPS 2021 paper "Revisiting Deep Learning Models for Tabular Data".</p>

<ul>
<li><code>pip install paper_tabular_dl_revisiting_models</code></li>
<li>Paper: <a href="https://arxiv.org/abs/2106.11959">arXiv</a></li>
<li>Code: <a href="https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/package">GitHub</a></li>
<li>Example (with a Colab link inside):
<a href="https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/package/example.ipynb">GitHub</a></li>
</ul>

<h1 id="span-stylecolorbrownwhat-to-expect-from-this-packagespan"><span style="color:brown">What to expect from this package</span></h1>

<p>This package provides a minimal and accessible (re)implementation and illustrations of
the main things described and used in the paper. In particular:</p>

<ul>
<li>This is NOT the code used to obtain the results in the paper.
To reproduce the paper, see the instructions in the root of the repository.</li>
<li>The package aims to follow the paper code, even if the latter is suboptimal.
Any divergence from the paper code is considered to be a bug.</li>
<li>We are committed to fix bugs.</li>
<li>Feel free to copy any part of the package source code and adjust it for your needs
(please, keep the license header and/or add a link to this package).</li>
<li>Adding new features and supporting other use cases is out of scope for this package.
You can submit a feature request if you think that the change will be truly tiny
and non-intrusive. But overall, see the previous point.</li>
</ul>

<h1 id="how-to-tune-hyperparameters">How to tune hyperparameters</h1>

<ul>
<li>In the paper, for hyperparameter tuning, we used the
<a href="https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html">TPE sampler from Optuna</a>
as can be seen in <a href="https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/bin/tune.py">bin/tune.py</a>.</li>
<li>The hyperparamer tuning spaces can be found:
<ul>
<li>in the <code>output/</code> directory in the main repository (<a href="https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/output/california_housing/mlp/tuning/0.toml">example for MLP on the California Housing dataset</a>)</li>
<li>in the appendix of the paper</li>
</ul></li>
<li>For <code><a href="#FTTransformer">FTTransformer</a></code>, there is also a default configuration for a quick start.</li>
</ul>

<h1 id="api">API</h1>

<div class="pdoc-alert pdoc-alert-note">

<p>Use the "View Source" buttons on the right to see the
implementation of the module and individual items.</p>

</div>
</div>

                        <input id="mod-paper_tabular_dl_revisiting_models-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-paper_tabular_dl_revisiting_models-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">   1</span></a><span class="c1"># Copyright (c) 2023 Authors of &quot;Revisiting Deep Learning Models for Tabular Data&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">   2</span></a>
</span><span id="L-3"><a href="#L-3"><span class="linenos">   3</span></a><span class="c1"># Permission is hereby granted, free of charge, to any person obtaining</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">   4</span></a><span class="c1"># a copy of this software and associated documentation files (the</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">   5</span></a><span class="c1"># &quot;Software&quot;), to deal in the Software without restriction, including</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">   6</span></a><span class="c1"># without limitation the rights to use, copy, modify, merge, publish,</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">   7</span></a><span class="c1"># distribute, sublicense, and/or sell copies of the Software, and to</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">   8</span></a><span class="c1"># permit persons to whom the Software is furnished to do so, subject to</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">   9</span></a><span class="c1"># the following conditions:</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">  10</span></a>
</span><span id="L-11"><a href="#L-11"><span class="linenos">  11</span></a><span class="c1"># The above copyright notice and this permission notice shall be</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">  12</span></a><span class="c1"># included in all copies or substantial portions of the Software.</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">  13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos">  14</span></a><span class="c1"># THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">  15</span></a><span class="c1"># EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">  16</span></a><span class="c1"># MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">  17</span></a><span class="c1"># NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">  18</span></a><span class="c1"># LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">  19</span></a><span class="c1"># OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">  20</span></a><span class="c1"># WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">  21</span></a>
</span><span id="L-22"><a href="#L-22"><span class="linenos">  22</span></a><span class="sd">&quot;&quot;&quot;The official package &amp; illustrations for the NeurIPS 2021 paper &quot;Revisiting Deep Learning Models for Tabular Data&quot;.</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">  23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos">  24</span></a><span class="sd">- `pip install paper_tabular_dl_revisiting_models`</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">  25</span></a><span class="sd">- Paper: [arXiv](https://arxiv.org/abs/2106.11959)</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">  26</span></a><span class="sd">- Code: [GitHub](https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/package)</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos">  27</span></a><span class="sd">- Example (with a Colab link inside):</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">  28</span></a><span class="sd">  [GitHub](https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/package/example.ipynb)</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">  29</span></a>
</span><span id="L-30"><a href="#L-30"><span class="linenos">  30</span></a><span class="sd"># &lt;span style=&quot;color:brown&quot;&gt;What to expect from this package&lt;/span&gt;</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">  31</span></a>
</span><span id="L-32"><a href="#L-32"><span class="linenos">  32</span></a><span class="sd">This package provides a minimal and accessible (re)implementation and illustrations of</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos">  33</span></a><span class="sd">the main things described and used in the paper. In particular:</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">  34</span></a><span class="sd">- This is NOT the code used to obtain the results in the paper.</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos">  35</span></a><span class="sd">  To reproduce the paper, see the instructions in the root of the repository.</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos">  36</span></a><span class="sd">- The package aims to follow the paper code, even if the latter is suboptimal.</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">  37</span></a><span class="sd">  Any divergence from the paper code is considered to be a bug.</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">  38</span></a><span class="sd">- We are committed to fix bugs.</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos">  39</span></a><span class="sd">- Feel free to copy any part of the package source code and adjust it for your needs</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">  40</span></a><span class="sd">  (please, keep the license header and/or add a link to this package).</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">  41</span></a><span class="sd">- Adding new features and supporting other use cases is out of scope for this package.</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">  42</span></a><span class="sd">  You can submit a feature request if you think that the change will be truly tiny</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">  43</span></a><span class="sd">  and non-intrusive. But overall, see the previous point.</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos">  44</span></a>
</span><span id="L-45"><a href="#L-45"><span class="linenos">  45</span></a><span class="sd"># How to tune hyperparameters</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">  46</span></a>
</span><span id="L-47"><a href="#L-47"><span class="linenos">  47</span></a><span class="sd">- In the paper, for hyperparameter tuning, we used the</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">  48</span></a><span class="sd">  [TPE sampler from Optuna](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html)</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos">  49</span></a><span class="sd">  as can be seen in [bin/tune.py](https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/bin/tune.py).</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos">  50</span></a><span class="sd">- The hyperparamer tuning spaces can be found:</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos">  51</span></a><span class="sd">    - in the `output/` directory in the main repository ([example for MLP on the California Housing dataset](https://github.com/yandex-research/tabular-dl-revisiting-models/blob/main/output/california_housing/mlp/tuning/0.toml))</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">  52</span></a><span class="sd">    - in the appendix of the paper</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">  53</span></a><span class="sd">- For `FTTransformer`, there is also a default configuration for a quick start.</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">  54</span></a>
</span><span id="L-55"><a href="#L-55"><span class="linenos">  55</span></a><span class="sd"># API</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">  56</span></a>
</span><span id="L-57"><a href="#L-57"><span class="linenos">  57</span></a><span class="sd">.. note::</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">  58</span></a><span class="sd">    Use the &quot;View Source&quot; buttons on the right to see the</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">  59</span></a><span class="sd">    implementation of the module and individual items.</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos">  60</span></a>
</span><span id="L-61"><a href="#L-61"><span class="linenos">  61</span></a><span class="sd">&quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos">  62</span></a><span class="n">__version__</span> <span class="o">=</span> <span class="s1">&#39;0.0.2&#39;</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos">  63</span></a>
</span><span id="L-64"><a href="#L-64"><span class="linenos">  64</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos">  65</span></a>    <span class="s1">&#39;MLP&#39;</span><span class="p">,</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos">  66</span></a>    <span class="s1">&#39;ResNet&#39;</span><span class="p">,</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos">  67</span></a>    <span class="s1">&#39;LinearEmbeddings&#39;</span><span class="p">,</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos">  68</span></a>    <span class="s1">&#39;CategoricalFeatureEmbeddings&#39;</span><span class="p">,</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos">  69</span></a>    <span class="s1">&#39;CLSEmbedding&#39;</span><span class="p">,</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos">  70</span></a>    <span class="s1">&#39;MultiheadAttention&#39;</span><span class="p">,</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos">  71</span></a>    <span class="s1">&#39;FTTransformerBackbone&#39;</span><span class="p">,</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos">  72</span></a>    <span class="s1">&#39;FTTransformer&#39;</span><span class="p">,</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos">  73</span></a><span class="p">]</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos">  74</span></a>
</span><span id="L-75"><a href="#L-75"><span class="linenos">  75</span></a>
</span><span id="L-76"><a href="#L-76"><span class="linenos">  76</span></a><span class="kn">import</span> <span class="nn">math</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos">  77</span></a><span class="kn">import</span> <span class="nn">typing</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos">  78</span></a><span class="kn">import</span> <span class="nn">warnings</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos">  79</span></a><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos">  80</span></a><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">cast</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos">  81</span></a>
</span><span id="L-82"><a href="#L-82"><span class="linenos">  82</span></a><span class="kn">import</span> <span class="nn">torch</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos">  83</span></a><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos">  84</span></a><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos">  85</span></a><span class="kn">import</span> <span class="nn">torch.optim</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos">  86</span></a><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos">  87</span></a><span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos">  88</span></a>
</span><span id="L-89"><a href="#L-89"><span class="linenos">  89</span></a><span class="n">_INTERNAL_ERROR_MESSAGE</span> <span class="o">=</span> <span class="s1">&#39;Internal error&#39;</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos">  90</span></a>
</span><span id="L-91"><a href="#L-91"><span class="linenos">  91</span></a>
</span><span id="L-92"><a href="#L-92"><span class="linenos">  92</span></a><span class="k">def</span> <span class="nf">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos">  93</span></a>    <span class="c1"># This is the initialization used in `torch.nn.Linear`.</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos">  94</span></a>    <span class="n">d_rsqrt</span> <span class="o">=</span> <span class="n">d</span><span class="o">**-</span><span class="mf">0.5</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos">  95</span></a>    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">d_rsqrt</span><span class="p">,</span> <span class="n">d_rsqrt</span><span class="p">)</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos">  96</span></a>
</span><span id="L-97"><a href="#L-97"><span class="linenos">  97</span></a>
</span><span id="L-98"><a href="#L-98"><span class="linenos">  98</span></a><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos">  99</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The MLP model from Section 3.1 in the paper.</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos"> 100</span></a>
</span><span id="L-101"><a href="#L-101"><span class="linenos"> 101</span></a><span class="sd">    ```</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos"> 102</span></a><span class="sd">    MLP:   (in) -&gt; Block  -&gt; ...  -&gt; Block   -&gt; (out)</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos"> 103</span></a><span class="sd">    Block: (in) -&gt; Linear -&gt; ReLU -&gt; Dropout -&gt; (out)</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos"> 104</span></a><span class="sd">    ```</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos"> 105</span></a>
</span><span id="L-106"><a href="#L-106"><span class="linenos"> 106</span></a><span class="sd">    **Shape**</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos"> 107</span></a>
</span><span id="L-108"><a href="#L-108"><span class="linenos"> 108</span></a><span class="sd">    - Input: `(*, d_in)`</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos"> 109</span></a><span class="sd">    - Output: `(*, d_out or d_block)`</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos"> 110</span></a>
</span><span id="L-111"><a href="#L-111"><span class="linenos"> 111</span></a><span class="sd">    **Examples**</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos"> 112</span></a>
</span><span id="L-113"><a href="#L-113"><span class="linenos"> 113</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos"> 114</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, 3)</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos"> 115</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos"> 116</span></a><span class="sd">    &gt;&gt;&gt; m = MLP(</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos"> 117</span></a><span class="sd">    ...    d_in=x.shape[1],</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos"> 118</span></a><span class="sd">    ...    d_out=d_out,</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos"> 119</span></a><span class="sd">    ...    n_blocks=4,</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos"> 120</span></a><span class="sd">    ...    d_block=5,</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos"> 121</span></a><span class="sd">    ...    dropout=0.1,</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos"> 122</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos"> 123</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, d_out)</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos"> 124</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos"> 125</span></a>
</span><span id="L-126"><a href="#L-126"><span class="linenos"> 126</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos"> 127</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos"> 128</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos"> 129</span></a>        <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos"> 130</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos"> 131</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos"> 132</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos"> 133</span></a>        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos"> 134</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos"> 135</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos"> 136</span></a><span class="sd">        Args:</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos"> 137</span></a><span class="sd">            d_in: the input size.</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos"> 138</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos"> 139</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos"> 140</span></a><span class="sd">            d_block: the block width.</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos"> 141</span></a><span class="sd">            dropout: the dropout rate.</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos"> 142</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos"> 143</span></a>        <span class="k">assert</span> <span class="n">n_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos"> 144</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos"> 145</span></a>
</span><span id="L-146"><a href="#L-146"><span class="linenos"> 146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos"> 147</span></a>            <span class="p">[</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos"> 148</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos"> 149</span></a>                    <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos"> 150</span></a>                        <span class="p">[</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos"> 151</span></a>                            <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos"> 152</span></a>                            <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos"> 153</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)),</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos"> 154</span></a>                        <span class="p">]</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos"> 155</span></a>                    <span class="p">)</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos"> 156</span></a>                <span class="p">)</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos"> 157</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos"> 158</span></a>            <span class="p">]</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos"> 159</span></a>        <span class="p">)</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos"> 160</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos"> 161</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos"> 162</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos"> 163</span></a>
</span><span id="L-164"><a href="#L-164"><span class="linenos"> 164</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos"> 165</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos"> 166</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos"> 167</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos"> 168</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos"> 169</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos"> 170</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos"> 171</span></a>
</span><span id="L-172"><a href="#L-172"><span class="linenos"> 172</span></a>
</span><span id="L-173"><a href="#L-173"><span class="linenos"> 173</span></a><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos"> 174</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The ResNet model from Section 3.2 in the paper.</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos"> 175</span></a>
</span><span id="L-176"><a href="#L-176"><span class="linenos"> 176</span></a><span class="sd">    ```</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos"> 177</span></a><span class="sd">    ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Output -&gt; (out)</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos"> 178</span></a>
</span><span id="L-179"><a href="#L-179"><span class="linenos"> 179</span></a><span class="sd">             |-&gt; BatchNorm -&gt; Linear -&gt; ReLU -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt; |</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos"> 180</span></a><span class="sd">             |                                                                  |</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos"> 181</span></a><span class="sd">    Block:  (in) ------------------------------------------------------------&gt; Add -&gt; (out)</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos"> 182</span></a>
</span><span id="L-183"><a href="#L-183"><span class="linenos"> 183</span></a><span class="sd">    Output: (in) -&gt; BatchNorm -&gt; ReLU -&gt; Linear -&gt; (out)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos"> 184</span></a><span class="sd">    ```</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos"> 185</span></a>
</span><span id="L-186"><a href="#L-186"><span class="linenos"> 186</span></a><span class="sd">    **Shape**</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos"> 187</span></a>
</span><span id="L-188"><a href="#L-188"><span class="linenos"> 188</span></a><span class="sd">    - Input: `(*, d_in)`</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos"> 189</span></a><span class="sd">    - Output: `(*, d_out or d_block)`</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos"> 190</span></a>
</span><span id="L-191"><a href="#L-191"><span class="linenos"> 191</span></a><span class="sd">    **Examples**</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos"> 192</span></a>
</span><span id="L-193"><a href="#L-193"><span class="linenos"> 193</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos"> 194</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, 2)</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos"> 195</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos"> 196</span></a><span class="sd">    &gt;&gt;&gt; m = ResNet(</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos"> 197</span></a><span class="sd">    ...     d_in=x.shape[1],</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos"> 198</span></a><span class="sd">    ...     d_out=d_out,</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos"> 199</span></a><span class="sd">    ...     n_blocks=2,</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos"> 200</span></a><span class="sd">    ...     d_block=3,</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos"> 201</span></a><span class="sd">    ...     d_hidden=4,</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos"> 202</span></a><span class="sd">    ...     d_hidden_multiplier=None,</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos"> 203</span></a><span class="sd">    ...     dropout1=0.25,</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos"> 204</span></a><span class="sd">    ...     dropout2=0.0,</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos"> 205</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos"> 206</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, d_out)</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos"> 207</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos"> 208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos"> 209</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos"> 210</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos"> 211</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos"> 212</span></a>        <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos"> 213</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos"> 214</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos"> 215</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos"> 216</span></a>        <span class="n">d_hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos"> 217</span></a>        <span class="n">d_hidden_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos"> 218</span></a>        <span class="n">dropout1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos"> 219</span></a>        <span class="n">dropout2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos"> 220</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos"> 221</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos"> 222</span></a><span class="sd">        Args:</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos"> 223</span></a><span class="sd">            d_in: the input size.</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos"> 224</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos"> 225</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos"> 226</span></a><span class="sd">            d_block: the &quot;main&quot; block width (i.e. its input and output size).</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos"> 227</span></a><span class="sd">            d_hidden: the block&#39;s hidden width.</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos"> 228</span></a><span class="sd">            d_hidden_multipler: the alternative way to set `d_hidden` as</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos"> 229</span></a><span class="sd">                `int(d_block * d_hidden_multipler)`.</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos"> 230</span></a><span class="sd">            dropout1: the hidden dropout rate.</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos"> 231</span></a><span class="sd">            dropout2: the residual dropout rate.</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos"> 232</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos"> 233</span></a>        <span class="k">assert</span> <span class="n">n_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos"> 234</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">d_hidden_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos"> 235</span></a>        <span class="k">if</span> <span class="n">d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos"> 236</span></a>            <span class="n">d_hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_block</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">d_hidden_multiplier</span><span class="p">))</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos"> 237</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos"> 238</span></a>
</span><span id="L-239"><a href="#L-239"><span class="linenos"> 239</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos"> 240</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The first linear layer (applied before the main blocks) which</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos"> 241</span></a><span class="sd">        projects the input from `d_in` to `d_block`.&quot;&quot;&quot;</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos"> 242</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos"> 243</span></a>            <span class="p">[</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos"> 244</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos"> 245</span></a>                    <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos"> 246</span></a>                        <span class="p">[</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos"> 247</span></a>                            <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos"> 248</span></a>                            <span class="p">(</span><span class="s1">&#39;linear1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">)),</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos"> 249</span></a>                            <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos"> 250</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout1</span><span class="p">)),</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos"> 251</span></a>                            <span class="p">(</span><span class="s1">&#39;linear2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos"> 252</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout2</span><span class="p">)),</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos"> 253</span></a>                        <span class="p">]</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos"> 254</span></a>                    <span class="p">)</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos"> 255</span></a>                <span class="p">)</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos"> 256</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos"> 257</span></a>            <span class="p">]</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos"> 258</span></a>        <span class="p">)</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos"> 259</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos"> 260</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos"> 261</span></a>            <span class="kc">None</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos"> 262</span></a>            <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos"> 263</span></a>            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos"> 264</span></a>                <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos"> 265</span></a>                    <span class="p">[</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos"> 266</span></a>                        <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos"> 267</span></a>                        <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos"> 268</span></a>                        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)),</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos"> 269</span></a>                    <span class="p">]</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos"> 270</span></a>                <span class="p">)</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos"> 271</span></a>            <span class="p">)</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos"> 272</span></a>        <span class="p">)</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos"> 273</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos"> 274</span></a>
</span><span id="L-275"><a href="#L-275"><span class="linenos"> 275</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos"> 276</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos"> 277</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos"> 278</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos"> 279</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos"> 280</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos"> 281</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos"> 282</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos"> 283</span></a>
</span><span id="L-284"><a href="#L-284"><span class="linenos"> 284</span></a>
</span><span id="L-285"><a href="#L-285"><span class="linenos"> 285</span></a><span class="k">class</span> <span class="nc">CLSEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos"> 286</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The [CLS]-token embedding for the Transformer backbone.</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos"> 287</span></a>
</span><span id="L-288"><a href="#L-288"><span class="linenos"> 288</span></a><span class="sd">    The module prepends the same trainable token embedding to</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos"> 289</span></a><span class="sd">    all objects in the batch.</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos"> 290</span></a>
</span><span id="L-291"><a href="#L-291"><span class="linenos"> 291</span></a><span class="sd">    **Shape**</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos"> 292</span></a>
</span><span id="L-293"><a href="#L-293"><span class="linenos"> 293</span></a><span class="sd">    - Input: `(batch_size, n_tokens, d_embedding)`</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos"> 294</span></a><span class="sd">    - Output: `(batch_size, 1 + n_tokens, d_embedding)`</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos"> 295</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos"> 296</span></a>
</span><span id="L-297"><a href="#L-297"><span class="linenos"> 297</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos"> 298</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos"> 299</span></a><span class="sd">        Args:</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos"> 300</span></a><span class="sd">            d_embedding: the size of one token embedding</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos"> 301</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos"> 302</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos"> 303</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">))</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos"> 304</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos"> 305</span></a>
</span><span id="L-306"><a href="#L-306"><span class="linenos"> 306</span></a>    <span class="nd">@property</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos"> 307</span></a>    <span class="k">def</span> <span class="nf">d_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos"> 308</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embedding size.&quot;&quot;&quot;</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos"> 309</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos"> 310</span></a>
</span><span id="L-311"><a href="#L-311"><span class="linenos"> 311</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos"> 312</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos"> 313</span></a>        <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos"> 314</span></a>
</span><span id="L-315"><a href="#L-315"><span class="linenos"> 315</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos"> 316</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos"> 317</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos"> 318</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos"> 319</span></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos"> 320</span></a>
</span><span id="L-321"><a href="#L-321"><span class="linenos"> 321</span></a>
</span><span id="L-322"><a href="#L-322"><span class="linenos"> 322</span></a><span class="k">class</span> <span class="nc">LinearEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos"> 323</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear embeddings for continuous features.</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos"> 324</span></a>
</span><span id="L-325"><a href="#L-325"><span class="linenos"> 325</span></a><span class="sd">    For the illustration, see `FTTransformer`.</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos"> 326</span></a>
</span><span id="L-327"><a href="#L-327"><span class="linenos"> 327</span></a><span class="sd">    **Shape**</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos"> 328</span></a>
</span><span id="L-329"><a href="#L-329"><span class="linenos"> 329</span></a><span class="sd">    - Input: `(*, n_features)`</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos"> 330</span></a><span class="sd">    - Output: `(*, n_features, d_embedding)`</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos"> 331</span></a>
</span><span id="L-332"><a href="#L-332"><span class="linenos"> 332</span></a><span class="sd">    **Examples**</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos"> 333</span></a>
</span><span id="L-334"><a href="#L-334"><span class="linenos"> 334</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos"> 335</span></a><span class="sd">    &gt;&gt;&gt; n_cont_features = 3</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos"> 336</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, n_cont_features)</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos"> 337</span></a><span class="sd">    &gt;&gt;&gt; d_embedding = 4</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos"> 338</span></a><span class="sd">    &gt;&gt;&gt; m = LinearEmbeddings(n_cont_features, d_embedding)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos"> 339</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, n_cont_features, d_embedding)</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos"> 340</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos"> 341</span></a>
</span><span id="L-342"><a href="#L-342"><span class="linenos"> 342</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos"> 343</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos"> 344</span></a><span class="sd">        Args:</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos"> 345</span></a><span class="sd">            n_features: the number of continous features</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos"> 346</span></a><span class="sd">            d_embedding: the embedding size</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos"> 347</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos"> 348</span></a>        <span class="k">assert</span> <span class="n">n_features</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos"> 349</span></a>        <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos"> 350</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos"> 351</span></a>
</span><span id="L-352"><a href="#L-352"><span class="linenos"> 352</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos"> 353</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The weight.&quot;&quot;&quot;</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos"> 354</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos"> 355</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The bias.&quot;&quot;&quot;</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos"> 356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos"> 357</span></a>
</span><span id="L-358"><a href="#L-358"><span class="linenos"> 358</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos"> 359</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos"> 360</span></a>        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos"> 361</span></a>            <span class="k">if</span> <span class="n">parameter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos"> 362</span></a>                <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos"> 363</span></a>
</span><span id="L-364"><a href="#L-364"><span class="linenos"> 364</span></a>    <span class="nd">@property</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos"> 365</span></a>    <span class="k">def</span> <span class="nf">n_features</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos"> 366</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The number of features.&quot;&quot;&quot;</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos"> 367</span></a>        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos"> 368</span></a>
</span><span id="L-369"><a href="#L-369"><span class="linenos"> 369</span></a>    <span class="nd">@property</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos"> 370</span></a>    <span class="k">def</span> <span class="nf">d_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos"> 371</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embedding size.&quot;&quot;&quot;</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos"> 372</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos"> 373</span></a>
</span><span id="L-374"><a href="#L-374"><span class="linenos"> 374</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos"> 375</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos"> 376</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos"> 377</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos"> 378</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos"> 379</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos"> 380</span></a>
</span><span id="L-381"><a href="#L-381"><span class="linenos"> 381</span></a>
</span><span id="L-382"><a href="#L-382"><span class="linenos"> 382</span></a><span class="k">class</span> <span class="nc">CategoricalFeatureEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos"> 383</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Embeddings for categorical features.</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos"> 384</span></a>
</span><span id="L-385"><a href="#L-385"><span class="linenos"> 385</span></a><span class="sd">    For the illustration, see `FTTransformer`.</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos"> 386</span></a>
</span><span id="L-387"><a href="#L-387"><span class="linenos"> 387</span></a><span class="sd">    **Notes**</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos"> 388</span></a>
</span><span id="L-389"><a href="#L-389"><span class="linenos"> 389</span></a><span class="sd">    - A cardinality of a categorical feature is the number of distinct values</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos"> 390</span></a><span class="sd">      that the feature takes</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos"> 391</span></a><span class="sd">    - A categorical feature must be represented by `int64` from `range(0, cardinality)`</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos"> 392</span></a>
</span><span id="L-393"><a href="#L-393"><span class="linenos"> 393</span></a><span class="sd">    **Shape**</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos"> 394</span></a>
</span><span id="L-395"><a href="#L-395"><span class="linenos"> 395</span></a><span class="sd">    - Input: `(*, len(cardinalities))`</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos"> 396</span></a><span class="sd">    - Output: `(*, len(cardinalities), d_embedding)`</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos"> 397</span></a>
</span><span id="L-398"><a href="#L-398"><span class="linenos"> 398</span></a><span class="sd">    **Examples**</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos"> 399</span></a>
</span><span id="L-400"><a href="#L-400"><span class="linenos"> 400</span></a><span class="sd">    &gt;&gt;&gt; cardinalities = [3, 10]</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos"> 401</span></a><span class="sd">    &gt;&gt;&gt; x = torch.tensor([</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos"> 402</span></a><span class="sd">    ...     [0, 5],</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos"> 403</span></a><span class="sd">    ...     [1, 7],</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos"> 404</span></a><span class="sd">    ...     [0, 2],</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos"> 405</span></a><span class="sd">    ...     [2, 4]</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos"> 406</span></a><span class="sd">    ... ])</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos"> 407</span></a><span class="sd">    &gt;&gt;&gt; batch_size, n_cat_features = x.shape</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos"> 408</span></a><span class="sd">    &gt;&gt;&gt; d_embedding = 3</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos"> 409</span></a><span class="sd">    &gt;&gt;&gt; m = CategoricalFeatureEmbeddings(cardinalities, d_embedding, True)</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos"> 410</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, n_cat_features, d_embedding)</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos"> 411</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos"> 412</span></a>
</span><span id="L-413"><a href="#L-413"><span class="linenos"> 413</span></a>    <span class="n">_category_offsets</span><span class="p">:</span> <span class="n">Tensor</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos"> 414</span></a>
</span><span id="L-415"><a href="#L-415"><span class="linenos"> 415</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos"> 416</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos"> 417</span></a><span class="sd">        Args:</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos"> 418</span></a><span class="sd">            cardinalities: the number of distinct values for each feature. For example,</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos"> 419</span></a><span class="sd">                `cardinalities=[3, 4]` describes two features, where the first</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos"> 420</span></a><span class="sd">                takes values in the range `[0, 1, 2]` and the second one takes</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos"> 421</span></a><span class="sd">                values in the range `[0, 1, 2, 3]`.</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos"> 422</span></a><span class="sd">            d_embedding: the embedding size.</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos"> 423</span></a><span class="sd">            bias: if `True`, for each feature, a trainable vector is added to the</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos"> 424</span></a><span class="sd">                embedding regardless of a feature value. For each feature, a separate</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos"> 425</span></a><span class="sd">                non-shared bias vector is allocated. In the paper, we used `bias=True`.</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos"> 426</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos"> 427</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos"> 428</span></a>        <span class="k">assert</span> <span class="n">cardinalities</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos"> 429</span></a>        <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos"> 430</span></a>
</span><span id="L-431"><a href="#L-431"><span class="linenos"> 431</span></a>        <span class="n">category_offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">cardinalities</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos"> 432</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;_category_offsets&#39;</span><span class="p">,</span> <span class="n">category_offsets</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos"> 433</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">),</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos"> 434</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings.&quot;&quot;&quot;</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos"> 435</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos"> 436</span></a>            <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">),</span> <span class="n">d_embedding</span><span class="p">))</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos"> 437</span></a>        <span class="p">)</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos"> 438</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The bias.&quot;&quot;&quot;</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos"> 439</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos"> 440</span></a>
</span><span id="L-441"><a href="#L-441"><span class="linenos"> 441</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos"> 442</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos"> 443</span></a>        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos"> 444</span></a>            <span class="k">if</span> <span class="n">parameter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos"> 445</span></a>                <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos"> 446</span></a>
</span><span id="L-447"><a href="#L-447"><span class="linenos"> 447</span></a>    <span class="nd">@property</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos"> 448</span></a>    <span class="k">def</span> <span class="nf">n_features</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos"> 449</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The number of features.&quot;&quot;&quot;</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos"> 450</span></a>        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_category_offsets</span><span class="p">)</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos"> 451</span></a>
</span><span id="L-452"><a href="#L-452"><span class="linenos"> 452</span></a>    <span class="nd">@property</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos"> 453</span></a>    <span class="k">def</span> <span class="nf">d_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos"> 454</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embedding size.&quot;&quot;&quot;</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos"> 455</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">embedding_dim</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos"> 456</span></a>
</span><span id="L-457"><a href="#L-457"><span class="linenos"> 457</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos"> 458</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos"> 459</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
</span><span id="L-460"><a href="#L-460"><span class="linenos"> 460</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_category_offsets</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos"> 461</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos"> 462</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos"> 463</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos"> 464</span></a>
</span><span id="L-465"><a href="#L-465"><span class="linenos"> 465</span></a>
</span><span id="L-466"><a href="#L-466"><span class="linenos"> 466</span></a><span class="n">_KV_COMPRESSION_SHARING</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;headwise&#39;</span><span class="p">,</span> <span class="s1">&#39;key-value&#39;</span><span class="p">]</span>
</span><span id="L-467"><a href="#L-467"><span class="linenos"> 467</span></a>
</span><span id="L-468"><a href="#L-468"><span class="linenos"> 468</span></a>
</span><span id="L-469"><a href="#L-469"><span class="linenos"> 469</span></a><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos"> 470</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multihead Attention (self-/cross-) with an optional linear attention.</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos"> 471</span></a>
</span><span id="L-472"><a href="#L-472"><span class="linenos"> 472</span></a><span class="sd">    - To learn more about Multihead Attention, see</span>
</span><span id="L-473"><a href="#L-473"><span class="linenos"> 473</span></a><span class="sd">      [&quot;Attention Is All You Need&quot;](https://arxiv.org/abs/1706.03762)</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos"> 474</span></a><span class="sd">    - To learn about the linear attention supported by this module, see</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos"> 475</span></a><span class="sd">      [&quot;Linformer: Self-Attention with Linear Complexity&quot;](https://arxiv.org/abs/2006.04768),</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos"> 476</span></a>
</span><span id="L-477"><a href="#L-477"><span class="linenos"> 477</span></a><span class="sd">    **Shape**</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos"> 478</span></a>
</span><span id="L-479"><a href="#L-479"><span class="linenos"> 479</span></a><span class="sd">    - Input:</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos"> 480</span></a><span class="sd">        - `x_q  ~ (batch_size, n_q_tokes,  d_embedding)`</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos"> 481</span></a><span class="sd">        - `x_kv ~ (batch_size, n_kv_tokes, d_embedding)`</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos"> 482</span></a><span class="sd">    - Output: `(batch_size, n_q_tokes, d_embedding)`</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos"> 483</span></a>
</span><span id="L-484"><a href="#L-484"><span class="linenos"> 484</span></a><span class="sd">    **Examples**</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos"> 485</span></a>
</span><span id="L-486"><a href="#L-486"><span class="linenos"> 486</span></a><span class="sd">    &gt;&gt;&gt; batch_size, n_tokens, d_embedding = 2, 3, 16</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos"> 487</span></a><span class="sd">    &gt;&gt;&gt; n_heads = 8</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos"> 488</span></a><span class="sd">    &gt;&gt;&gt; a = torch.randn(batch_size, n_tokens, d_embedding)</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos"> 489</span></a><span class="sd">    &gt;&gt;&gt; b = torch.randn(batch_size, n_tokens * 2, d_embedding)</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos"> 490</span></a><span class="sd">    &gt;&gt;&gt; m = MultiheadAttention(</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos"> 491</span></a><span class="sd">    ...     d_embedding=d_embedding, n_heads=n_heads, dropout=0.2</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos"> 492</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="L-493"><a href="#L-493"><span class="linenos"> 493</span></a><span class="sd">    &gt;&gt;&gt;</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos"> 494</span></a><span class="sd">    &gt;&gt;&gt; # self-attention</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos"> 495</span></a><span class="sd">    &gt;&gt;&gt; assert m(a, a).shape == a.shape</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos"> 496</span></a><span class="sd">    &gt;&gt;&gt;</span>
</span><span id="L-497"><a href="#L-497"><span class="linenos"> 497</span></a><span class="sd">    &gt;&gt;&gt; # cross-attention</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos"> 498</span></a><span class="sd">    &gt;&gt;&gt; assert m(a, b).shape == a.shape</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos"> 499</span></a><span class="sd">    &gt;&gt;&gt;</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos"> 500</span></a><span class="sd">    &gt;&gt;&gt; # Linformer attention</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos"> 501</span></a><span class="sd">    &gt;&gt;&gt; m = MultiheadAttention(</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos"> 502</span></a><span class="sd">    ...     d_embedding=d_embedding,</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos"> 503</span></a><span class="sd">    ...     n_heads=n_heads,</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos"> 504</span></a><span class="sd">    ...     dropout=0.2,</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos"> 505</span></a><span class="sd">    ...     n_tokens=n_tokens,</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos"> 506</span></a><span class="sd">    ...     kv_compression_ratio=0.5,</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos"> 507</span></a><span class="sd">    ...     kv_compression_sharing=&#39;headwise&#39;,</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos"> 508</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos"> 509</span></a><span class="sd">    &gt;&gt;&gt; assert m(a, a).shape == a.shape</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos"> 510</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos"> 511</span></a>
</span><span id="L-512"><a href="#L-512"><span class="linenos"> 512</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos"> 513</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos"> 514</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos"> 515</span></a>        <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos"> 516</span></a>        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos"> 517</span></a>        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos"> 518</span></a>        <span class="c1"># Linformer arguments.</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos"> 519</span></a>        <span class="n">n_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos"> 520</span></a>        <span class="n">kv_compression_ratio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos"> 521</span></a>        <span class="n">kv_compression_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos"> 522</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos"> 523</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos"> 524</span></a><span class="sd">        Args:</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos"> 525</span></a><span class="sd">            d_embedding: the embedding size for one token.</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos"> 526</span></a><span class="sd">                Must be a multiple of `n_heads`.</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos"> 527</span></a><span class="sd">            n_heads: the number of heads. If greater than 1, then the module will have</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos"> 528</span></a><span class="sd">                an additional output layer (the so called &quot;mixing&quot; layer).</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos"> 529</span></a><span class="sd">            dropout: the dropout rate for the attention probability map.</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos"> 530</span></a><span class="sd">            n_tokens: the number of tokens</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos"> 531</span></a><span class="sd">                (must be provided if `kv_compression_ratio` is not None)</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos"> 532</span></a><span class="sd">            kv_compression_ratio: Linformer-style compression rate.</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos"> 533</span></a><span class="sd">                Must be within the interval `(0.0, 1.0)`.</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos"> 534</span></a><span class="sd">            kv_compression_sharing: Linformer compression sharing policy.</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos"> 535</span></a><span class="sd">                Must be provided if `kv_compression_ratio` is not None.</span>
</span><span id="L-536"><a href="#L-536"><span class="linenos"> 536</span></a><span class="sd">                (non-shared Linformer compression is not supported; the &quot;layerwise&quot;</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos"> 537</span></a><span class="sd">                sharing policy is not supported).</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos"> 538</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos"> 539</span></a>        <span class="k">if</span> <span class="n">n_heads</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos"> 540</span></a>            <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-541"><a href="#L-541"><span class="linenos"> 541</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos"> 542</span></a>
</span><span id="L-543"><a href="#L-543"><span class="linenos"> 543</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-544"><a href="#L-544"><span class="linenos"> 544</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The query projection layer.&quot;&quot;&quot;</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos"> 545</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos"> 546</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The key projection layer.&quot;&quot;&quot;</span>
</span><span id="L-547"><a href="#L-547"><span class="linenos"> 547</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos"> 548</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The value projection layer.&quot;&quot;&quot;</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos"> 549</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span> <span class="k">if</span> <span class="n">n_heads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos"> 550</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output mixing layer (presented if `n_heads &gt; 1`).&quot;&quot;&quot;</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos"> 551</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos"> 552</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span> <span class="k">if</span> <span class="n">dropout</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos"> 553</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The dropout for the attention probability map.&quot;&quot;&quot;</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos"> 554</span></a>
</span><span id="L-555"><a href="#L-555"><span class="linenos"> 555</span></a>        <span class="k">if</span> <span class="n">kv_compression_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos"> 556</span></a>            <span class="k">assert</span> <span class="n">n_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos"> 557</span></a>            <span class="k">assert</span> <span class="n">kv_compression_sharing</span> <span class="ow">in</span> <span class="n">typing</span><span class="o">.</span><span class="n">get_args</span><span class="p">(</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">)</span>
</span><span id="L-558"><a href="#L-558"><span class="linenos"> 558</span></a>            <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">kv_compression_ratio</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos"> 559</span></a>
</span><span id="L-560"><a href="#L-560"><span class="linenos"> 560</span></a>            <span class="k">def</span> <span class="nf">make_kv_compression</span><span class="p">():</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos"> 561</span></a>                <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos"> 562</span></a>                    <span class="n">n_tokens</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n_tokens</span> <span class="o">*</span> <span class="n">kv_compression_ratio</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos"> 563</span></a>                <span class="p">)</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos"> 564</span></a>
</span><span id="L-565"><a href="#L-565"><span class="linenos"> 565</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="o">=</span> <span class="n">make_kv_compression</span><span class="p">()</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos"> 566</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-567"><a href="#L-567"><span class="linenos"> 567</span></a>                <span class="n">make_kv_compression</span><span class="p">()</span> <span class="k">if</span> <span class="n">kv_compression_sharing</span> <span class="o">==</span> <span class="s1">&#39;headwise&#39;</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos"> 568</span></a>            <span class="p">)</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos"> 569</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos"> 570</span></a>            <span class="k">assert</span> <span class="n">n_tokens</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos"> 571</span></a>            <span class="k">assert</span> <span class="n">kv_compression_sharing</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos"> 572</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos"> 573</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos"> 574</span></a>
</span><span id="L-575"><a href="#L-575"><span class="linenos"> 575</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">]:</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos"> 576</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos"> 577</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos"> 578</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos"> 579</span></a>
</span><span id="L-580"><a href="#L-580"><span class="linenos"> 580</span></a>    <span class="k">def</span> <span class="nf">_reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos"> 581</span></a>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-582"><a href="#L-582"><span class="linenos"> 582</span></a>        <span class="n">d_head</span> <span class="o">=</span> <span class="n">d</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos"> 583</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="L-584"><a href="#L-584"><span class="linenos"> 584</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos"> 585</span></a>            <span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos"> 586</span></a>            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos"> 587</span></a>        <span class="p">)</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos"> 588</span></a>
</span><span id="L-589"><a href="#L-589"><span class="linenos"> 589</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_kv</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos"> 590</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos"> 591</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">x_q</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">x_kv</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">x_kv</span><span class="p">)</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos"> 592</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos"> 593</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos"> 594</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos"> 595</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span>
</span><span id="L-596"><a href="#L-596"><span class="linenos"> 596</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos"> 597</span></a>                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos"> 598</span></a>            <span class="p">)(</span><span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos"> 599</span></a>
</span><span id="L-600"><a href="#L-600"><span class="linenos"> 600</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos"> 601</span></a>        <span class="n">d_head_key</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos"> 602</span></a>        <span class="n">d_head_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos"> 603</span></a>        <span class="n">n_q_tokens</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos"> 604</span></a>
</span><span id="L-605"><a href="#L-605"><span class="linenos"> 605</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos"> 606</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos"> 607</span></a>        <span class="n">attention_logits</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_head_key</span><span class="p">)</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos"> 608</span></a>        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos"> 609</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos"> 610</span></a>            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos"> 611</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos"> 612</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos"> 613</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">n_q_tokens</span><span class="p">,</span> <span class="n">d_head_value</span><span class="p">)</span>
</span><span id="L-614"><a href="#L-614"><span class="linenos"> 614</span></a>            <span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos"> 615</span></a>            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_q_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span> <span class="o">*</span> <span class="n">d_head_value</span><span class="p">)</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos"> 616</span></a>        <span class="p">)</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos"> 617</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos"> 618</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-619"><a href="#L-619"><span class="linenos"> 619</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-620"><a href="#L-620"><span class="linenos"> 620</span></a>
</span><span id="L-621"><a href="#L-621"><span class="linenos"> 621</span></a>
</span><span id="L-622"><a href="#L-622"><span class="linenos"> 622</span></a><span class="k">class</span> <span class="nc">_ReGLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-623"><a href="#L-623"><span class="linenos"> 623</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos"> 624</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos"> 625</span></a>        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos"> 626</span></a>        <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos"> 627</span></a>
</span><span id="L-628"><a href="#L-628"><span class="linenos"> 628</span></a>
</span><span id="L-629"><a href="#L-629"><span class="linenos"> 629</span></a><span class="k">class</span> <span class="nc">FTTransformerBackbone</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos"> 630</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The backbone of FT-Transformer.</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos"> 631</span></a>
</span><span id="L-632"><a href="#L-632"><span class="linenos"> 632</span></a><span class="sd">    For the illustration, see `FTTransformer`.</span>
</span><span id="L-633"><a href="#L-633"><span class="linenos"> 633</span></a>
</span><span id="L-634"><a href="#L-634"><span class="linenos"> 634</span></a><span class="sd">    In fact, it is almost idential to Transformer from the paper</span>
</span><span id="L-635"><a href="#L-635"><span class="linenos"> 635</span></a><span class="sd">    [&quot;Attention Is All You Need&quot;](https://arxiv.org/abs/1706.03762).</span>
</span><span id="L-636"><a href="#L-636"><span class="linenos"> 636</span></a><span class="sd">    The differences are as follows:</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos"> 637</span></a><span class="sd">    - the so called &quot;PreNorm&quot; variation is used</span>
</span><span id="L-638"><a href="#L-638"><span class="linenos"> 638</span></a><span class="sd">      (`norm_first=True` in terms of `torch.nn.TransformerEncoderLayer`)</span>
</span><span id="L-639"><a href="#L-639"><span class="linenos"> 639</span></a><span class="sd">    - the very first normalization is skipped. This is **CRUCIAL** for FT-Transformer.</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos"> 640</span></a><span class="sd">    - the ReGLU activation is used in the feed-forward blocks. This is unlikely to be</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos"> 641</span></a><span class="sd">      crucial, but this is what we used in the paper.</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos"> 642</span></a>
</span><span id="L-643"><a href="#L-643"><span class="linenos"> 643</span></a><span class="sd">    **Shape**</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos"> 644</span></a>
</span><span id="L-645"><a href="#L-645"><span class="linenos"> 645</span></a><span class="sd">    - Input: `(batch_size, n_tokens, d_block)`</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos"> 646</span></a><span class="sd">    - Output: `(batch_size, d_out or d_block)`</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos"> 647</span></a>
</span><span id="L-648"><a href="#L-648"><span class="linenos"> 648</span></a><span class="sd">    **Examples**</span>
</span><span id="L-649"><a href="#L-649"><span class="linenos"> 649</span></a>
</span><span id="L-650"><a href="#L-650"><span class="linenos"> 650</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos"> 651</span></a><span class="sd">    &gt;&gt;&gt; n_tokens = 3</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos"> 652</span></a><span class="sd">    &gt;&gt;&gt; d_block = 16</span>
</span><span id="L-653"><a href="#L-653"><span class="linenos"> 653</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, n_tokens, d_block)</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos"> 654</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos"> 655</span></a><span class="sd">    &gt;&gt;&gt; m = Transformer(</span>
</span><span id="L-656"><a href="#L-656"><span class="linenos"> 656</span></a><span class="sd">    ...     d_out=d_out,</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos"> 657</span></a><span class="sd">    ...     n_blocks=2,</span>
</span><span id="L-658"><a href="#L-658"><span class="linenos"> 658</span></a><span class="sd">    ...     d_block=d_block,</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos"> 659</span></a><span class="sd">    ...     attention_n_heads=8,</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos"> 660</span></a><span class="sd">    ...     attention_dropout=0.2,</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos"> 661</span></a><span class="sd">    ...     ffn_d_hidden=None,</span>
</span><span id="L-662"><a href="#L-662"><span class="linenos"> 662</span></a><span class="sd">    ...     ffn_d_hidden_multiplier=4 / 3,</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos"> 663</span></a><span class="sd">    ...     ffn_dropout=0.1,</span>
</span><span id="L-664"><a href="#L-664"><span class="linenos"> 664</span></a><span class="sd">    ...     residual_dropout=0.0,</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos"> 665</span></a><span class="sd">    ... )</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos"> 666</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, d_out)</span>
</span><span id="L-667"><a href="#L-667"><span class="linenos"> 667</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-668"><a href="#L-668"><span class="linenos"> 668</span></a>
</span><span id="L-669"><a href="#L-669"><span class="linenos"> 669</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-670"><a href="#L-670"><span class="linenos"> 670</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos"> 671</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="L-672"><a href="#L-672"><span class="linenos"> 672</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="L-673"><a href="#L-673"><span class="linenos"> 673</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos"> 674</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos"> 675</span></a>        <span class="n">attention_n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-676"><a href="#L-676"><span class="linenos"> 676</span></a>        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos"> 677</span></a>        <span class="n">ffn_d_hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="L-678"><a href="#L-678"><span class="linenos"> 678</span></a>        <span class="n">ffn_d_hidden_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos"> 679</span></a>        <span class="n">ffn_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos"> 680</span></a>        <span class="n">residual_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-681"><a href="#L-681"><span class="linenos"> 681</span></a>        <span class="n">n_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos"> 682</span></a>        <span class="n">attention_kv_compression_ratio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos"> 683</span></a>        <span class="n">attention_kv_compression_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos"> 684</span></a>    <span class="p">):</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos"> 685</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-686"><a href="#L-686"><span class="linenos"> 686</span></a><span class="sd">        Args:</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos"> 687</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos"> 688</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="L-689"><a href="#L-689"><span class="linenos"> 689</span></a><span class="sd">            d_block: the block width</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos"> 690</span></a><span class="sd">                (or, equivalently, the embedding size of each feature).</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos"> 691</span></a><span class="sd">                Must be a multiple of `attention_n_heads`.</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos"> 692</span></a><span class="sd">            attention_n_heads: the argument for `MultiheadAttention`.</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos"> 693</span></a><span class="sd">            attention_dropout: the argument for `MultiheadAttention`.</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos"> 694</span></a><span class="sd">            ffn_d_hidden: the hidden representation size after the activation in the</span>
</span><span id="L-695"><a href="#L-695"><span class="linenos"> 695</span></a><span class="sd">                feed-forward blocks (or, equivalently, the *input* size of the *second*</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos"> 696</span></a><span class="sd">                linear layer in the feed-forward blocks). Since `Transformer` uses ReGLU</span>
</span><span id="L-697"><a href="#L-697"><span class="linenos"> 697</span></a><span class="sd">                activation function, the *output* size of the *first*</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos"> 698</span></a><span class="sd">                linear layer will be `2 * ffn_d_hidden`.</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos"> 699</span></a><span class="sd">            ffn_d_hidden_multiplier: the alternative way to set `ffn_d_hidden` as</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos"> 700</span></a><span class="sd">                `int(d_block * ffn_d_hidden_multiplier)`</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos"> 701</span></a><span class="sd">            ffn_dropout: the dropout rate for the hidden representation</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos"> 702</span></a><span class="sd">                in the feed-forward blocks.</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos"> 703</span></a><span class="sd">            residual_dropout: the dropout rate for all residual branches.</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos"> 704</span></a><span class="sd">            n_tokens: the argument for `MultiheadAttention`.</span>
</span><span id="L-705"><a href="#L-705"><span class="linenos"> 705</span></a><span class="sd">            attention_kv_compression_ratio: the argument for `MultiheadAttention`.</span>
</span><span id="L-706"><a href="#L-706"><span class="linenos"> 706</span></a><span class="sd">                Use this option with caution:</span>
</span><span id="L-707"><a href="#L-707"><span class="linenos"> 707</span></a><span class="sd">                - it can affect task performance in an unpredictable way</span>
</span><span id="L-708"><a href="#L-708"><span class="linenos"> 708</span></a><span class="sd">                - it can make things *slower* when the number of features</span>
</span><span id="L-709"><a href="#L-709"><span class="linenos"> 709</span></a><span class="sd">                  is not large enough</span>
</span><span id="L-710"><a href="#L-710"><span class="linenos"> 710</span></a><span class="sd">            attention_kv_compression_sharing: the argument for `MultiheadAttention`.</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos"> 711</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-712"><a href="#L-712"><span class="linenos"> 712</span></a>        <span class="k">if</span> <span class="n">attention_kv_compression_sharing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-713"><a href="#L-713"><span class="linenos"> 713</span></a>            <span class="k">assert</span> <span class="n">attention_kv_compression_sharing</span> <span class="ow">in</span> <span class="n">typing</span><span class="o">.</span><span class="n">get_args</span><span class="p">(</span>
</span><span id="L-714"><a href="#L-714"><span class="linenos"> 714</span></a>                <span class="n">_KV_COMPRESSION_SHARING</span>
</span><span id="L-715"><a href="#L-715"><span class="linenos"> 715</span></a>            <span class="p">)</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos"> 716</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">ffn_d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">ffn_d_hidden_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="L-717"><a href="#L-717"><span class="linenos"> 717</span></a>        <span class="k">if</span> <span class="n">ffn_d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-718"><a href="#L-718"><span class="linenos"> 718</span></a>            <span class="n">ffn_d_hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_block</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">ffn_d_hidden_multiplier</span><span class="p">))</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos"> 719</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-720"><a href="#L-720"><span class="linenos"> 720</span></a>
</span><span id="L-721"><a href="#L-721"><span class="linenos"> 721</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cls_embedding</span> <span class="o">=</span> <span class="n">CLSEmbedding</span><span class="p">(</span><span class="n">d_block</span><span class="p">)</span>
</span><span id="L-722"><a href="#L-722"><span class="linenos"> 722</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The [CLS]-token embedding.&quot;&quot;&quot;</span>
</span><span id="L-723"><a href="#L-723"><span class="linenos"> 723</span></a>
</span><span id="L-724"><a href="#L-724"><span class="linenos"> 724</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-725"><a href="#L-725"><span class="linenos"> 725</span></a>            <span class="p">[</span>
</span><span id="L-726"><a href="#L-726"><span class="linenos"> 726</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span>
</span><span id="L-727"><a href="#L-727"><span class="linenos"> 727</span></a>                    <span class="p">{</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos"> 728</span></a>                        <span class="c1"># &gt;&gt;&gt; attention</span>
</span><span id="L-729"><a href="#L-729"><span class="linenos"> 729</span></a>                        <span class="s1">&#39;attention&#39;</span><span class="p">:</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
</span><span id="L-730"><a href="#L-730"><span class="linenos"> 730</span></a>                            <span class="n">d_embedding</span><span class="o">=</span><span class="n">d_block</span><span class="p">,</span>
</span><span id="L-731"><a href="#L-731"><span class="linenos"> 731</span></a>                            <span class="n">n_heads</span><span class="o">=</span><span class="n">attention_n_heads</span><span class="p">,</span>
</span><span id="L-732"><a href="#L-732"><span class="linenos"> 732</span></a>                            <span class="n">dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos"> 733</span></a>                            <span class="n">n_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos"> 734</span></a>                            <span class="n">kv_compression_ratio</span><span class="o">=</span><span class="n">attention_kv_compression_ratio</span><span class="p">,</span>
</span><span id="L-735"><a href="#L-735"><span class="linenos"> 735</span></a>                            <span class="n">kv_compression_sharing</span><span class="o">=</span><span class="n">attention_kv_compression_sharing</span><span class="p">,</span>
</span><span id="L-736"><a href="#L-736"><span class="linenos"> 736</span></a>                        <span class="p">),</span>
</span><span id="L-737"><a href="#L-737"><span class="linenos"> 737</span></a>                        <span class="s1">&#39;attention_residual_dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">residual_dropout</span><span class="p">),</span>
</span><span id="L-738"><a href="#L-738"><span class="linenos"> 738</span></a>                        <span class="c1"># &gt;&gt;&gt; feed-forward</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos"> 739</span></a>                        <span class="s1">&#39;ffn_normalization&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">),</span>
</span><span id="L-740"><a href="#L-740"><span class="linenos"> 740</span></a>                        <span class="s1">&#39;ffn&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-741"><a href="#L-741"><span class="linenos"> 741</span></a>                            <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos"> 742</span></a>                                <span class="p">[</span>
</span><span id="L-743"><a href="#L-743"><span class="linenos"> 743</span></a>                                    <span class="c1"># Multiplying dimension by 2 to compensate for</span>
</span><span id="L-744"><a href="#L-744"><span class="linenos"> 744</span></a>                                    <span class="c1"># ReGLU which (internally) divides dimension by 2.</span>
</span><span id="L-745"><a href="#L-745"><span class="linenos"> 745</span></a>                                    <span class="p">(</span><span class="s1">&#39;linear1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">ffn_d_hidden</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos"> 746</span></a>                                    <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">_ReGLU</span><span class="p">()),</span>
</span><span id="L-747"><a href="#L-747"><span class="linenos"> 747</span></a>                                    <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">ffn_dropout</span><span class="p">)),</span>
</span><span id="L-748"><a href="#L-748"><span class="linenos"> 748</span></a>                                    <span class="p">(</span><span class="s1">&#39;linear2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ffn_d_hidden</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos"> 749</span></a>                                <span class="p">]</span>
</span><span id="L-750"><a href="#L-750"><span class="linenos"> 750</span></a>                            <span class="p">)</span>
</span><span id="L-751"><a href="#L-751"><span class="linenos"> 751</span></a>                        <span class="p">),</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos"> 752</span></a>                        <span class="s1">&#39;ffn_residual_dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">residual_dropout</span><span class="p">),</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos"> 753</span></a>                        <span class="c1"># &gt;&gt;&gt; output (for hooks-based introspection)</span>
</span><span id="L-754"><a href="#L-754"><span class="linenos"> 754</span></a>                        <span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
</span><span id="L-755"><a href="#L-755"><span class="linenos"> 755</span></a>                        <span class="c1"># &gt;&gt;&gt; the very first normalization</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos"> 756</span></a>                        <span class="o">**</span><span class="p">(</span>
</span><span id="L-757"><a href="#L-757"><span class="linenos"> 757</span></a>                            <span class="p">{}</span>
</span><span id="L-758"><a href="#L-758"><span class="linenos"> 758</span></a>                            <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos"> 759</span></a>                            <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;attention_normalization&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">)}</span>
</span><span id="L-760"><a href="#L-760"><span class="linenos"> 760</span></a>                        <span class="p">),</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos"> 761</span></a>                    <span class="p">}</span>
</span><span id="L-762"><a href="#L-762"><span class="linenos"> 762</span></a>                <span class="p">)</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos"> 763</span></a>                <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="L-764"><a href="#L-764"><span class="linenos"> 764</span></a>            <span class="p">]</span>
</span><span id="L-765"><a href="#L-765"><span class="linenos"> 765</span></a>        <span class="p">)</span>
</span><span id="L-766"><a href="#L-766"><span class="linenos"> 766</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos"> 767</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-768"><a href="#L-768"><span class="linenos"> 768</span></a>            <span class="kc">None</span>
</span><span id="L-769"><a href="#L-769"><span class="linenos"> 769</span></a>            <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="L-770"><a href="#L-770"><span class="linenos"> 770</span></a>            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos"> 771</span></a>                <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos"> 772</span></a>                    <span class="p">[</span>
</span><span id="L-773"><a href="#L-773"><span class="linenos"> 773</span></a>                        <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos"> 774</span></a>                        <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="L-775"><a href="#L-775"><span class="linenos"> 775</span></a>                        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)),</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos"> 776</span></a>                    <span class="p">]</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos"> 777</span></a>                <span class="p">)</span>
</span><span id="L-778"><a href="#L-778"><span class="linenos"> 778</span></a>            <span class="p">)</span>
</span><span id="L-779"><a href="#L-779"><span class="linenos"> 779</span></a>        <span class="p">)</span>
</span><span id="L-780"><a href="#L-780"><span class="linenos"> 780</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span><span id="L-781"><a href="#L-781"><span class="linenos"> 781</span></a>
</span><span id="L-782"><a href="#L-782"><span class="linenos"> 782</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos"> 783</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="L-784"><a href="#L-784"><span class="linenos"> 784</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
</span><span id="L-785"><a href="#L-785"><span class="linenos"> 785</span></a>
</span><span id="L-786"><a href="#L-786"><span class="linenos"> 786</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-787"><a href="#L-787"><span class="linenos"> 787</span></a>
</span><span id="L-788"><a href="#L-788"><span class="linenos"> 788</span></a>        <span class="n">n_blocks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span>
</span><span id="L-789"><a href="#L-789"><span class="linenos"> 789</span></a>        <span class="k">for</span> <span class="n">i_block</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span><span id="L-790"><a href="#L-790"><span class="linenos"> 790</span></a>            <span class="n">block</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">,</span> <span class="n">block</span><span class="p">)</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos"> 791</span></a>
</span><span id="L-792"><a href="#L-792"><span class="linenos"> 792</span></a>            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-793"><a href="#L-793"><span class="linenos"> 793</span></a>            <span class="k">if</span> <span class="s1">&#39;attention_normalization&#39;</span> <span class="ow">in</span> <span class="n">block</span><span class="p">:</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos"> 794</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention_normalization&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-795"><a href="#L-795"><span class="linenos"> 795</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i_block</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos"> 796</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention_residual_dropout&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-797"><a href="#L-797"><span class="linenos"> 797</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x_identity</span> <span class="o">+</span> <span class="n">x</span>
</span><span id="L-798"><a href="#L-798"><span class="linenos"> 798</span></a>
</span><span id="L-799"><a href="#L-799"><span class="linenos"> 799</span></a>            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos"> 800</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn_normalization&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-801"><a href="#L-801"><span class="linenos"> 801</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-802"><a href="#L-802"><span class="linenos"> 802</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn_residual_dropout&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-803"><a href="#L-803"><span class="linenos"> 803</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x_identity</span> <span class="o">+</span> <span class="n">x</span>
</span><span id="L-804"><a href="#L-804"><span class="linenos"> 804</span></a>
</span><span id="L-805"><a href="#L-805"><span class="linenos"> 805</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos"> 806</span></a>
</span><span id="L-807"><a href="#L-807"><span class="linenos"> 807</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># The representation of [CLS]-token.</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos"> 808</span></a>
</span><span id="L-809"><a href="#L-809"><span class="linenos"> 809</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-810"><a href="#L-810"><span class="linenos"> 810</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-811"><a href="#L-811"><span class="linenos"> 811</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-812"><a href="#L-812"><span class="linenos"> 812</span></a>
</span><span id="L-813"><a href="#L-813"><span class="linenos"> 813</span></a>
</span><span id="L-814"><a href="#L-814"><span class="linenos"> 814</span></a><span class="k">class</span> <span class="nc">FTTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-815"><a href="#L-815"><span class="linenos"> 815</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The FT-Transformer model from Section 3.3 in the paper.</span>
</span><span id="L-816"><a href="#L-816"><span class="linenos"> 816</span></a>
</span><span id="L-817"><a href="#L-817"><span class="linenos"> 817</span></a><span class="sd">    &lt;img src=&quot;ft-transformer-overview.png&quot; width=100%&gt;</span>
</span><span id="L-818"><a href="#L-818"><span class="linenos"> 818</span></a>
</span><span id="L-819"><a href="#L-819"><span class="linenos"> 819</span></a><span class="sd">    We should admit that &quot;Feature Tokenizer&quot; is a bad and misleading name,</span>
</span><span id="L-820"><a href="#L-820"><span class="linenos"> 820</span></a><span class="sd">    which misuses the term &quot;token&quot;. A better name would be &quot;Feature Embeddings&quot;.</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos"> 821</span></a>
</span><span id="L-822"><a href="#L-822"><span class="linenos"> 822</span></a><span class="sd">    &lt;img src=&quot;ft-transformer-details.png&quot; width=100%&gt;</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos"> 823</span></a>
</span><span id="L-824"><a href="#L-824"><span class="linenos"> 824</span></a><span class="sd">    The default hyperparameters can be obtained with `FTTransformer.get_default_kwargs`.</span>
</span><span id="L-825"><a href="#L-825"><span class="linenos"> 825</span></a>
</span><span id="L-826"><a href="#L-826"><span class="linenos"> 826</span></a><span class="sd">    **Shape**</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos"> 827</span></a>
</span><span id="L-828"><a href="#L-828"><span class="linenos"> 828</span></a><span class="sd">    - Input:</span>
</span><span id="L-829"><a href="#L-829"><span class="linenos"> 829</span></a><span class="sd">        - continuous features: `x_cont ~ (batch_size, n_cont_features)`</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos"> 830</span></a><span class="sd">        - categorical features: `x_cat ~ (batch_size, len(cat_cardinalities))`</span>
</span><span id="L-831"><a href="#L-831"><span class="linenos"> 831</span></a><span class="sd">    - Output: `(batch_size, d_out or d_block)`</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos"> 832</span></a>
</span><span id="L-833"><a href="#L-833"><span class="linenos"> 833</span></a><span class="sd">    **Examples**</span>
</span><span id="L-834"><a href="#L-834"><span class="linenos"> 834</span></a>
</span><span id="L-835"><a href="#L-835"><span class="linenos"> 835</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="L-836"><a href="#L-836"><span class="linenos"> 836</span></a><span class="sd">    &gt;&gt;&gt; n_cont_feaatures = 3</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos"> 837</span></a><span class="sd">    &gt;&gt;&gt; cardinalities = [3, 4]</span>
</span><span id="L-838"><a href="#L-838"><span class="linenos"> 838</span></a><span class="sd">    &gt;&gt;&gt; x_cont = torch.randn(batch_size, n_cont_feaatures)</span>
</span><span id="L-839"><a href="#L-839"><span class="linenos"> 839</span></a><span class="sd">    &gt;&gt;&gt; x_cat = torch.column_stack([</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos"> 840</span></a><span class="sd">    ...     torch.randint(0, c, (batch_size,))</span>
</span><span id="L-841"><a href="#L-841"><span class="linenos"> 841</span></a><span class="sd">    ...     for c in cardinalities</span>
</span><span id="L-842"><a href="#L-842"><span class="linenos"> 842</span></a><span class="sd">    ... ])</span>
</span><span id="L-843"><a href="#L-843"><span class="linenos"> 843</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="L-844"><a href="#L-844"><span class="linenos"> 844</span></a><span class="sd">    &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="L-845"><a href="#L-845"><span class="linenos"> 845</span></a><span class="sd">    ...     n_cont_features=n_cont_feaatures,</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos"> 846</span></a><span class="sd">    ...     cat_cardinalities=cardinalities,</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos"> 847</span></a><span class="sd">    ...     d_out=d_out,</span>
</span><span id="L-848"><a href="#L-848"><span class="linenos"> 848</span></a><span class="sd">    ...     n_blocks=2,</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos"> 849</span></a><span class="sd">    ...     d_block=16,</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos"> 850</span></a><span class="sd">    ...     attention_n_heads=8,</span>
</span><span id="L-851"><a href="#L-851"><span class="linenos"> 851</span></a><span class="sd">    ...     attention_dropout=0.2,</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos"> 852</span></a><span class="sd">    ...     ffn_d_hidden=None,</span>
</span><span id="L-853"><a href="#L-853"><span class="linenos"> 853</span></a><span class="sd">    ...     ffn_d_hidden_multiplier=4 / 3,</span>
</span><span id="L-854"><a href="#L-854"><span class="linenos"> 854</span></a><span class="sd">    ...     ffn_dropout=0.1,</span>
</span><span id="L-855"><a href="#L-855"><span class="linenos"> 855</span></a><span class="sd">    ...     residual_dropout=0.0,</span>
</span><span id="L-856"><a href="#L-856"><span class="linenos"> 856</span></a><span class="sd">    ... )</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos"> 857</span></a><span class="sd">    &gt;&gt;&gt; assert m(x_cont, x_cat).shape == (batch_size, d_out)</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos"> 858</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-859"><a href="#L-859"><span class="linenos"> 859</span></a>
</span><span id="L-860"><a href="#L-860"><span class="linenos"> 860</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-861"><a href="#L-861"><span class="linenos"> 861</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-862"><a href="#L-862"><span class="linenos"> 862</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="L-863"><a href="#L-863"><span class="linenos"> 863</span></a>        <span class="n">n_cont_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-864"><a href="#L-864"><span class="linenos"> 864</span></a>        <span class="n">cat_cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="L-865"><a href="#L-865"><span class="linenos"> 865</span></a>        <span class="n">_is_default</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos"> 866</span></a>        <span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">,</span>
</span><span id="L-867"><a href="#L-867"><span class="linenos"> 867</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos"> 868</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos"> 869</span></a><span class="sd">        Args:</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos"> 870</span></a><span class="sd">            n_cont_features: the number of continuous features</span>
</span><span id="L-871"><a href="#L-871"><span class="linenos"> 871</span></a><span class="sd">            cat_cardinalities: the cardinalities of categorical features (see</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos"> 872</span></a><span class="sd">                `CategoricalFeatureEmbeddings` for details). Pass en empty list</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos"> 873</span></a><span class="sd">                if there are no categorical features.</span>
</span><span id="L-874"><a href="#L-874"><span class="linenos"> 874</span></a><span class="sd">            _is_default: this is a technical argument, don&#39;t set it manually.</span>
</span><span id="L-875"><a href="#L-875"><span class="linenos"> 875</span></a><span class="sd">            transformer_kwargs: the keyword arguments for the `Transformer` backbone.</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos"> 876</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-877"><a href="#L-877"><span class="linenos"> 877</span></a>        <span class="k">assert</span> <span class="n">n_cont_features</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</span><span id="L-878"><a href="#L-878"><span class="linenos"> 878</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cat_cardinalities</span><span class="p">)</span>
</span><span id="L-879"><a href="#L-879"><span class="linenos"> 879</span></a>        <span class="k">assert</span> <span class="n">n_cont_features</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">cat_cardinalities</span>
</span><span id="L-880"><a href="#L-880"><span class="linenos"> 880</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos"> 881</span></a>
</span><span id="L-882"><a href="#L-882"><span class="linenos"> 882</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">transformer_kwargs</span><span class="p">[</span><span class="s1">&#39;d_block&#39;</span><span class="p">]</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos"> 883</span></a>        <span class="c1"># &gt;&gt;&gt; Feature embeddings (see Figure 2a in the paper).</span>
</span><span id="L-884"><a href="#L-884"><span class="linenos"> 884</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cont_embeddings</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-885"><a href="#L-885"><span class="linenos"> 885</span></a>            <span class="n">LinearEmbeddings</span><span class="p">(</span><span class="n">n_cont_features</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span> <span class="k">if</span> <span class="n">n_cont_features</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-886"><a href="#L-886"><span class="linenos"> 886</span></a>        <span class="p">)</span>
</span><span id="L-887"><a href="#L-887"><span class="linenos"> 887</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings for continuous features.&quot;&quot;&quot;</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos"> 888</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cat_embeddings</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-889"><a href="#L-889"><span class="linenos"> 889</span></a>            <span class="n">CategoricalFeatureEmbeddings</span><span class="p">(</span><span class="n">cat_cardinalities</span><span class="p">,</span> <span class="n">d_block</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="L-890"><a href="#L-890"><span class="linenos"> 890</span></a>            <span class="k">if</span> <span class="n">cat_cardinalities</span>
</span><span id="L-891"><a href="#L-891"><span class="linenos"> 891</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-892"><a href="#L-892"><span class="linenos"> 892</span></a>        <span class="p">)</span>
</span><span id="L-893"><a href="#L-893"><span class="linenos"> 893</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings for categorical features.&quot;&quot;&quot;</span>
</span><span id="L-894"><a href="#L-894"><span class="linenos"> 894</span></a>        <span class="c1"># &gt;&gt;&gt;</span>
</span><span id="L-895"><a href="#L-895"><span class="linenos"> 895</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">FTTransformerBackbone</span><span class="p">(</span><span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">)</span>
</span><span id="L-896"><a href="#L-896"><span class="linenos"> 896</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The transformer backbone.&quot;&quot;&quot;</span>
</span><span id="L-897"><a href="#L-897"><span class="linenos"> 897</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_is_default</span> <span class="o">=</span> <span class="n">_is_default</span>
</span><span id="L-898"><a href="#L-898"><span class="linenos"> 898</span></a>
</span><span id="L-899"><a href="#L-899"><span class="linenos"> 899</span></a>    <span class="nd">@classmethod</span>
</span><span id="L-900"><a href="#L-900"><span class="linenos"> 900</span></a>    <span class="k">def</span> <span class="nf">get_default_kwargs</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
</span><span id="L-901"><a href="#L-901"><span class="linenos"> 901</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the default hyperparameters.</span>
</span><span id="L-902"><a href="#L-902"><span class="linenos"> 902</span></a>
</span><span id="L-903"><a href="#L-903"><span class="linenos"> 903</span></a><span class="sd">        Args:</span>
</span><span id="L-904"><a href="#L-904"><span class="linenos"> 904</span></a><span class="sd">            n_blocks: the number of blocks. The supported values are in `range(1, 7)`.</span>
</span><span id="L-905"><a href="#L-905"><span class="linenos"> 905</span></a><span class="sd">        Returns:</span>
</span><span id="L-906"><a href="#L-906"><span class="linenos"> 906</span></a><span class="sd">            the default keyword arguments for the constructor</span>
</span><span id="L-907"><a href="#L-907"><span class="linenos"> 907</span></a>
</span><span id="L-908"><a href="#L-908"><span class="linenos"> 908</span></a><span class="sd">        **Examples**</span>
</span><span id="L-909"><a href="#L-909"><span class="linenos"> 909</span></a>
</span><span id="L-910"><a href="#L-910"><span class="linenos"> 910</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="L-911"><a href="#L-911"><span class="linenos"> 911</span></a><span class="sd">        ...     n_cont_features=3,</span>
</span><span id="L-912"><a href="#L-912"><span class="linenos"> 912</span></a><span class="sd">        ...     cat_cardinalities=[4, 5],</span>
</span><span id="L-913"><a href="#L-913"><span class="linenos"> 913</span></a><span class="sd">        ...     d_out=1,</span>
</span><span id="L-914"><a href="#L-914"><span class="linenos"> 914</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs()</span>
</span><span id="L-915"><a href="#L-915"><span class="linenos"> 915</span></a><span class="sd">        ... )</span>
</span><span id="L-916"><a href="#L-916"><span class="linenos"> 916</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-917"><a href="#L-917"><span class="linenos"> 917</span></a>        <span class="k">assert</span> <span class="p">(</span>
</span><span id="L-918"><a href="#L-918"><span class="linenos"> 918</span></a>            <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">n_blocks</span> <span class="o">&lt;=</span> <span class="mi">6</span>
</span><span id="L-919"><a href="#L-919"><span class="linenos"> 919</span></a>        <span class="p">),</span> <span class="s1">&#39;We offer default configurations only for `n_blocks in range(1, 7)`&#39;</span>
</span><span id="L-920"><a href="#L-920"><span class="linenos"> 920</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="L-921"><a href="#L-921"><span class="linenos"> 921</span></a>            <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="n">n_blocks</span><span class="p">,</span>
</span><span id="L-922"><a href="#L-922"><span class="linenos"> 922</span></a>            <span class="s1">&#39;d_block&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">384</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="L-923"><a href="#L-923"><span class="linenos"> 923</span></a>            <span class="s1">&#39;attention_n_heads&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="L-924"><a href="#L-924"><span class="linenos"> 924</span></a>            <span class="s1">&#39;attention_dropout&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="L-925"><a href="#L-925"><span class="linenos"> 925</span></a>            <span class="c1"># Because of the ReGLU activation used by FT-Transformer,</span>
</span><span id="L-926"><a href="#L-926"><span class="linenos"> 926</span></a>            <span class="c1"># 4 / 3 results in roughly the same number of parameters as 2.0</span>
</span><span id="L-927"><a href="#L-927"><span class="linenos"> 927</span></a>            <span class="c1"># would with simple element-wise activations (e.g. ReLU).</span>
</span><span id="L-928"><a href="#L-928"><span class="linenos"> 928</span></a>            <span class="s1">&#39;ffn_d_hidden&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-929"><a href="#L-929"><span class="linenos"> 929</span></a>            <span class="s1">&#39;ffn_d_hidden_multiplier&#39;</span><span class="p">:</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="L-930"><a href="#L-930"><span class="linenos"> 930</span></a>            <span class="s1">&#39;ffn_dropout&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="L-931"><a href="#L-931"><span class="linenos"> 931</span></a>            <span class="s1">&#39;residual_dropout&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="L-932"><a href="#L-932"><span class="linenos"> 932</span></a>            <span class="s1">&#39;_is_default&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-933"><a href="#L-933"><span class="linenos"> 933</span></a>        <span class="p">}</span>
</span><span id="L-934"><a href="#L-934"><span class="linenos"> 934</span></a>
</span><span id="L-935"><a href="#L-935"><span class="linenos"> 935</span></a>    <span class="k">def</span> <span class="nf">make_parameter_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
</span><span id="L-936"><a href="#L-936"><span class="linenos"> 936</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Make parameter groups for optimizers.</span>
</span><span id="L-937"><a href="#L-937"><span class="linenos"> 937</span></a>
</span><span id="L-938"><a href="#L-938"><span class="linenos"> 938</span></a><span class="sd">        The difference with calling this method instead of</span>
</span><span id="L-939"><a href="#L-939"><span class="linenos"> 939</span></a><span class="sd">        `.parameters()` is that this method always sets `weight_decay=0.0`</span>
</span><span id="L-940"><a href="#L-940"><span class="linenos"> 940</span></a><span class="sd">        for some of the parameters.</span>
</span><span id="L-941"><a href="#L-941"><span class="linenos"> 941</span></a>
</span><span id="L-942"><a href="#L-942"><span class="linenos"> 942</span></a><span class="sd">        **Examples**</span>
</span><span id="L-943"><a href="#L-943"><span class="linenos"> 943</span></a>
</span><span id="L-944"><a href="#L-944"><span class="linenos"> 944</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="L-945"><a href="#L-945"><span class="linenos"> 945</span></a><span class="sd">        ...     n_cont_features=2,</span>
</span><span id="L-946"><a href="#L-946"><span class="linenos"> 946</span></a><span class="sd">        ...     cat_cardinalities=[3, 4],</span>
</span><span id="L-947"><a href="#L-947"><span class="linenos"> 947</span></a><span class="sd">        ...     d_out=5,</span>
</span><span id="L-948"><a href="#L-948"><span class="linenos"> 948</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs(),</span>
</span><span id="L-949"><a href="#L-949"><span class="linenos"> 949</span></a><span class="sd">        ... )</span>
</span><span id="L-950"><a href="#L-950"><span class="linenos"> 950</span></a><span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.AdamW(</span>
</span><span id="L-951"><a href="#L-951"><span class="linenos"> 951</span></a><span class="sd">        ...     m.make_parameter_groups(),</span>
</span><span id="L-952"><a href="#L-952"><span class="linenos"> 952</span></a><span class="sd">        ...     lr=1e-4,</span>
</span><span id="L-953"><a href="#L-953"><span class="linenos"> 953</span></a><span class="sd">        ...     weight_decay=1e-5,</span>
</span><span id="L-954"><a href="#L-954"><span class="linenos"> 954</span></a><span class="sd">        ... )</span>
</span><span id="L-955"><a href="#L-955"><span class="linenos"> 955</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-956"><a href="#L-956"><span class="linenos"> 956</span></a>        <span class="n">main_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[]}</span>
</span><span id="L-957"><a href="#L-957"><span class="linenos"> 957</span></a>        <span class="n">zero_wd_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
</span><span id="L-958"><a href="#L-958"><span class="linenos"> 958</span></a>
</span><span id="L-959"><a href="#L-959"><span class="linenos"> 959</span></a>        <span class="n">zero_wd_subnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="s1">&#39;.bias&#39;</span><span class="p">]</span>
</span><span id="L-960"><a href="#L-960"><span class="linenos"> 960</span></a>        <span class="k">for</span> <span class="n">modulename</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;cont_embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;cat_embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;cls_embedding&#39;</span><span class="p">]:</span>
</span><span id="L-961"><a href="#L-961"><span class="linenos"> 961</span></a>            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modulename</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-962"><a href="#L-962"><span class="linenos"> 962</span></a>                <span class="n">zero_wd_subnames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modulename</span><span class="p">)</span>
</span><span id="L-963"><a href="#L-963"><span class="linenos"> 963</span></a>        <span class="c1"># Check that there are no typos in the above list.</span>
</span><span id="L-964"><a href="#L-964"><span class="linenos"> 964</span></a>        <span class="k">for</span> <span class="n">subname</span> <span class="ow">in</span> <span class="n">zero_wd_subnames</span><span class="p">:</span>
</span><span id="L-965"><a href="#L-965"><span class="linenos"> 965</span></a>            <span class="k">assert</span> <span class="nb">any</span><span class="p">(</span>
</span><span id="L-966"><a href="#L-966"><span class="linenos"> 966</span></a>                <span class="n">subname</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span id="L-967"><a href="#L-967"><span class="linenos"> 967</span></a>            <span class="p">),</span> <span class="n">_INTERNAL_ERROR_MESSAGE</span>
</span><span id="L-968"><a href="#L-968"><span class="linenos"> 968</span></a>
</span><span id="L-969"><a href="#L-969"><span class="linenos"> 969</span></a>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span><span id="L-970"><a href="#L-970"><span class="linenos"> 970</span></a>            <span class="n">zero_wd_condition</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">subname</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">subname</span> <span class="ow">in</span> <span class="n">zero_wd_subnames</span><span class="p">)</span>
</span><span id="L-971"><a href="#L-971"><span class="linenos"> 971</span></a>            <span class="p">(</span><span class="n">zero_wd_group</span> <span class="k">if</span> <span class="n">zero_wd_condition</span> <span class="k">else</span> <span class="n">main_group</span><span class="p">)[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-972"><a href="#L-972"><span class="linenos"> 972</span></a>                <span class="n">parameter</span>
</span><span id="L-973"><a href="#L-973"><span class="linenos"> 973</span></a>            <span class="p">)</span>
</span><span id="L-974"><a href="#L-974"><span class="linenos"> 974</span></a>        <span class="k">return</span> <span class="p">[</span><span class="n">main_group</span><span class="p">,</span> <span class="n">zero_wd_group</span><span class="p">]</span>
</span><span id="L-975"><a href="#L-975"><span class="linenos"> 975</span></a>
</span><span id="L-976"><a href="#L-976"><span class="linenos"> 976</span></a>    <span class="k">def</span> <span class="nf">make_default_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">:</span>
</span><span id="L-977"><a href="#L-977"><span class="linenos"> 977</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the &quot;default&quot; `torch.nn.AdamW` suitable for the *default* FT-Transformer.</span>
</span><span id="L-978"><a href="#L-978"><span class="linenos"> 978</span></a>
</span><span id="L-979"><a href="#L-979"><span class="linenos"> 979</span></a><span class="sd">        Returns:</span>
</span><span id="L-980"><a href="#L-980"><span class="linenos"> 980</span></a><span class="sd">            optimizer</span>
</span><span id="L-981"><a href="#L-981"><span class="linenos"> 981</span></a>
</span><span id="L-982"><a href="#L-982"><span class="linenos"> 982</span></a><span class="sd">        **Examples**</span>
</span><span id="L-983"><a href="#L-983"><span class="linenos"> 983</span></a>
</span><span id="L-984"><a href="#L-984"><span class="linenos"> 984</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="L-985"><a href="#L-985"><span class="linenos"> 985</span></a><span class="sd">        ...     n_cont_features=2,</span>
</span><span id="L-986"><a href="#L-986"><span class="linenos"> 986</span></a><span class="sd">        ...     cat_cardinalities=[3, 4],</span>
</span><span id="L-987"><a href="#L-987"><span class="linenos"> 987</span></a><span class="sd">        ...     d_out=5,</span>
</span><span id="L-988"><a href="#L-988"><span class="linenos"> 988</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs(),</span>
</span><span id="L-989"><a href="#L-989"><span class="linenos"> 989</span></a><span class="sd">        ... )</span>
</span><span id="L-990"><a href="#L-990"><span class="linenos"> 990</span></a><span class="sd">        &gt;&gt;&gt; optimizer = m.make_default_optimizer()</span>
</span><span id="L-991"><a href="#L-991"><span class="linenos"> 991</span></a><span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
</span><span id="L-992"><a href="#L-992"><span class="linenos"> 992</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default</span><span class="p">:</span>
</span><span id="L-993"><a href="#L-993"><span class="linenos"> 993</span></a>            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
</span><span id="L-994"><a href="#L-994"><span class="linenos"> 994</span></a>                <span class="s1">&#39;The default opimizer is supposed to be used in a combination&#39;</span>
</span><span id="L-995"><a href="#L-995"><span class="linenos"> 995</span></a>                <span class="s1">&#39; with the default FT-Transformer.&#39;</span>
</span><span id="L-996"><a href="#L-996"><span class="linenos"> 996</span></a>            <span class="p">)</span>
</span><span id="L-997"><a href="#L-997"><span class="linenos"> 997</span></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
</span><span id="L-998"><a href="#L-998"><span class="linenos"> 998</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">make_parameter_groups</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span>
</span><span id="L-999"><a href="#L-999"><span class="linenos"> 999</span></a>        <span class="p">)</span>
</span><span id="L-1000"><a href="#L-1000"><span class="linenos">1000</span></a>
</span><span id="L-1001"><a href="#L-1001"><span class="linenos">1001</span></a>    <span class="n">_FORWARD_BAD_ARGS_MESSAGE</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-1002"><a href="#L-1002"><span class="linenos">1002</span></a>        <span class="s1">&#39;Based on the arguments passed to the constructor of FT-Transformer, </span><span class="si">{}</span><span class="s1">&#39;</span>
</span><span id="L-1003"><a href="#L-1003"><span class="linenos">1003</span></a>    <span class="p">)</span>
</span><span id="L-1004"><a href="#L-1004"><span class="linenos">1004</span></a>
</span><span id="L-1005"><a href="#L-1005"><span class="linenos">1005</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_cont</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">x_cat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-1006"><a href="#L-1006"><span class="linenos">1006</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.</span>
</span><span id="L-1007"><a href="#L-1007"><span class="linenos">1007</span></a>
</span><span id="L-1008"><a href="#L-1008"><span class="linenos">1008</span></a><span class="sd">        Args:</span>
</span><span id="L-1009"><a href="#L-1009"><span class="linenos">1009</span></a><span class="sd">            x_cont: the continuous features.</span>
</span><span id="L-1010"><a href="#L-1010"><span class="linenos">1010</span></a><span class="sd">            x_cat: the categorical features.</span>
</span><span id="L-1011"><a href="#L-1011"><span class="linenos">1011</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1012"><a href="#L-1012"><span class="linenos">1012</span></a>        <span class="k">assert</span> <span class="n">x_cont</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">x_cat</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-1013"><a href="#L-1013"><span class="linenos">1013</span></a>
</span><span id="L-1014"><a href="#L-1014"><span class="linenos">1014</span></a>        <span class="n">x_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-1015"><a href="#L-1015"><span class="linenos">1015</span></a>        <span class="k">for</span> <span class="n">argname</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="L-1016"><a href="#L-1016"><span class="linenos">1016</span></a>            <span class="p">(</span><span class="s1">&#39;x_cont&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_embeddings</span><span class="p">),</span>
</span><span id="L-1017"><a href="#L-1017"><span class="linenos">1017</span></a>            <span class="p">(</span><span class="s1">&#39;x_cat&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embeddings</span><span class="p">),</span>
</span><span id="L-1018"><a href="#L-1018"><span class="linenos">1018</span></a>        <span class="p">]:</span>
</span><span id="L-1019"><a href="#L-1019"><span class="linenos">1019</span></a>            <span class="n">argvalue</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">argname</span><span class="p">]</span>
</span><span id="L-1020"><a href="#L-1020"><span class="linenos">1020</span></a>            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1021"><a href="#L-1021"><span class="linenos">1021</span></a>                <span class="k">assert</span> <span class="n">argvalue</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">FTTransformer</span><span class="o">.</span><span class="n">_FORWARD_BAD_ARGS_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="L-1022"><a href="#L-1022"><span class="linenos">1022</span></a>                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">argname</span><span class="si">}</span><span class="s1"> must be None&#39;</span>
</span><span id="L-1023"><a href="#L-1023"><span class="linenos">1023</span></a>                <span class="p">)</span>
</span><span id="L-1024"><a href="#L-1024"><span class="linenos">1024</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1025"><a href="#L-1025"><span class="linenos">1025</span></a>                <span class="k">assert</span> <span class="p">(</span>
</span><span id="L-1026"><a href="#L-1026"><span class="linenos">1026</span></a>                    <span class="n">argvalue</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-1027"><a href="#L-1027"><span class="linenos">1027</span></a>                <span class="p">),</span> <span class="n">FTTransformer</span><span class="o">.</span><span class="n">_FORWARD_BAD_ARGS_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="L-1028"><a href="#L-1028"><span class="linenos">1028</span></a>                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">argname</span><span class="si">}</span><span class="s1"> must not be None&#39;</span>
</span><span id="L-1029"><a href="#L-1029"><span class="linenos">1029</span></a>                <span class="p">)</span>
</span><span id="L-1030"><a href="#L-1030"><span class="linenos">1030</span></a>                <span class="n">x_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="p">(</span><span class="n">argvalue</span><span class="p">))</span>
</span><span id="L-1031"><a href="#L-1031"><span class="linenos">1031</span></a>        <span class="k">assert</span> <span class="n">x_embeddings</span>
</span><span id="L-1032"><a href="#L-1032"><span class="linenos">1032</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1033"><a href="#L-1033"><span class="linenos">1033</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-1034"><a href="#L-1034"><span class="linenos">1034</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            </section>
                <section id="MLP">
                            <input id="MLP-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MLP</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MLP-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP-99"><a href="#MLP-99"><span class="linenos"> 99</span></a><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MLP-100"><a href="#MLP-100"><span class="linenos">100</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The MLP model from Section 3.1 in the paper.</span>
</span><span id="MLP-101"><a href="#MLP-101"><span class="linenos">101</span></a>
</span><span id="MLP-102"><a href="#MLP-102"><span class="linenos">102</span></a><span class="sd">    ```</span>
</span><span id="MLP-103"><a href="#MLP-103"><span class="linenos">103</span></a><span class="sd">    MLP:   (in) -&gt; Block  -&gt; ...  -&gt; Block   -&gt; (out)</span>
</span><span id="MLP-104"><a href="#MLP-104"><span class="linenos">104</span></a><span class="sd">    Block: (in) -&gt; Linear -&gt; ReLU -&gt; Dropout -&gt; (out)</span>
</span><span id="MLP-105"><a href="#MLP-105"><span class="linenos">105</span></a><span class="sd">    ```</span>
</span><span id="MLP-106"><a href="#MLP-106"><span class="linenos">106</span></a>
</span><span id="MLP-107"><a href="#MLP-107"><span class="linenos">107</span></a><span class="sd">    **Shape**</span>
</span><span id="MLP-108"><a href="#MLP-108"><span class="linenos">108</span></a>
</span><span id="MLP-109"><a href="#MLP-109"><span class="linenos">109</span></a><span class="sd">    - Input: `(*, d_in)`</span>
</span><span id="MLP-110"><a href="#MLP-110"><span class="linenos">110</span></a><span class="sd">    - Output: `(*, d_out or d_block)`</span>
</span><span id="MLP-111"><a href="#MLP-111"><span class="linenos">111</span></a>
</span><span id="MLP-112"><a href="#MLP-112"><span class="linenos">112</span></a><span class="sd">    **Examples**</span>
</span><span id="MLP-113"><a href="#MLP-113"><span class="linenos">113</span></a>
</span><span id="MLP-114"><a href="#MLP-114"><span class="linenos">114</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="MLP-115"><a href="#MLP-115"><span class="linenos">115</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, 3)</span>
</span><span id="MLP-116"><a href="#MLP-116"><span class="linenos">116</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="MLP-117"><a href="#MLP-117"><span class="linenos">117</span></a><span class="sd">    &gt;&gt;&gt; m = MLP(</span>
</span><span id="MLP-118"><a href="#MLP-118"><span class="linenos">118</span></a><span class="sd">    ...    d_in=x.shape[1],</span>
</span><span id="MLP-119"><a href="#MLP-119"><span class="linenos">119</span></a><span class="sd">    ...    d_out=d_out,</span>
</span><span id="MLP-120"><a href="#MLP-120"><span class="linenos">120</span></a><span class="sd">    ...    n_blocks=4,</span>
</span><span id="MLP-121"><a href="#MLP-121"><span class="linenos">121</span></a><span class="sd">    ...    d_block=5,</span>
</span><span id="MLP-122"><a href="#MLP-122"><span class="linenos">122</span></a><span class="sd">    ...    dropout=0.1,</span>
</span><span id="MLP-123"><a href="#MLP-123"><span class="linenos">123</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="MLP-124"><a href="#MLP-124"><span class="linenos">124</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, d_out)</span>
</span><span id="MLP-125"><a href="#MLP-125"><span class="linenos">125</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MLP-126"><a href="#MLP-126"><span class="linenos">126</span></a>
</span><span id="MLP-127"><a href="#MLP-127"><span class="linenos">127</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="MLP-128"><a href="#MLP-128"><span class="linenos">128</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MLP-129"><a href="#MLP-129"><span class="linenos">129</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="MLP-130"><a href="#MLP-130"><span class="linenos">130</span></a>        <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MLP-131"><a href="#MLP-131"><span class="linenos">131</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="MLP-132"><a href="#MLP-132"><span class="linenos">132</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MLP-133"><a href="#MLP-133"><span class="linenos">133</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MLP-134"><a href="#MLP-134"><span class="linenos">134</span></a>        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="MLP-135"><a href="#MLP-135"><span class="linenos">135</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MLP-136"><a href="#MLP-136"><span class="linenos">136</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-137"><a href="#MLP-137"><span class="linenos">137</span></a><span class="sd">        Args:</span>
</span><span id="MLP-138"><a href="#MLP-138"><span class="linenos">138</span></a><span class="sd">            d_in: the input size.</span>
</span><span id="MLP-139"><a href="#MLP-139"><span class="linenos">139</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="MLP-140"><a href="#MLP-140"><span class="linenos">140</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="MLP-141"><a href="#MLP-141"><span class="linenos">141</span></a><span class="sd">            d_block: the block width.</span>
</span><span id="MLP-142"><a href="#MLP-142"><span class="linenos">142</span></a><span class="sd">            dropout: the dropout rate.</span>
</span><span id="MLP-143"><a href="#MLP-143"><span class="linenos">143</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-144"><a href="#MLP-144"><span class="linenos">144</span></a>        <span class="k">assert</span> <span class="n">n_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="MLP-145"><a href="#MLP-145"><span class="linenos">145</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MLP-146"><a href="#MLP-146"><span class="linenos">146</span></a>
</span><span id="MLP-147"><a href="#MLP-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="MLP-148"><a href="#MLP-148"><span class="linenos">148</span></a>            <span class="p">[</span>
</span><span id="MLP-149"><a href="#MLP-149"><span class="linenos">149</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="MLP-150"><a href="#MLP-150"><span class="linenos">150</span></a>                    <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="MLP-151"><a href="#MLP-151"><span class="linenos">151</span></a>                        <span class="p">[</span>
</span><span id="MLP-152"><a href="#MLP-152"><span class="linenos">152</span></a>                            <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="MLP-153"><a href="#MLP-153"><span class="linenos">153</span></a>                            <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="MLP-154"><a href="#MLP-154"><span class="linenos">154</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)),</span>
</span><span id="MLP-155"><a href="#MLP-155"><span class="linenos">155</span></a>                        <span class="p">]</span>
</span><span id="MLP-156"><a href="#MLP-156"><span class="linenos">156</span></a>                    <span class="p">)</span>
</span><span id="MLP-157"><a href="#MLP-157"><span class="linenos">157</span></a>                <span class="p">)</span>
</span><span id="MLP-158"><a href="#MLP-158"><span class="linenos">158</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="MLP-159"><a href="#MLP-159"><span class="linenos">159</span></a>            <span class="p">]</span>
</span><span id="MLP-160"><a href="#MLP-160"><span class="linenos">160</span></a>        <span class="p">)</span>
</span><span id="MLP-161"><a href="#MLP-161"><span class="linenos">161</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="MLP-162"><a href="#MLP-162"><span class="linenos">162</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</span><span id="MLP-163"><a href="#MLP-163"><span class="linenos">163</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span><span id="MLP-164"><a href="#MLP-164"><span class="linenos">164</span></a>
</span><span id="MLP-165"><a href="#MLP-165"><span class="linenos">165</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MLP-166"><a href="#MLP-166"><span class="linenos">166</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="MLP-167"><a href="#MLP-167"><span class="linenos">167</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="MLP-168"><a href="#MLP-168"><span class="linenos">168</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP-169"><a href="#MLP-169"><span class="linenos">169</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MLP-170"><a href="#MLP-170"><span class="linenos">170</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP-171"><a href="#MLP-171"><span class="linenos">171</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>The MLP model from Section 3.1 in the paper.</p>

<pre><code>MLP:   (in) -&gt; Block  -&gt; ...  -&gt; Block   -&gt; (out)
Block: (in) -&gt; Linear -&gt; ReLU -&gt; Dropout -&gt; (out)
</code></pre>

<p><strong>Shape</strong></p>

<ul>
<li>Input: <code>(*, d_in)</code></li>
<li>Output: <code>(*, d_out or d_block)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_out</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">d_in</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>   <span class="n">d_out</span><span class="o">=</span><span class="n">d_out</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">d_block</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</code></pre>
</div>
</div>


                            <div id="MLP.__init__" class="classattr">
                                        <input id="MLP.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MLP</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="o">*</span>,</span><span class="param">	<span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_out</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span></span>)</span>

                <label class="view-source-button" for="MLP.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP.__init__-127"><a href="#MLP.__init__-127"><span class="linenos">127</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="MLP.__init__-128"><a href="#MLP.__init__-128"><span class="linenos">128</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MLP.__init__-129"><a href="#MLP.__init__-129"><span class="linenos">129</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="MLP.__init__-130"><a href="#MLP.__init__-130"><span class="linenos">130</span></a>        <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MLP.__init__-131"><a href="#MLP.__init__-131"><span class="linenos">131</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="MLP.__init__-132"><a href="#MLP.__init__-132"><span class="linenos">132</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MLP.__init__-133"><a href="#MLP.__init__-133"><span class="linenos">133</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MLP.__init__-134"><a href="#MLP.__init__-134"><span class="linenos">134</span></a>        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="MLP.__init__-135"><a href="#MLP.__init__-135"><span class="linenos">135</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MLP.__init__-136"><a href="#MLP.__init__-136"><span class="linenos">136</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP.__init__-137"><a href="#MLP.__init__-137"><span class="linenos">137</span></a><span class="sd">        Args:</span>
</span><span id="MLP.__init__-138"><a href="#MLP.__init__-138"><span class="linenos">138</span></a><span class="sd">            d_in: the input size.</span>
</span><span id="MLP.__init__-139"><a href="#MLP.__init__-139"><span class="linenos">139</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="MLP.__init__-140"><a href="#MLP.__init__-140"><span class="linenos">140</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="MLP.__init__-141"><a href="#MLP.__init__-141"><span class="linenos">141</span></a><span class="sd">            d_block: the block width.</span>
</span><span id="MLP.__init__-142"><a href="#MLP.__init__-142"><span class="linenos">142</span></a><span class="sd">            dropout: the dropout rate.</span>
</span><span id="MLP.__init__-143"><a href="#MLP.__init__-143"><span class="linenos">143</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP.__init__-144"><a href="#MLP.__init__-144"><span class="linenos">144</span></a>        <span class="k">assert</span> <span class="n">n_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="MLP.__init__-145"><a href="#MLP.__init__-145"><span class="linenos">145</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MLP.__init__-146"><a href="#MLP.__init__-146"><span class="linenos">146</span></a>
</span><span id="MLP.__init__-147"><a href="#MLP.__init__-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="MLP.__init__-148"><a href="#MLP.__init__-148"><span class="linenos">148</span></a>            <span class="p">[</span>
</span><span id="MLP.__init__-149"><a href="#MLP.__init__-149"><span class="linenos">149</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="MLP.__init__-150"><a href="#MLP.__init__-150"><span class="linenos">150</span></a>                    <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="MLP.__init__-151"><a href="#MLP.__init__-151"><span class="linenos">151</span></a>                        <span class="p">[</span>
</span><span id="MLP.__init__-152"><a href="#MLP.__init__-152"><span class="linenos">152</span></a>                            <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="MLP.__init__-153"><a href="#MLP.__init__-153"><span class="linenos">153</span></a>                            <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="MLP.__init__-154"><a href="#MLP.__init__-154"><span class="linenos">154</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)),</span>
</span><span id="MLP.__init__-155"><a href="#MLP.__init__-155"><span class="linenos">155</span></a>                        <span class="p">]</span>
</span><span id="MLP.__init__-156"><a href="#MLP.__init__-156"><span class="linenos">156</span></a>                    <span class="p">)</span>
</span><span id="MLP.__init__-157"><a href="#MLP.__init__-157"><span class="linenos">157</span></a>                <span class="p">)</span>
</span><span id="MLP.__init__-158"><a href="#MLP.__init__-158"><span class="linenos">158</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="MLP.__init__-159"><a href="#MLP.__init__-159"><span class="linenos">159</span></a>            <span class="p">]</span>
</span><span id="MLP.__init__-160"><a href="#MLP.__init__-160"><span class="linenos">160</span></a>        <span class="p">)</span>
</span><span id="MLP.__init__-161"><a href="#MLP.__init__-161"><span class="linenos">161</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="MLP.__init__-162"><a href="#MLP.__init__-162"><span class="linenos">162</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</span><span id="MLP.__init__-163"><a href="#MLP.__init__-163"><span class="linenos">163</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>d_in:</strong>  the input size.</li>
<li><strong>d_out:</strong>  the output size.</li>
<li><strong>n_blocks:</strong>  the number of blocks.</li>
<li><strong>d_block:</strong>  the block width.</li>
<li><strong>dropout:</strong>  the dropout rate.</li>
</ul>
</div>


                            </div>
                            <div id="MLP.blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">blocks</span>

        
    </div>
    <a class="headerlink" href="#MLP.blocks"></a>
    
            <div class="docstring"><p>The blocks.</p>
</div>


                            </div>
                            <div id="MLP.output" class="classattr">
                                <div class="attr variable">
            <span class="name">output</span>

        
    </div>
    <a class="headerlink" href="#MLP.output"></a>
    
            <div class="docstring"><p>The output module.</p>
</div>


                            </div>
                            <div id="MLP.forward" class="classattr">
                                        <input id="MLP.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MLP.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP.forward-165"><a href="#MLP.forward-165"><span class="linenos">165</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MLP.forward-166"><a href="#MLP.forward-166"><span class="linenos">166</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="MLP.forward-167"><a href="#MLP.forward-167"><span class="linenos">167</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="MLP.forward-168"><a href="#MLP.forward-168"><span class="linenos">168</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP.forward-169"><a href="#MLP.forward-169"><span class="linenos">169</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MLP.forward-170"><a href="#MLP.forward-170"><span class="linenos">170</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP.forward-171"><a href="#MLP.forward-171"><span class="linenos">171</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MLP.dump_patches" class="variable">dump_patches</dd>
                <dd id="MLP.training" class="variable">training</dd>
                <dd id="MLP.register_buffer" class="function">register_buffer</dd>
                <dd id="MLP.register_parameter" class="function">register_parameter</dd>
                <dd id="MLP.add_module" class="function">add_module</dd>
                <dd id="MLP.apply" class="function">apply</dd>
                <dd id="MLP.cuda" class="function">cuda</dd>
                <dd id="MLP.xpu" class="function">xpu</dd>
                <dd id="MLP.cpu" class="function">cpu</dd>
                <dd id="MLP.type" class="function">type</dd>
                <dd id="MLP.float" class="function">float</dd>
                <dd id="MLP.double" class="function">double</dd>
                <dd id="MLP.half" class="function">half</dd>
                <dd id="MLP.bfloat16" class="function">bfloat16</dd>
                <dd id="MLP.to" class="function">to</dd>
                <dd id="MLP.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MLP.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MLP.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MLP.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MLP.state_dict" class="function">state_dict</dd>
                <dd id="MLP.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MLP.parameters" class="function">parameters</dd>
                <dd id="MLP.named_parameters" class="function">named_parameters</dd>
                <dd id="MLP.buffers" class="function">buffers</dd>
                <dd id="MLP.named_buffers" class="function">named_buffers</dd>
                <dd id="MLP.children" class="function">children</dd>
                <dd id="MLP.named_children" class="function">named_children</dd>
                <dd id="MLP.modules" class="function">modules</dd>
                <dd id="MLP.named_modules" class="function">named_modules</dd>
                <dd id="MLP.train" class="function">train</dd>
                <dd id="MLP.eval" class="function">eval</dd>
                <dd id="MLP.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MLP.zero_grad" class="function">zero_grad</dd>
                <dd id="MLP.share_memory" class="function">share_memory</dd>
                <dd id="MLP.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="ResNet">
                            <input id="ResNet-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResNet</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="ResNet-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet-174"><a href="#ResNet-174"><span class="linenos">174</span></a><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="ResNet-175"><a href="#ResNet-175"><span class="linenos">175</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The ResNet model from Section 3.2 in the paper.</span>
</span><span id="ResNet-176"><a href="#ResNet-176"><span class="linenos">176</span></a>
</span><span id="ResNet-177"><a href="#ResNet-177"><span class="linenos">177</span></a><span class="sd">    ```</span>
</span><span id="ResNet-178"><a href="#ResNet-178"><span class="linenos">178</span></a><span class="sd">    ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Output -&gt; (out)</span>
</span><span id="ResNet-179"><a href="#ResNet-179"><span class="linenos">179</span></a>
</span><span id="ResNet-180"><a href="#ResNet-180"><span class="linenos">180</span></a><span class="sd">             |-&gt; BatchNorm -&gt; Linear -&gt; ReLU -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt; |</span>
</span><span id="ResNet-181"><a href="#ResNet-181"><span class="linenos">181</span></a><span class="sd">             |                                                                  |</span>
</span><span id="ResNet-182"><a href="#ResNet-182"><span class="linenos">182</span></a><span class="sd">    Block:  (in) ------------------------------------------------------------&gt; Add -&gt; (out)</span>
</span><span id="ResNet-183"><a href="#ResNet-183"><span class="linenos">183</span></a>
</span><span id="ResNet-184"><a href="#ResNet-184"><span class="linenos">184</span></a><span class="sd">    Output: (in) -&gt; BatchNorm -&gt; ReLU -&gt; Linear -&gt; (out)</span>
</span><span id="ResNet-185"><a href="#ResNet-185"><span class="linenos">185</span></a><span class="sd">    ```</span>
</span><span id="ResNet-186"><a href="#ResNet-186"><span class="linenos">186</span></a>
</span><span id="ResNet-187"><a href="#ResNet-187"><span class="linenos">187</span></a><span class="sd">    **Shape**</span>
</span><span id="ResNet-188"><a href="#ResNet-188"><span class="linenos">188</span></a>
</span><span id="ResNet-189"><a href="#ResNet-189"><span class="linenos">189</span></a><span class="sd">    - Input: `(*, d_in)`</span>
</span><span id="ResNet-190"><a href="#ResNet-190"><span class="linenos">190</span></a><span class="sd">    - Output: `(*, d_out or d_block)`</span>
</span><span id="ResNet-191"><a href="#ResNet-191"><span class="linenos">191</span></a>
</span><span id="ResNet-192"><a href="#ResNet-192"><span class="linenos">192</span></a><span class="sd">    **Examples**</span>
</span><span id="ResNet-193"><a href="#ResNet-193"><span class="linenos">193</span></a>
</span><span id="ResNet-194"><a href="#ResNet-194"><span class="linenos">194</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="ResNet-195"><a href="#ResNet-195"><span class="linenos">195</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, 2)</span>
</span><span id="ResNet-196"><a href="#ResNet-196"><span class="linenos">196</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="ResNet-197"><a href="#ResNet-197"><span class="linenos">197</span></a><span class="sd">    &gt;&gt;&gt; m = ResNet(</span>
</span><span id="ResNet-198"><a href="#ResNet-198"><span class="linenos">198</span></a><span class="sd">    ...     d_in=x.shape[1],</span>
</span><span id="ResNet-199"><a href="#ResNet-199"><span class="linenos">199</span></a><span class="sd">    ...     d_out=d_out,</span>
</span><span id="ResNet-200"><a href="#ResNet-200"><span class="linenos">200</span></a><span class="sd">    ...     n_blocks=2,</span>
</span><span id="ResNet-201"><a href="#ResNet-201"><span class="linenos">201</span></a><span class="sd">    ...     d_block=3,</span>
</span><span id="ResNet-202"><a href="#ResNet-202"><span class="linenos">202</span></a><span class="sd">    ...     d_hidden=4,</span>
</span><span id="ResNet-203"><a href="#ResNet-203"><span class="linenos">203</span></a><span class="sd">    ...     d_hidden_multiplier=None,</span>
</span><span id="ResNet-204"><a href="#ResNet-204"><span class="linenos">204</span></a><span class="sd">    ...     dropout1=0.25,</span>
</span><span id="ResNet-205"><a href="#ResNet-205"><span class="linenos">205</span></a><span class="sd">    ...     dropout2=0.0,</span>
</span><span id="ResNet-206"><a href="#ResNet-206"><span class="linenos">206</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="ResNet-207"><a href="#ResNet-207"><span class="linenos">207</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, d_out)</span>
</span><span id="ResNet-208"><a href="#ResNet-208"><span class="linenos">208</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ResNet-209"><a href="#ResNet-209"><span class="linenos">209</span></a>
</span><span id="ResNet-210"><a href="#ResNet-210"><span class="linenos">210</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet-211"><a href="#ResNet-211"><span class="linenos">211</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet-212"><a href="#ResNet-212"><span class="linenos">212</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="ResNet-213"><a href="#ResNet-213"><span class="linenos">213</span></a>        <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet-214"><a href="#ResNet-214"><span class="linenos">214</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="ResNet-215"><a href="#ResNet-215"><span class="linenos">215</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet-216"><a href="#ResNet-216"><span class="linenos">216</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet-217"><a href="#ResNet-217"><span class="linenos">217</span></a>        <span class="n">d_hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="ResNet-218"><a href="#ResNet-218"><span class="linenos">218</span></a>        <span class="n">d_hidden_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="ResNet-219"><a href="#ResNet-219"><span class="linenos">219</span></a>        <span class="n">dropout1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="ResNet-220"><a href="#ResNet-220"><span class="linenos">220</span></a>        <span class="n">dropout2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="ResNet-221"><a href="#ResNet-221"><span class="linenos">221</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet-222"><a href="#ResNet-222"><span class="linenos">222</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="ResNet-223"><a href="#ResNet-223"><span class="linenos">223</span></a><span class="sd">        Args:</span>
</span><span id="ResNet-224"><a href="#ResNet-224"><span class="linenos">224</span></a><span class="sd">            d_in: the input size.</span>
</span><span id="ResNet-225"><a href="#ResNet-225"><span class="linenos">225</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="ResNet-226"><a href="#ResNet-226"><span class="linenos">226</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="ResNet-227"><a href="#ResNet-227"><span class="linenos">227</span></a><span class="sd">            d_block: the &quot;main&quot; block width (i.e. its input and output size).</span>
</span><span id="ResNet-228"><a href="#ResNet-228"><span class="linenos">228</span></a><span class="sd">            d_hidden: the block&#39;s hidden width.</span>
</span><span id="ResNet-229"><a href="#ResNet-229"><span class="linenos">229</span></a><span class="sd">            d_hidden_multipler: the alternative way to set `d_hidden` as</span>
</span><span id="ResNet-230"><a href="#ResNet-230"><span class="linenos">230</span></a><span class="sd">                `int(d_block * d_hidden_multipler)`.</span>
</span><span id="ResNet-231"><a href="#ResNet-231"><span class="linenos">231</span></a><span class="sd">            dropout1: the hidden dropout rate.</span>
</span><span id="ResNet-232"><a href="#ResNet-232"><span class="linenos">232</span></a><span class="sd">            dropout2: the residual dropout rate.</span>
</span><span id="ResNet-233"><a href="#ResNet-233"><span class="linenos">233</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet-234"><a href="#ResNet-234"><span class="linenos">234</span></a>        <span class="k">assert</span> <span class="n">n_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="ResNet-235"><a href="#ResNet-235"><span class="linenos">235</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">d_hidden_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="ResNet-236"><a href="#ResNet-236"><span class="linenos">236</span></a>        <span class="k">if</span> <span class="n">d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet-237"><a href="#ResNet-237"><span class="linenos">237</span></a>            <span class="n">d_hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_block</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">d_hidden_multiplier</span><span class="p">))</span>
</span><span id="ResNet-238"><a href="#ResNet-238"><span class="linenos">238</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ResNet-239"><a href="#ResNet-239"><span class="linenos">239</span></a>
</span><span id="ResNet-240"><a href="#ResNet-240"><span class="linenos">240</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span>
</span><span id="ResNet-241"><a href="#ResNet-241"><span class="linenos">241</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The first linear layer (applied before the main blocks) which</span>
</span><span id="ResNet-242"><a href="#ResNet-242"><span class="linenos">242</span></a><span class="sd">        projects the input from `d_in` to `d_block`.&quot;&quot;&quot;</span>
</span><span id="ResNet-243"><a href="#ResNet-243"><span class="linenos">243</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="ResNet-244"><a href="#ResNet-244"><span class="linenos">244</span></a>            <span class="p">[</span>
</span><span id="ResNet-245"><a href="#ResNet-245"><span class="linenos">245</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="ResNet-246"><a href="#ResNet-246"><span class="linenos">246</span></a>                    <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="ResNet-247"><a href="#ResNet-247"><span class="linenos">247</span></a>                        <span class="p">[</span>
</span><span id="ResNet-248"><a href="#ResNet-248"><span class="linenos">248</span></a>                            <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="ResNet-249"><a href="#ResNet-249"><span class="linenos">249</span></a>                            <span class="p">(</span><span class="s1">&#39;linear1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">)),</span>
</span><span id="ResNet-250"><a href="#ResNet-250"><span class="linenos">250</span></a>                            <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="ResNet-251"><a href="#ResNet-251"><span class="linenos">251</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout1</span><span class="p">)),</span>
</span><span id="ResNet-252"><a href="#ResNet-252"><span class="linenos">252</span></a>                            <span class="p">(</span><span class="s1">&#39;linear2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="ResNet-253"><a href="#ResNet-253"><span class="linenos">253</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout2</span><span class="p">)),</span>
</span><span id="ResNet-254"><a href="#ResNet-254"><span class="linenos">254</span></a>                        <span class="p">]</span>
</span><span id="ResNet-255"><a href="#ResNet-255"><span class="linenos">255</span></a>                    <span class="p">)</span>
</span><span id="ResNet-256"><a href="#ResNet-256"><span class="linenos">256</span></a>                <span class="p">)</span>
</span><span id="ResNet-257"><a href="#ResNet-257"><span class="linenos">257</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="ResNet-258"><a href="#ResNet-258"><span class="linenos">258</span></a>            <span class="p">]</span>
</span><span id="ResNet-259"><a href="#ResNet-259"><span class="linenos">259</span></a>        <span class="p">)</span>
</span><span id="ResNet-260"><a href="#ResNet-260"><span class="linenos">260</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="ResNet-261"><a href="#ResNet-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNet-262"><a href="#ResNet-262"><span class="linenos">262</span></a>            <span class="kc">None</span>
</span><span id="ResNet-263"><a href="#ResNet-263"><span class="linenos">263</span></a>            <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="ResNet-264"><a href="#ResNet-264"><span class="linenos">264</span></a>            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="ResNet-265"><a href="#ResNet-265"><span class="linenos">265</span></a>                <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="ResNet-266"><a href="#ResNet-266"><span class="linenos">266</span></a>                    <span class="p">[</span>
</span><span id="ResNet-267"><a href="#ResNet-267"><span class="linenos">267</span></a>                        <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="ResNet-268"><a href="#ResNet-268"><span class="linenos">268</span></a>                        <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="ResNet-269"><a href="#ResNet-269"><span class="linenos">269</span></a>                        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)),</span>
</span><span id="ResNet-270"><a href="#ResNet-270"><span class="linenos">270</span></a>                    <span class="p">]</span>
</span><span id="ResNet-271"><a href="#ResNet-271"><span class="linenos">271</span></a>                <span class="p">)</span>
</span><span id="ResNet-272"><a href="#ResNet-272"><span class="linenos">272</span></a>            <span class="p">)</span>
</span><span id="ResNet-273"><a href="#ResNet-273"><span class="linenos">273</span></a>        <span class="p">)</span>
</span><span id="ResNet-274"><a href="#ResNet-274"><span class="linenos">274</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span><span id="ResNet-275"><a href="#ResNet-275"><span class="linenos">275</span></a>
</span><span id="ResNet-276"><a href="#ResNet-276"><span class="linenos">276</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="ResNet-277"><a href="#ResNet-277"><span class="linenos">277</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="ResNet-278"><a href="#ResNet-278"><span class="linenos">278</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNet-279"><a href="#ResNet-279"><span class="linenos">279</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="ResNet-280"><a href="#ResNet-280"><span class="linenos">280</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNet-281"><a href="#ResNet-281"><span class="linenos">281</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet-282"><a href="#ResNet-282"><span class="linenos">282</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNet-283"><a href="#ResNet-283"><span class="linenos">283</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>The ResNet model from Section 3.2 in the paper.</p>

<pre><code>ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Output -&gt; (out)

         |-&gt; BatchNorm -&gt; Linear -&gt; ReLU -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt; |
         |                                                                  |
Block:  (in) ------------------------------------------------------------&gt; Add -&gt; (out)

Output: (in) -&gt; BatchNorm -&gt; ReLU -&gt; Linear -&gt; (out)
</code></pre>

<p><strong>Shape</strong></p>

<ul>
<li>Input: <code>(*, d_in)</code></li>
<li>Output: <code>(*, d_out or d_block)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_out</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">ResNet</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">d_in</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">d_out</span><span class="o">=</span><span class="n">d_out</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_block</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_hidden_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">dropout1</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">dropout2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</code></pre>
</div>
</div>


                            <div id="ResNet.__init__" class="classattr">
                                        <input id="ResNet.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ResNet</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="o">*</span>,</span><span class="param">	<span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_out</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_hidden</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">d_hidden_multiplier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">dropout1</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">dropout2</span><span class="p">:</span> <span class="nb">float</span></span>)</span>

                <label class="view-source-button" for="ResNet.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet.__init__-210"><a href="#ResNet.__init__-210"><span class="linenos">210</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="ResNet.__init__-211"><a href="#ResNet.__init__-211"><span class="linenos">211</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ResNet.__init__-212"><a href="#ResNet.__init__-212"><span class="linenos">212</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="ResNet.__init__-213"><a href="#ResNet.__init__-213"><span class="linenos">213</span></a>        <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet.__init__-214"><a href="#ResNet.__init__-214"><span class="linenos">214</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="ResNet.__init__-215"><a href="#ResNet.__init__-215"><span class="linenos">215</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet.__init__-216"><a href="#ResNet.__init__-216"><span class="linenos">216</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="ResNet.__init__-217"><a href="#ResNet.__init__-217"><span class="linenos">217</span></a>        <span class="n">d_hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="ResNet.__init__-218"><a href="#ResNet.__init__-218"><span class="linenos">218</span></a>        <span class="n">d_hidden_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="ResNet.__init__-219"><a href="#ResNet.__init__-219"><span class="linenos">219</span></a>        <span class="n">dropout1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="ResNet.__init__-220"><a href="#ResNet.__init__-220"><span class="linenos">220</span></a>        <span class="n">dropout2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="ResNet.__init__-221"><a href="#ResNet.__init__-221"><span class="linenos">221</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet.__init__-222"><a href="#ResNet.__init__-222"><span class="linenos">222</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="ResNet.__init__-223"><a href="#ResNet.__init__-223"><span class="linenos">223</span></a><span class="sd">        Args:</span>
</span><span id="ResNet.__init__-224"><a href="#ResNet.__init__-224"><span class="linenos">224</span></a><span class="sd">            d_in: the input size.</span>
</span><span id="ResNet.__init__-225"><a href="#ResNet.__init__-225"><span class="linenos">225</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="ResNet.__init__-226"><a href="#ResNet.__init__-226"><span class="linenos">226</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="ResNet.__init__-227"><a href="#ResNet.__init__-227"><span class="linenos">227</span></a><span class="sd">            d_block: the &quot;main&quot; block width (i.e. its input and output size).</span>
</span><span id="ResNet.__init__-228"><a href="#ResNet.__init__-228"><span class="linenos">228</span></a><span class="sd">            d_hidden: the block&#39;s hidden width.</span>
</span><span id="ResNet.__init__-229"><a href="#ResNet.__init__-229"><span class="linenos">229</span></a><span class="sd">            d_hidden_multipler: the alternative way to set `d_hidden` as</span>
</span><span id="ResNet.__init__-230"><a href="#ResNet.__init__-230"><span class="linenos">230</span></a><span class="sd">                `int(d_block * d_hidden_multipler)`.</span>
</span><span id="ResNet.__init__-231"><a href="#ResNet.__init__-231"><span class="linenos">231</span></a><span class="sd">            dropout1: the hidden dropout rate.</span>
</span><span id="ResNet.__init__-232"><a href="#ResNet.__init__-232"><span class="linenos">232</span></a><span class="sd">            dropout2: the residual dropout rate.</span>
</span><span id="ResNet.__init__-233"><a href="#ResNet.__init__-233"><span class="linenos">233</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ResNet.__init__-234"><a href="#ResNet.__init__-234"><span class="linenos">234</span></a>        <span class="k">assert</span> <span class="n">n_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="ResNet.__init__-235"><a href="#ResNet.__init__-235"><span class="linenos">235</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">d_hidden_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="ResNet.__init__-236"><a href="#ResNet.__init__-236"><span class="linenos">236</span></a>        <span class="k">if</span> <span class="n">d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet.__init__-237"><a href="#ResNet.__init__-237"><span class="linenos">237</span></a>            <span class="n">d_hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_block</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">d_hidden_multiplier</span><span class="p">))</span>
</span><span id="ResNet.__init__-238"><a href="#ResNet.__init__-238"><span class="linenos">238</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="ResNet.__init__-239"><a href="#ResNet.__init__-239"><span class="linenos">239</span></a>
</span><span id="ResNet.__init__-240"><a href="#ResNet.__init__-240"><span class="linenos">240</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span>
</span><span id="ResNet.__init__-241"><a href="#ResNet.__init__-241"><span class="linenos">241</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The first linear layer (applied before the main blocks) which</span>
</span><span id="ResNet.__init__-242"><a href="#ResNet.__init__-242"><span class="linenos">242</span></a><span class="sd">        projects the input from `d_in` to `d_block`.&quot;&quot;&quot;</span>
</span><span id="ResNet.__init__-243"><a href="#ResNet.__init__-243"><span class="linenos">243</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="ResNet.__init__-244"><a href="#ResNet.__init__-244"><span class="linenos">244</span></a>            <span class="p">[</span>
</span><span id="ResNet.__init__-245"><a href="#ResNet.__init__-245"><span class="linenos">245</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="ResNet.__init__-246"><a href="#ResNet.__init__-246"><span class="linenos">246</span></a>                    <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="ResNet.__init__-247"><a href="#ResNet.__init__-247"><span class="linenos">247</span></a>                        <span class="p">[</span>
</span><span id="ResNet.__init__-248"><a href="#ResNet.__init__-248"><span class="linenos">248</span></a>                            <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="ResNet.__init__-249"><a href="#ResNet.__init__-249"><span class="linenos">249</span></a>                            <span class="p">(</span><span class="s1">&#39;linear1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">)),</span>
</span><span id="ResNet.__init__-250"><a href="#ResNet.__init__-250"><span class="linenos">250</span></a>                            <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="ResNet.__init__-251"><a href="#ResNet.__init__-251"><span class="linenos">251</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout1</span><span class="p">)),</span>
</span><span id="ResNet.__init__-252"><a href="#ResNet.__init__-252"><span class="linenos">252</span></a>                            <span class="p">(</span><span class="s1">&#39;linear2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="ResNet.__init__-253"><a href="#ResNet.__init__-253"><span class="linenos">253</span></a>                            <span class="p">(</span><span class="s1">&#39;dropout2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout2</span><span class="p">)),</span>
</span><span id="ResNet.__init__-254"><a href="#ResNet.__init__-254"><span class="linenos">254</span></a>                        <span class="p">]</span>
</span><span id="ResNet.__init__-255"><a href="#ResNet.__init__-255"><span class="linenos">255</span></a>                    <span class="p">)</span>
</span><span id="ResNet.__init__-256"><a href="#ResNet.__init__-256"><span class="linenos">256</span></a>                <span class="p">)</span>
</span><span id="ResNet.__init__-257"><a href="#ResNet.__init__-257"><span class="linenos">257</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="ResNet.__init__-258"><a href="#ResNet.__init__-258"><span class="linenos">258</span></a>            <span class="p">]</span>
</span><span id="ResNet.__init__-259"><a href="#ResNet.__init__-259"><span class="linenos">259</span></a>        <span class="p">)</span>
</span><span id="ResNet.__init__-260"><a href="#ResNet.__init__-260"><span class="linenos">260</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="ResNet.__init__-261"><a href="#ResNet.__init__-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ResNet.__init__-262"><a href="#ResNet.__init__-262"><span class="linenos">262</span></a>            <span class="kc">None</span>
</span><span id="ResNet.__init__-263"><a href="#ResNet.__init__-263"><span class="linenos">263</span></a>            <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="ResNet.__init__-264"><a href="#ResNet.__init__-264"><span class="linenos">264</span></a>            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="ResNet.__init__-265"><a href="#ResNet.__init__-265"><span class="linenos">265</span></a>                <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="ResNet.__init__-266"><a href="#ResNet.__init__-266"><span class="linenos">266</span></a>                    <span class="p">[</span>
</span><span id="ResNet.__init__-267"><a href="#ResNet.__init__-267"><span class="linenos">267</span></a>                        <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="ResNet.__init__-268"><a href="#ResNet.__init__-268"><span class="linenos">268</span></a>                        <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="ResNet.__init__-269"><a href="#ResNet.__init__-269"><span class="linenos">269</span></a>                        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)),</span>
</span><span id="ResNet.__init__-270"><a href="#ResNet.__init__-270"><span class="linenos">270</span></a>                    <span class="p">]</span>
</span><span id="ResNet.__init__-271"><a href="#ResNet.__init__-271"><span class="linenos">271</span></a>                <span class="p">)</span>
</span><span id="ResNet.__init__-272"><a href="#ResNet.__init__-272"><span class="linenos">272</span></a>            <span class="p">)</span>
</span><span id="ResNet.__init__-273"><a href="#ResNet.__init__-273"><span class="linenos">273</span></a>        <span class="p">)</span>
</span><span id="ResNet.__init__-274"><a href="#ResNet.__init__-274"><span class="linenos">274</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>d_in:</strong>  the input size.</li>
<li><strong>d_out:</strong>  the output size.</li>
<li><strong>n_blocks:</strong>  the number of blocks.</li>
<li><strong>d_block:</strong>  the "main" block width (i.e. its input and output size).</li>
<li><strong>d_hidden:</strong>  the block's hidden width.</li>
<li><strong>d_hidden_multipler:</strong>  the alternative way to set <code>d_hidden</code> as
<code>int(d_block * d_hidden_multipler)</code>.</li>
<li><strong>dropout1:</strong>  the hidden dropout rate.</li>
<li><strong>dropout2:</strong>  the residual dropout rate.</li>
</ul>
</div>


                            </div>
                            <div id="ResNet.input_projection" class="classattr">
                                <div class="attr variable">
            <span class="name">input_projection</span>

        
    </div>
    <a class="headerlink" href="#ResNet.input_projection"></a>
    
            <div class="docstring"><p>The first linear layer (applied before the main blocks) which
projects the input from <code>d_in</code> to <code>d_block</code>.</p>
</div>


                            </div>
                            <div id="ResNet.blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">blocks</span>

        
    </div>
    <a class="headerlink" href="#ResNet.blocks"></a>
    
            <div class="docstring"><p>The blocks.</p>
</div>


                            </div>
                            <div id="ResNet.output" class="classattr">
                                <div class="attr variable">
            <span class="name">output</span>

        
    </div>
    <a class="headerlink" href="#ResNet.output"></a>
    
            <div class="docstring"><p>The output module.</p>
</div>


                            </div>
                            <div id="ResNet.forward" class="classattr">
                                        <input id="ResNet.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="ResNet.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResNet.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResNet.forward-276"><a href="#ResNet.forward-276"><span class="linenos">276</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="ResNet.forward-277"><a href="#ResNet.forward-277"><span class="linenos">277</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="ResNet.forward-278"><a href="#ResNet.forward-278"><span class="linenos">278</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNet.forward-279"><a href="#ResNet.forward-279"><span class="linenos">279</span></a>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="ResNet.forward-280"><a href="#ResNet.forward-280"><span class="linenos">280</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNet.forward-281"><a href="#ResNet.forward-281"><span class="linenos">281</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ResNet.forward-282"><a href="#ResNet.forward-282"><span class="linenos">282</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="ResNet.forward-283"><a href="#ResNet.forward-283"><span class="linenos">283</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="ResNet.dump_patches" class="variable">dump_patches</dd>
                <dd id="ResNet.training" class="variable">training</dd>
                <dd id="ResNet.register_buffer" class="function">register_buffer</dd>
                <dd id="ResNet.register_parameter" class="function">register_parameter</dd>
                <dd id="ResNet.add_module" class="function">add_module</dd>
                <dd id="ResNet.apply" class="function">apply</dd>
                <dd id="ResNet.cuda" class="function">cuda</dd>
                <dd id="ResNet.xpu" class="function">xpu</dd>
                <dd id="ResNet.cpu" class="function">cpu</dd>
                <dd id="ResNet.type" class="function">type</dd>
                <dd id="ResNet.float" class="function">float</dd>
                <dd id="ResNet.double" class="function">double</dd>
                <dd id="ResNet.half" class="function">half</dd>
                <dd id="ResNet.bfloat16" class="function">bfloat16</dd>
                <dd id="ResNet.to" class="function">to</dd>
                <dd id="ResNet.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="ResNet.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="ResNet.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="ResNet.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="ResNet.state_dict" class="function">state_dict</dd>
                <dd id="ResNet.load_state_dict" class="function">load_state_dict</dd>
                <dd id="ResNet.parameters" class="function">parameters</dd>
                <dd id="ResNet.named_parameters" class="function">named_parameters</dd>
                <dd id="ResNet.buffers" class="function">buffers</dd>
                <dd id="ResNet.named_buffers" class="function">named_buffers</dd>
                <dd id="ResNet.children" class="function">children</dd>
                <dd id="ResNet.named_children" class="function">named_children</dd>
                <dd id="ResNet.modules" class="function">modules</dd>
                <dd id="ResNet.named_modules" class="function">named_modules</dd>
                <dd id="ResNet.train" class="function">train</dd>
                <dd id="ResNet.eval" class="function">eval</dd>
                <dd id="ResNet.requires_grad_" class="function">requires_grad_</dd>
                <dd id="ResNet.zero_grad" class="function">zero_grad</dd>
                <dd id="ResNet.share_memory" class="function">share_memory</dd>
                <dd id="ResNet.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="LinearEmbeddings">
                            <input id="LinearEmbeddings-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">LinearEmbeddings</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="LinearEmbeddings-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbeddings"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbeddings-323"><a href="#LinearEmbeddings-323"><span class="linenos">323</span></a><span class="k">class</span> <span class="nc">LinearEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="LinearEmbeddings-324"><a href="#LinearEmbeddings-324"><span class="linenos">324</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear embeddings for continuous features.</span>
</span><span id="LinearEmbeddings-325"><a href="#LinearEmbeddings-325"><span class="linenos">325</span></a>
</span><span id="LinearEmbeddings-326"><a href="#LinearEmbeddings-326"><span class="linenos">326</span></a><span class="sd">    For the illustration, see `FTTransformer`.</span>
</span><span id="LinearEmbeddings-327"><a href="#LinearEmbeddings-327"><span class="linenos">327</span></a>
</span><span id="LinearEmbeddings-328"><a href="#LinearEmbeddings-328"><span class="linenos">328</span></a><span class="sd">    **Shape**</span>
</span><span id="LinearEmbeddings-329"><a href="#LinearEmbeddings-329"><span class="linenos">329</span></a>
</span><span id="LinearEmbeddings-330"><a href="#LinearEmbeddings-330"><span class="linenos">330</span></a><span class="sd">    - Input: `(*, n_features)`</span>
</span><span id="LinearEmbeddings-331"><a href="#LinearEmbeddings-331"><span class="linenos">331</span></a><span class="sd">    - Output: `(*, n_features, d_embedding)`</span>
</span><span id="LinearEmbeddings-332"><a href="#LinearEmbeddings-332"><span class="linenos">332</span></a>
</span><span id="LinearEmbeddings-333"><a href="#LinearEmbeddings-333"><span class="linenos">333</span></a><span class="sd">    **Examples**</span>
</span><span id="LinearEmbeddings-334"><a href="#LinearEmbeddings-334"><span class="linenos">334</span></a>
</span><span id="LinearEmbeddings-335"><a href="#LinearEmbeddings-335"><span class="linenos">335</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="LinearEmbeddings-336"><a href="#LinearEmbeddings-336"><span class="linenos">336</span></a><span class="sd">    &gt;&gt;&gt; n_cont_features = 3</span>
</span><span id="LinearEmbeddings-337"><a href="#LinearEmbeddings-337"><span class="linenos">337</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, n_cont_features)</span>
</span><span id="LinearEmbeddings-338"><a href="#LinearEmbeddings-338"><span class="linenos">338</span></a><span class="sd">    &gt;&gt;&gt; d_embedding = 4</span>
</span><span id="LinearEmbeddings-339"><a href="#LinearEmbeddings-339"><span class="linenos">339</span></a><span class="sd">    &gt;&gt;&gt; m = LinearEmbeddings(n_cont_features, d_embedding)</span>
</span><span id="LinearEmbeddings-340"><a href="#LinearEmbeddings-340"><span class="linenos">340</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, n_cont_features, d_embedding)</span>
</span><span id="LinearEmbeddings-341"><a href="#LinearEmbeddings-341"><span class="linenos">341</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-342"><a href="#LinearEmbeddings-342"><span class="linenos">342</span></a>
</span><span id="LinearEmbeddings-343"><a href="#LinearEmbeddings-343"><span class="linenos">343</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearEmbeddings-344"><a href="#LinearEmbeddings-344"><span class="linenos">344</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-345"><a href="#LinearEmbeddings-345"><span class="linenos">345</span></a><span class="sd">        Args:</span>
</span><span id="LinearEmbeddings-346"><a href="#LinearEmbeddings-346"><span class="linenos">346</span></a><span class="sd">            n_features: the number of continous features</span>
</span><span id="LinearEmbeddings-347"><a href="#LinearEmbeddings-347"><span class="linenos">347</span></a><span class="sd">            d_embedding: the embedding size</span>
</span><span id="LinearEmbeddings-348"><a href="#LinearEmbeddings-348"><span class="linenos">348</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-349"><a href="#LinearEmbeddings-349"><span class="linenos">349</span></a>        <span class="k">assert</span> <span class="n">n_features</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="LinearEmbeddings-350"><a href="#LinearEmbeddings-350"><span class="linenos">350</span></a>        <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="LinearEmbeddings-351"><a href="#LinearEmbeddings-351"><span class="linenos">351</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="LinearEmbeddings-352"><a href="#LinearEmbeddings-352"><span class="linenos">352</span></a>
</span><span id="LinearEmbeddings-353"><a href="#LinearEmbeddings-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
</span><span id="LinearEmbeddings-354"><a href="#LinearEmbeddings-354"><span class="linenos">354</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The weight.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-355"><a href="#LinearEmbeddings-355"><span class="linenos">355</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
</span><span id="LinearEmbeddings-356"><a href="#LinearEmbeddings-356"><span class="linenos">356</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The bias.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-357"><a href="#LinearEmbeddings-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="LinearEmbeddings-358"><a href="#LinearEmbeddings-358"><span class="linenos">358</span></a>
</span><span id="LinearEmbeddings-359"><a href="#LinearEmbeddings-359"><span class="linenos">359</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearEmbeddings-360"><a href="#LinearEmbeddings-360"><span class="linenos">360</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-361"><a href="#LinearEmbeddings-361"><span class="linenos">361</span></a>        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
</span><span id="LinearEmbeddings-362"><a href="#LinearEmbeddings-362"><span class="linenos">362</span></a>            <span class="k">if</span> <span class="n">parameter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearEmbeddings-363"><a href="#LinearEmbeddings-363"><span class="linenos">363</span></a>                <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span><span id="LinearEmbeddings-364"><a href="#LinearEmbeddings-364"><span class="linenos">364</span></a>
</span><span id="LinearEmbeddings-365"><a href="#LinearEmbeddings-365"><span class="linenos">365</span></a>    <span class="nd">@property</span>
</span><span id="LinearEmbeddings-366"><a href="#LinearEmbeddings-366"><span class="linenos">366</span></a>    <span class="k">def</span> <span class="nf">n_features</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="LinearEmbeddings-367"><a href="#LinearEmbeddings-367"><span class="linenos">367</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The number of features.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-368"><a href="#LinearEmbeddings-368"><span class="linenos">368</span></a>        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span><span id="LinearEmbeddings-369"><a href="#LinearEmbeddings-369"><span class="linenos">369</span></a>
</span><span id="LinearEmbeddings-370"><a href="#LinearEmbeddings-370"><span class="linenos">370</span></a>    <span class="nd">@property</span>
</span><span id="LinearEmbeddings-371"><a href="#LinearEmbeddings-371"><span class="linenos">371</span></a>    <span class="k">def</span> <span class="nf">d_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="LinearEmbeddings-372"><a href="#LinearEmbeddings-372"><span class="linenos">372</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embedding size.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-373"><a href="#LinearEmbeddings-373"><span class="linenos">373</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="LinearEmbeddings-374"><a href="#LinearEmbeddings-374"><span class="linenos">374</span></a>
</span><span id="LinearEmbeddings-375"><a href="#LinearEmbeddings-375"><span class="linenos">375</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="LinearEmbeddings-376"><a href="#LinearEmbeddings-376"><span class="linenos">376</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings-377"><a href="#LinearEmbeddings-377"><span class="linenos">377</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
</span><span id="LinearEmbeddings-378"><a href="#LinearEmbeddings-378"><span class="linenos">378</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span><span id="LinearEmbeddings-379"><a href="#LinearEmbeddings-379"><span class="linenos">379</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="LinearEmbeddings-380"><a href="#LinearEmbeddings-380"><span class="linenos">380</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Linear embeddings for continuous features.</p>

<p>For the illustration, see <code><a href="#FTTransformer">FTTransformer</a></code>.</p>

<p><strong>Shape</strong></p>

<ul>
<li>Input: <code>(*, n_features)</code></li>
<li>Output: <code>(*, n_features, d_embedding)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_cont_features</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_cont_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_embedding</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">LinearEmbeddings</span><span class="p">(</span><span class="n">n_cont_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_cont_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</code></pre>
</div>
</div>


                            <div id="LinearEmbeddings.__init__" class="classattr">
                                        <input id="LinearEmbeddings.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">LinearEmbeddings</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">n_features</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

                <label class="view-source-button" for="LinearEmbeddings.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbeddings.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbeddings.__init__-343"><a href="#LinearEmbeddings.__init__-343"><span class="linenos">343</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearEmbeddings.__init__-344"><a href="#LinearEmbeddings.__init__-344"><span class="linenos">344</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings.__init__-345"><a href="#LinearEmbeddings.__init__-345"><span class="linenos">345</span></a><span class="sd">        Args:</span>
</span><span id="LinearEmbeddings.__init__-346"><a href="#LinearEmbeddings.__init__-346"><span class="linenos">346</span></a><span class="sd">            n_features: the number of continous features</span>
</span><span id="LinearEmbeddings.__init__-347"><a href="#LinearEmbeddings.__init__-347"><span class="linenos">347</span></a><span class="sd">            d_embedding: the embedding size</span>
</span><span id="LinearEmbeddings.__init__-348"><a href="#LinearEmbeddings.__init__-348"><span class="linenos">348</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings.__init__-349"><a href="#LinearEmbeddings.__init__-349"><span class="linenos">349</span></a>        <span class="k">assert</span> <span class="n">n_features</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="LinearEmbeddings.__init__-350"><a href="#LinearEmbeddings.__init__-350"><span class="linenos">350</span></a>        <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="LinearEmbeddings.__init__-351"><a href="#LinearEmbeddings.__init__-351"><span class="linenos">351</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="LinearEmbeddings.__init__-352"><a href="#LinearEmbeddings.__init__-352"><span class="linenos">352</span></a>
</span><span id="LinearEmbeddings.__init__-353"><a href="#LinearEmbeddings.__init__-353"><span class="linenos">353</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
</span><span id="LinearEmbeddings.__init__-354"><a href="#LinearEmbeddings.__init__-354"><span class="linenos">354</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The weight.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings.__init__-355"><a href="#LinearEmbeddings.__init__-355"><span class="linenos">355</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
</span><span id="LinearEmbeddings.__init__-356"><a href="#LinearEmbeddings.__init__-356"><span class="linenos">356</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The bias.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings.__init__-357"><a href="#LinearEmbeddings.__init__-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>n_features:</strong>  the number of continous features</li>
<li><strong>d_embedding:</strong>  the embedding size</li>
</ul>
</div>


                            </div>
                            <div id="LinearEmbeddings.weight" class="classattr">
                                <div class="attr variable">
            <span class="name">weight</span>

        
    </div>
    <a class="headerlink" href="#LinearEmbeddings.weight"></a>
    
            <div class="docstring"><p>The weight.</p>
</div>


                            </div>
                            <div id="LinearEmbeddings.bias" class="classattr">
                                <div class="attr variable">
            <span class="name">bias</span>

        
    </div>
    <a class="headerlink" href="#LinearEmbeddings.bias"></a>
    
            <div class="docstring"><p>The bias.</p>
</div>


                            </div>
                            <div id="LinearEmbeddings.reset_parameters" class="classattr">
                                        <input id="LinearEmbeddings.reset_parameters-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">reset_parameters</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="LinearEmbeddings.reset_parameters-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbeddings.reset_parameters"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbeddings.reset_parameters-359"><a href="#LinearEmbeddings.reset_parameters-359"><span class="linenos">359</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearEmbeddings.reset_parameters-360"><a href="#LinearEmbeddings.reset_parameters-360"><span class="linenos">360</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings.reset_parameters-361"><a href="#LinearEmbeddings.reset_parameters-361"><span class="linenos">361</span></a>        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
</span><span id="LinearEmbeddings.reset_parameters-362"><a href="#LinearEmbeddings.reset_parameters-362"><span class="linenos">362</span></a>            <span class="k">if</span> <span class="n">parameter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="LinearEmbeddings.reset_parameters-363"><a href="#LinearEmbeddings.reset_parameters-363"><span class="linenos">363</span></a>                <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Reinitialize all parameters.</p>
</div>


                            </div>
                            <div id="LinearEmbeddings.n_features" class="classattr">
                                <div class="attr variable">
            <span class="name">n_features</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#LinearEmbeddings.n_features"></a>
    
            <div class="docstring"><p>The number of features.</p>
</div>


                            </div>
                            <div id="LinearEmbeddings.d_embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">d_embedding</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#LinearEmbeddings.d_embedding"></a>
    
            <div class="docstring"><p>The embedding size.</p>
</div>


                            </div>
                            <div id="LinearEmbeddings.forward" class="classattr">
                                        <input id="LinearEmbeddings.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="LinearEmbeddings.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LinearEmbeddings.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LinearEmbeddings.forward-375"><a href="#LinearEmbeddings.forward-375"><span class="linenos">375</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="LinearEmbeddings.forward-376"><a href="#LinearEmbeddings.forward-376"><span class="linenos">376</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="LinearEmbeddings.forward-377"><a href="#LinearEmbeddings.forward-377"><span class="linenos">377</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
</span><span id="LinearEmbeddings.forward-378"><a href="#LinearEmbeddings.forward-378"><span class="linenos">378</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span><span id="LinearEmbeddings.forward-379"><a href="#LinearEmbeddings.forward-379"><span class="linenos">379</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="LinearEmbeddings.forward-380"><a href="#LinearEmbeddings.forward-380"><span class="linenos">380</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="LinearEmbeddings.dump_patches" class="variable">dump_patches</dd>
                <dd id="LinearEmbeddings.training" class="variable">training</dd>
                <dd id="LinearEmbeddings.register_buffer" class="function">register_buffer</dd>
                <dd id="LinearEmbeddings.register_parameter" class="function">register_parameter</dd>
                <dd id="LinearEmbeddings.add_module" class="function">add_module</dd>
                <dd id="LinearEmbeddings.apply" class="function">apply</dd>
                <dd id="LinearEmbeddings.cuda" class="function">cuda</dd>
                <dd id="LinearEmbeddings.xpu" class="function">xpu</dd>
                <dd id="LinearEmbeddings.cpu" class="function">cpu</dd>
                <dd id="LinearEmbeddings.type" class="function">type</dd>
                <dd id="LinearEmbeddings.float" class="function">float</dd>
                <dd id="LinearEmbeddings.double" class="function">double</dd>
                <dd id="LinearEmbeddings.half" class="function">half</dd>
                <dd id="LinearEmbeddings.bfloat16" class="function">bfloat16</dd>
                <dd id="LinearEmbeddings.to" class="function">to</dd>
                <dd id="LinearEmbeddings.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="LinearEmbeddings.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="LinearEmbeddings.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="LinearEmbeddings.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="LinearEmbeddings.state_dict" class="function">state_dict</dd>
                <dd id="LinearEmbeddings.load_state_dict" class="function">load_state_dict</dd>
                <dd id="LinearEmbeddings.parameters" class="function">parameters</dd>
                <dd id="LinearEmbeddings.named_parameters" class="function">named_parameters</dd>
                <dd id="LinearEmbeddings.buffers" class="function">buffers</dd>
                <dd id="LinearEmbeddings.named_buffers" class="function">named_buffers</dd>
                <dd id="LinearEmbeddings.children" class="function">children</dd>
                <dd id="LinearEmbeddings.named_children" class="function">named_children</dd>
                <dd id="LinearEmbeddings.modules" class="function">modules</dd>
                <dd id="LinearEmbeddings.named_modules" class="function">named_modules</dd>
                <dd id="LinearEmbeddings.train" class="function">train</dd>
                <dd id="LinearEmbeddings.eval" class="function">eval</dd>
                <dd id="LinearEmbeddings.requires_grad_" class="function">requires_grad_</dd>
                <dd id="LinearEmbeddings.zero_grad" class="function">zero_grad</dd>
                <dd id="LinearEmbeddings.share_memory" class="function">share_memory</dd>
                <dd id="LinearEmbeddings.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="CategoricalFeatureEmbeddings">
                            <input id="CategoricalFeatureEmbeddings-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">CategoricalFeatureEmbeddings</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="CategoricalFeatureEmbeddings-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CategoricalFeatureEmbeddings-383"><a href="#CategoricalFeatureEmbeddings-383"><span class="linenos">383</span></a><span class="k">class</span> <span class="nc">CategoricalFeatureEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="CategoricalFeatureEmbeddings-384"><a href="#CategoricalFeatureEmbeddings-384"><span class="linenos">384</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Embeddings for categorical features.</span>
</span><span id="CategoricalFeatureEmbeddings-385"><a href="#CategoricalFeatureEmbeddings-385"><span class="linenos">385</span></a>
</span><span id="CategoricalFeatureEmbeddings-386"><a href="#CategoricalFeatureEmbeddings-386"><span class="linenos">386</span></a><span class="sd">    For the illustration, see `FTTransformer`.</span>
</span><span id="CategoricalFeatureEmbeddings-387"><a href="#CategoricalFeatureEmbeddings-387"><span class="linenos">387</span></a>
</span><span id="CategoricalFeatureEmbeddings-388"><a href="#CategoricalFeatureEmbeddings-388"><span class="linenos">388</span></a><span class="sd">    **Notes**</span>
</span><span id="CategoricalFeatureEmbeddings-389"><a href="#CategoricalFeatureEmbeddings-389"><span class="linenos">389</span></a>
</span><span id="CategoricalFeatureEmbeddings-390"><a href="#CategoricalFeatureEmbeddings-390"><span class="linenos">390</span></a><span class="sd">    - A cardinality of a categorical feature is the number of distinct values</span>
</span><span id="CategoricalFeatureEmbeddings-391"><a href="#CategoricalFeatureEmbeddings-391"><span class="linenos">391</span></a><span class="sd">      that the feature takes</span>
</span><span id="CategoricalFeatureEmbeddings-392"><a href="#CategoricalFeatureEmbeddings-392"><span class="linenos">392</span></a><span class="sd">    - A categorical feature must be represented by `int64` from `range(0, cardinality)`</span>
</span><span id="CategoricalFeatureEmbeddings-393"><a href="#CategoricalFeatureEmbeddings-393"><span class="linenos">393</span></a>
</span><span id="CategoricalFeatureEmbeddings-394"><a href="#CategoricalFeatureEmbeddings-394"><span class="linenos">394</span></a><span class="sd">    **Shape**</span>
</span><span id="CategoricalFeatureEmbeddings-395"><a href="#CategoricalFeatureEmbeddings-395"><span class="linenos">395</span></a>
</span><span id="CategoricalFeatureEmbeddings-396"><a href="#CategoricalFeatureEmbeddings-396"><span class="linenos">396</span></a><span class="sd">    - Input: `(*, len(cardinalities))`</span>
</span><span id="CategoricalFeatureEmbeddings-397"><a href="#CategoricalFeatureEmbeddings-397"><span class="linenos">397</span></a><span class="sd">    - Output: `(*, len(cardinalities), d_embedding)`</span>
</span><span id="CategoricalFeatureEmbeddings-398"><a href="#CategoricalFeatureEmbeddings-398"><span class="linenos">398</span></a>
</span><span id="CategoricalFeatureEmbeddings-399"><a href="#CategoricalFeatureEmbeddings-399"><span class="linenos">399</span></a><span class="sd">    **Examples**</span>
</span><span id="CategoricalFeatureEmbeddings-400"><a href="#CategoricalFeatureEmbeddings-400"><span class="linenos">400</span></a>
</span><span id="CategoricalFeatureEmbeddings-401"><a href="#CategoricalFeatureEmbeddings-401"><span class="linenos">401</span></a><span class="sd">    &gt;&gt;&gt; cardinalities = [3, 10]</span>
</span><span id="CategoricalFeatureEmbeddings-402"><a href="#CategoricalFeatureEmbeddings-402"><span class="linenos">402</span></a><span class="sd">    &gt;&gt;&gt; x = torch.tensor([</span>
</span><span id="CategoricalFeatureEmbeddings-403"><a href="#CategoricalFeatureEmbeddings-403"><span class="linenos">403</span></a><span class="sd">    ...     [0, 5],</span>
</span><span id="CategoricalFeatureEmbeddings-404"><a href="#CategoricalFeatureEmbeddings-404"><span class="linenos">404</span></a><span class="sd">    ...     [1, 7],</span>
</span><span id="CategoricalFeatureEmbeddings-405"><a href="#CategoricalFeatureEmbeddings-405"><span class="linenos">405</span></a><span class="sd">    ...     [0, 2],</span>
</span><span id="CategoricalFeatureEmbeddings-406"><a href="#CategoricalFeatureEmbeddings-406"><span class="linenos">406</span></a><span class="sd">    ...     [2, 4]</span>
</span><span id="CategoricalFeatureEmbeddings-407"><a href="#CategoricalFeatureEmbeddings-407"><span class="linenos">407</span></a><span class="sd">    ... ])</span>
</span><span id="CategoricalFeatureEmbeddings-408"><a href="#CategoricalFeatureEmbeddings-408"><span class="linenos">408</span></a><span class="sd">    &gt;&gt;&gt; batch_size, n_cat_features = x.shape</span>
</span><span id="CategoricalFeatureEmbeddings-409"><a href="#CategoricalFeatureEmbeddings-409"><span class="linenos">409</span></a><span class="sd">    &gt;&gt;&gt; d_embedding = 3</span>
</span><span id="CategoricalFeatureEmbeddings-410"><a href="#CategoricalFeatureEmbeddings-410"><span class="linenos">410</span></a><span class="sd">    &gt;&gt;&gt; m = CategoricalFeatureEmbeddings(cardinalities, d_embedding, True)</span>
</span><span id="CategoricalFeatureEmbeddings-411"><a href="#CategoricalFeatureEmbeddings-411"><span class="linenos">411</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, n_cat_features, d_embedding)</span>
</span><span id="CategoricalFeatureEmbeddings-412"><a href="#CategoricalFeatureEmbeddings-412"><span class="linenos">412</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-413"><a href="#CategoricalFeatureEmbeddings-413"><span class="linenos">413</span></a>
</span><span id="CategoricalFeatureEmbeddings-414"><a href="#CategoricalFeatureEmbeddings-414"><span class="linenos">414</span></a>    <span class="n">_category_offsets</span><span class="p">:</span> <span class="n">Tensor</span>
</span><span id="CategoricalFeatureEmbeddings-415"><a href="#CategoricalFeatureEmbeddings-415"><span class="linenos">415</span></a>
</span><span id="CategoricalFeatureEmbeddings-416"><a href="#CategoricalFeatureEmbeddings-416"><span class="linenos">416</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-417"><a href="#CategoricalFeatureEmbeddings-417"><span class="linenos">417</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-418"><a href="#CategoricalFeatureEmbeddings-418"><span class="linenos">418</span></a><span class="sd">        Args:</span>
</span><span id="CategoricalFeatureEmbeddings-419"><a href="#CategoricalFeatureEmbeddings-419"><span class="linenos">419</span></a><span class="sd">            cardinalities: the number of distinct values for each feature. For example,</span>
</span><span id="CategoricalFeatureEmbeddings-420"><a href="#CategoricalFeatureEmbeddings-420"><span class="linenos">420</span></a><span class="sd">                `cardinalities=[3, 4]` describes two features, where the first</span>
</span><span id="CategoricalFeatureEmbeddings-421"><a href="#CategoricalFeatureEmbeddings-421"><span class="linenos">421</span></a><span class="sd">                takes values in the range `[0, 1, 2]` and the second one takes</span>
</span><span id="CategoricalFeatureEmbeddings-422"><a href="#CategoricalFeatureEmbeddings-422"><span class="linenos">422</span></a><span class="sd">                values in the range `[0, 1, 2, 3]`.</span>
</span><span id="CategoricalFeatureEmbeddings-423"><a href="#CategoricalFeatureEmbeddings-423"><span class="linenos">423</span></a><span class="sd">            d_embedding: the embedding size.</span>
</span><span id="CategoricalFeatureEmbeddings-424"><a href="#CategoricalFeatureEmbeddings-424"><span class="linenos">424</span></a><span class="sd">            bias: if `True`, for each feature, a trainable vector is added to the</span>
</span><span id="CategoricalFeatureEmbeddings-425"><a href="#CategoricalFeatureEmbeddings-425"><span class="linenos">425</span></a><span class="sd">                embedding regardless of a feature value. For each feature, a separate</span>
</span><span id="CategoricalFeatureEmbeddings-426"><a href="#CategoricalFeatureEmbeddings-426"><span class="linenos">426</span></a><span class="sd">                non-shared bias vector is allocated. In the paper, we used `bias=True`.</span>
</span><span id="CategoricalFeatureEmbeddings-427"><a href="#CategoricalFeatureEmbeddings-427"><span class="linenos">427</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-428"><a href="#CategoricalFeatureEmbeddings-428"><span class="linenos">428</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="CategoricalFeatureEmbeddings-429"><a href="#CategoricalFeatureEmbeddings-429"><span class="linenos">429</span></a>        <span class="k">assert</span> <span class="n">cardinalities</span>
</span><span id="CategoricalFeatureEmbeddings-430"><a href="#CategoricalFeatureEmbeddings-430"><span class="linenos">430</span></a>        <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="CategoricalFeatureEmbeddings-431"><a href="#CategoricalFeatureEmbeddings-431"><span class="linenos">431</span></a>
</span><span id="CategoricalFeatureEmbeddings-432"><a href="#CategoricalFeatureEmbeddings-432"><span class="linenos">432</span></a>        <span class="n">category_offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">cardinalities</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings-433"><a href="#CategoricalFeatureEmbeddings-433"><span class="linenos">433</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;_category_offsets&#39;</span><span class="p">,</span> <span class="n">category_offsets</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings-434"><a href="#CategoricalFeatureEmbeddings-434"><span class="linenos">434</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">),</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings-435"><a href="#CategoricalFeatureEmbeddings-435"><span class="linenos">435</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-436"><a href="#CategoricalFeatureEmbeddings-436"><span class="linenos">436</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CategoricalFeatureEmbeddings-437"><a href="#CategoricalFeatureEmbeddings-437"><span class="linenos">437</span></a>            <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">),</span> <span class="n">d_embedding</span><span class="p">))</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="CategoricalFeatureEmbeddings-438"><a href="#CategoricalFeatureEmbeddings-438"><span class="linenos">438</span></a>        <span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings-439"><a href="#CategoricalFeatureEmbeddings-439"><span class="linenos">439</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The bias.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-440"><a href="#CategoricalFeatureEmbeddings-440"><span class="linenos">440</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="CategoricalFeatureEmbeddings-441"><a href="#CategoricalFeatureEmbeddings-441"><span class="linenos">441</span></a>
</span><span id="CategoricalFeatureEmbeddings-442"><a href="#CategoricalFeatureEmbeddings-442"><span class="linenos">442</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-443"><a href="#CategoricalFeatureEmbeddings-443"><span class="linenos">443</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-444"><a href="#CategoricalFeatureEmbeddings-444"><span class="linenos">444</span></a>        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
</span><span id="CategoricalFeatureEmbeddings-445"><a href="#CategoricalFeatureEmbeddings-445"><span class="linenos">445</span></a>            <span class="k">if</span> <span class="n">parameter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-446"><a href="#CategoricalFeatureEmbeddings-446"><span class="linenos">446</span></a>                <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings-447"><a href="#CategoricalFeatureEmbeddings-447"><span class="linenos">447</span></a>
</span><span id="CategoricalFeatureEmbeddings-448"><a href="#CategoricalFeatureEmbeddings-448"><span class="linenos">448</span></a>    <span class="nd">@property</span>
</span><span id="CategoricalFeatureEmbeddings-449"><a href="#CategoricalFeatureEmbeddings-449"><span class="linenos">449</span></a>    <span class="k">def</span> <span class="nf">n_features</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-450"><a href="#CategoricalFeatureEmbeddings-450"><span class="linenos">450</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The number of features.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-451"><a href="#CategoricalFeatureEmbeddings-451"><span class="linenos">451</span></a>        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_category_offsets</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings-452"><a href="#CategoricalFeatureEmbeddings-452"><span class="linenos">452</span></a>
</span><span id="CategoricalFeatureEmbeddings-453"><a href="#CategoricalFeatureEmbeddings-453"><span class="linenos">453</span></a>    <span class="nd">@property</span>
</span><span id="CategoricalFeatureEmbeddings-454"><a href="#CategoricalFeatureEmbeddings-454"><span class="linenos">454</span></a>    <span class="k">def</span> <span class="nf">d_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-455"><a href="#CategoricalFeatureEmbeddings-455"><span class="linenos">455</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embedding size.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-456"><a href="#CategoricalFeatureEmbeddings-456"><span class="linenos">456</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">embedding_dim</span>
</span><span id="CategoricalFeatureEmbeddings-457"><a href="#CategoricalFeatureEmbeddings-457"><span class="linenos">457</span></a>
</span><span id="CategoricalFeatureEmbeddings-458"><a href="#CategoricalFeatureEmbeddings-458"><span class="linenos">458</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-459"><a href="#CategoricalFeatureEmbeddings-459"><span class="linenos">459</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings-460"><a href="#CategoricalFeatureEmbeddings-460"><span class="linenos">460</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
</span><span id="CategoricalFeatureEmbeddings-461"><a href="#CategoricalFeatureEmbeddings-461"><span class="linenos">461</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_category_offsets</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
</span><span id="CategoricalFeatureEmbeddings-462"><a href="#CategoricalFeatureEmbeddings-462"><span class="linenos">462</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings-463"><a href="#CategoricalFeatureEmbeddings-463"><span class="linenos">463</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="CategoricalFeatureEmbeddings-464"><a href="#CategoricalFeatureEmbeddings-464"><span class="linenos">464</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Embeddings for categorical features.</p>

<p>For the illustration, see <code><a href="#FTTransformer">FTTransformer</a></code>.</p>

<p><strong>Notes</strong></p>

<ul>
<li>A cardinality of a categorical feature is the number of distinct values
that the feature takes</li>
<li>A categorical feature must be represented by <code>int64</code> from <code>range(0, cardinality)</code></li>
</ul>

<p><strong>Shape</strong></p>

<ul>
<li>Input: <code>(*, len(cardinalities))</code></li>
<li>Output: <code>(*, len(cardinalities), d_embedding)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">cardinalities</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_cat_features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_embedding</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">CategoricalFeatureEmbeddings</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_cat_features</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</code></pre>
</div>
</div>


                            <div id="CategoricalFeatureEmbeddings.__init__" class="classattr">
                                        <input id="CategoricalFeatureEmbeddings.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">CategoricalFeatureEmbeddings</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>, </span><span class="param"><span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span></span>)</span>

                <label class="view-source-button" for="CategoricalFeatureEmbeddings.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CategoricalFeatureEmbeddings.__init__-416"><a href="#CategoricalFeatureEmbeddings.__init__-416"><span class="linenos">416</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-417"><a href="#CategoricalFeatureEmbeddings.__init__-417"><span class="linenos">417</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-418"><a href="#CategoricalFeatureEmbeddings.__init__-418"><span class="linenos">418</span></a><span class="sd">        Args:</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-419"><a href="#CategoricalFeatureEmbeddings.__init__-419"><span class="linenos">419</span></a><span class="sd">            cardinalities: the number of distinct values for each feature. For example,</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-420"><a href="#CategoricalFeatureEmbeddings.__init__-420"><span class="linenos">420</span></a><span class="sd">                `cardinalities=[3, 4]` describes two features, where the first</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-421"><a href="#CategoricalFeatureEmbeddings.__init__-421"><span class="linenos">421</span></a><span class="sd">                takes values in the range `[0, 1, 2]` and the second one takes</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-422"><a href="#CategoricalFeatureEmbeddings.__init__-422"><span class="linenos">422</span></a><span class="sd">                values in the range `[0, 1, 2, 3]`.</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-423"><a href="#CategoricalFeatureEmbeddings.__init__-423"><span class="linenos">423</span></a><span class="sd">            d_embedding: the embedding size.</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-424"><a href="#CategoricalFeatureEmbeddings.__init__-424"><span class="linenos">424</span></a><span class="sd">            bias: if `True`, for each feature, a trainable vector is added to the</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-425"><a href="#CategoricalFeatureEmbeddings.__init__-425"><span class="linenos">425</span></a><span class="sd">                embedding regardless of a feature value. For each feature, a separate</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-426"><a href="#CategoricalFeatureEmbeddings.__init__-426"><span class="linenos">426</span></a><span class="sd">                non-shared bias vector is allocated. In the paper, we used `bias=True`.</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-427"><a href="#CategoricalFeatureEmbeddings.__init__-427"><span class="linenos">427</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-428"><a href="#CategoricalFeatureEmbeddings.__init__-428"><span class="linenos">428</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-429"><a href="#CategoricalFeatureEmbeddings.__init__-429"><span class="linenos">429</span></a>        <span class="k">assert</span> <span class="n">cardinalities</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-430"><a href="#CategoricalFeatureEmbeddings.__init__-430"><span class="linenos">430</span></a>        <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-431"><a href="#CategoricalFeatureEmbeddings.__init__-431"><span class="linenos">431</span></a>
</span><span id="CategoricalFeatureEmbeddings.__init__-432"><a href="#CategoricalFeatureEmbeddings.__init__-432"><span class="linenos">432</span></a>        <span class="n">category_offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">cardinalities</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-433"><a href="#CategoricalFeatureEmbeddings.__init__-433"><span class="linenos">433</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;_category_offsets&#39;</span><span class="p">,</span> <span class="n">category_offsets</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-434"><a href="#CategoricalFeatureEmbeddings.__init__-434"><span class="linenos">434</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">),</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-435"><a href="#CategoricalFeatureEmbeddings.__init__-435"><span class="linenos">435</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-436"><a href="#CategoricalFeatureEmbeddings.__init__-436"><span class="linenos">436</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-437"><a href="#CategoricalFeatureEmbeddings.__init__-437"><span class="linenos">437</span></a>            <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cardinalities</span><span class="p">),</span> <span class="n">d_embedding</span><span class="p">))</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-438"><a href="#CategoricalFeatureEmbeddings.__init__-438"><span class="linenos">438</span></a>        <span class="p">)</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-439"><a href="#CategoricalFeatureEmbeddings.__init__-439"><span class="linenos">439</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The bias.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings.__init__-440"><a href="#CategoricalFeatureEmbeddings.__init__-440"><span class="linenos">440</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>cardinalities:</strong>  the number of distinct values for each feature. For example,
<code>cardinalities=[3, 4]</code> describes two features, where the first
takes values in the range <code>[0, 1, 2]</code> and the second one takes
values in the range <code>[0, 1, 2, 3]</code>.</li>
<li><strong>d_embedding:</strong>  the embedding size.</li>
<li><strong>bias:</strong>  if <code>True</code>, for each feature, a trainable vector is added to the
embedding regardless of a feature value. For each feature, a separate
non-shared bias vector is allocated. In the paper, we used <code>bias=True</code>.</li>
</ul>
</div>


                            </div>
                            <div id="CategoricalFeatureEmbeddings.embeddings" class="classattr">
                                <div class="attr variable">
            <span class="name">embeddings</span>

        
    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.embeddings"></a>
    
            <div class="docstring"><p>The embeddings.</p>
</div>


                            </div>
                            <div id="CategoricalFeatureEmbeddings.bias" class="classattr">
                                <div class="attr variable">
            <span class="name">bias</span>

        
    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.bias"></a>
    
            <div class="docstring"><p>The bias.</p>
</div>


                            </div>
                            <div id="CategoricalFeatureEmbeddings.reset_parameters" class="classattr">
                                        <input id="CategoricalFeatureEmbeddings.reset_parameters-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">reset_parameters</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CategoricalFeatureEmbeddings.reset_parameters-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.reset_parameters"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CategoricalFeatureEmbeddings.reset_parameters-442"><a href="#CategoricalFeatureEmbeddings.reset_parameters-442"><span class="linenos">442</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings.reset_parameters-443"><a href="#CategoricalFeatureEmbeddings.reset_parameters-443"><span class="linenos">443</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings.reset_parameters-444"><a href="#CategoricalFeatureEmbeddings.reset_parameters-444"><span class="linenos">444</span></a>        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
</span><span id="CategoricalFeatureEmbeddings.reset_parameters-445"><a href="#CategoricalFeatureEmbeddings.reset_parameters-445"><span class="linenos">445</span></a>            <span class="k">if</span> <span class="n">parameter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings.reset_parameters-446"><a href="#CategoricalFeatureEmbeddings.reset_parameters-446"><span class="linenos">446</span></a>                <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Reinitialize all parameters.</p>
</div>


                            </div>
                            <div id="CategoricalFeatureEmbeddings.n_features" class="classattr">
                                <div class="attr variable">
            <span class="name">n_features</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.n_features"></a>
    
            <div class="docstring"><p>The number of features.</p>
</div>


                            </div>
                            <div id="CategoricalFeatureEmbeddings.d_embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">d_embedding</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.d_embedding"></a>
    
            <div class="docstring"><p>The embedding size.</p>
</div>


                            </div>
                            <div id="CategoricalFeatureEmbeddings.forward" class="classattr">
                                        <input id="CategoricalFeatureEmbeddings.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="CategoricalFeatureEmbeddings.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CategoricalFeatureEmbeddings.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CategoricalFeatureEmbeddings.forward-458"><a href="#CategoricalFeatureEmbeddings.forward-458"><span class="linenos">458</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings.forward-459"><a href="#CategoricalFeatureEmbeddings.forward-459"><span class="linenos">459</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="CategoricalFeatureEmbeddings.forward-460"><a href="#CategoricalFeatureEmbeddings.forward-460"><span class="linenos">460</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
</span><span id="CategoricalFeatureEmbeddings.forward-461"><a href="#CategoricalFeatureEmbeddings.forward-461"><span class="linenos">461</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_category_offsets</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
</span><span id="CategoricalFeatureEmbeddings.forward-462"><a href="#CategoricalFeatureEmbeddings.forward-462"><span class="linenos">462</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CategoricalFeatureEmbeddings.forward-463"><a href="#CategoricalFeatureEmbeddings.forward-463"><span class="linenos">463</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="CategoricalFeatureEmbeddings.forward-464"><a href="#CategoricalFeatureEmbeddings.forward-464"><span class="linenos">464</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="CategoricalFeatureEmbeddings.dump_patches" class="variable">dump_patches</dd>
                <dd id="CategoricalFeatureEmbeddings.training" class="variable">training</dd>
                <dd id="CategoricalFeatureEmbeddings.register_buffer" class="function">register_buffer</dd>
                <dd id="CategoricalFeatureEmbeddings.register_parameter" class="function">register_parameter</dd>
                <dd id="CategoricalFeatureEmbeddings.add_module" class="function">add_module</dd>
                <dd id="CategoricalFeatureEmbeddings.apply" class="function">apply</dd>
                <dd id="CategoricalFeatureEmbeddings.cuda" class="function">cuda</dd>
                <dd id="CategoricalFeatureEmbeddings.xpu" class="function">xpu</dd>
                <dd id="CategoricalFeatureEmbeddings.cpu" class="function">cpu</dd>
                <dd id="CategoricalFeatureEmbeddings.type" class="function">type</dd>
                <dd id="CategoricalFeatureEmbeddings.float" class="function">float</dd>
                <dd id="CategoricalFeatureEmbeddings.double" class="function">double</dd>
                <dd id="CategoricalFeatureEmbeddings.half" class="function">half</dd>
                <dd id="CategoricalFeatureEmbeddings.bfloat16" class="function">bfloat16</dd>
                <dd id="CategoricalFeatureEmbeddings.to" class="function">to</dd>
                <dd id="CategoricalFeatureEmbeddings.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="CategoricalFeatureEmbeddings.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="CategoricalFeatureEmbeddings.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="CategoricalFeatureEmbeddings.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="CategoricalFeatureEmbeddings.state_dict" class="function">state_dict</dd>
                <dd id="CategoricalFeatureEmbeddings.load_state_dict" class="function">load_state_dict</dd>
                <dd id="CategoricalFeatureEmbeddings.parameters" class="function">parameters</dd>
                <dd id="CategoricalFeatureEmbeddings.named_parameters" class="function">named_parameters</dd>
                <dd id="CategoricalFeatureEmbeddings.buffers" class="function">buffers</dd>
                <dd id="CategoricalFeatureEmbeddings.named_buffers" class="function">named_buffers</dd>
                <dd id="CategoricalFeatureEmbeddings.children" class="function">children</dd>
                <dd id="CategoricalFeatureEmbeddings.named_children" class="function">named_children</dd>
                <dd id="CategoricalFeatureEmbeddings.modules" class="function">modules</dd>
                <dd id="CategoricalFeatureEmbeddings.named_modules" class="function">named_modules</dd>
                <dd id="CategoricalFeatureEmbeddings.train" class="function">train</dd>
                <dd id="CategoricalFeatureEmbeddings.eval" class="function">eval</dd>
                <dd id="CategoricalFeatureEmbeddings.requires_grad_" class="function">requires_grad_</dd>
                <dd id="CategoricalFeatureEmbeddings.zero_grad" class="function">zero_grad</dd>
                <dd id="CategoricalFeatureEmbeddings.share_memory" class="function">share_memory</dd>
                <dd id="CategoricalFeatureEmbeddings.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="CLSEmbedding">
                            <input id="CLSEmbedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">CLSEmbedding</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="CLSEmbedding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLSEmbedding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLSEmbedding-286"><a href="#CLSEmbedding-286"><span class="linenos">286</span></a><span class="k">class</span> <span class="nc">CLSEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="CLSEmbedding-287"><a href="#CLSEmbedding-287"><span class="linenos">287</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The [CLS]-token embedding for the Transformer backbone.</span>
</span><span id="CLSEmbedding-288"><a href="#CLSEmbedding-288"><span class="linenos">288</span></a>
</span><span id="CLSEmbedding-289"><a href="#CLSEmbedding-289"><span class="linenos">289</span></a><span class="sd">    The module prepends the same trainable token embedding to</span>
</span><span id="CLSEmbedding-290"><a href="#CLSEmbedding-290"><span class="linenos">290</span></a><span class="sd">    all objects in the batch.</span>
</span><span id="CLSEmbedding-291"><a href="#CLSEmbedding-291"><span class="linenos">291</span></a>
</span><span id="CLSEmbedding-292"><a href="#CLSEmbedding-292"><span class="linenos">292</span></a><span class="sd">    **Shape**</span>
</span><span id="CLSEmbedding-293"><a href="#CLSEmbedding-293"><span class="linenos">293</span></a>
</span><span id="CLSEmbedding-294"><a href="#CLSEmbedding-294"><span class="linenos">294</span></a><span class="sd">    - Input: `(batch_size, n_tokens, d_embedding)`</span>
</span><span id="CLSEmbedding-295"><a href="#CLSEmbedding-295"><span class="linenos">295</span></a><span class="sd">    - Output: `(batch_size, 1 + n_tokens, d_embedding)`</span>
</span><span id="CLSEmbedding-296"><a href="#CLSEmbedding-296"><span class="linenos">296</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="CLSEmbedding-297"><a href="#CLSEmbedding-297"><span class="linenos">297</span></a>
</span><span id="CLSEmbedding-298"><a href="#CLSEmbedding-298"><span class="linenos">298</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLSEmbedding-299"><a href="#CLSEmbedding-299"><span class="linenos">299</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding-300"><a href="#CLSEmbedding-300"><span class="linenos">300</span></a><span class="sd">        Args:</span>
</span><span id="CLSEmbedding-301"><a href="#CLSEmbedding-301"><span class="linenos">301</span></a><span class="sd">            d_embedding: the size of one token embedding</span>
</span><span id="CLSEmbedding-302"><a href="#CLSEmbedding-302"><span class="linenos">302</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CLSEmbedding-303"><a href="#CLSEmbedding-303"><span class="linenos">303</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="CLSEmbedding-304"><a href="#CLSEmbedding-304"><span class="linenos">304</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">))</span>
</span><span id="CLSEmbedding-305"><a href="#CLSEmbedding-305"><span class="linenos">305</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="CLSEmbedding-306"><a href="#CLSEmbedding-306"><span class="linenos">306</span></a>
</span><span id="CLSEmbedding-307"><a href="#CLSEmbedding-307"><span class="linenos">307</span></a>    <span class="nd">@property</span>
</span><span id="CLSEmbedding-308"><a href="#CLSEmbedding-308"><span class="linenos">308</span></a>    <span class="k">def</span> <span class="nf">d_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="CLSEmbedding-309"><a href="#CLSEmbedding-309"><span class="linenos">309</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embedding size.&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding-310"><a href="#CLSEmbedding-310"><span class="linenos">310</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CLSEmbedding-311"><a href="#CLSEmbedding-311"><span class="linenos">311</span></a>
</span><span id="CLSEmbedding-312"><a href="#CLSEmbedding-312"><span class="linenos">312</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLSEmbedding-313"><a href="#CLSEmbedding-313"><span class="linenos">313</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding-314"><a href="#CLSEmbedding-314"><span class="linenos">314</span></a>        <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span><span id="CLSEmbedding-315"><a href="#CLSEmbedding-315"><span class="linenos">315</span></a>
</span><span id="CLSEmbedding-316"><a href="#CLSEmbedding-316"><span class="linenos">316</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="CLSEmbedding-317"><a href="#CLSEmbedding-317"><span class="linenos">317</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding-318"><a href="#CLSEmbedding-318"><span class="linenos">318</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
</span><span id="CLSEmbedding-319"><a href="#CLSEmbedding-319"><span class="linenos">319</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span>
</span><span id="CLSEmbedding-320"><a href="#CLSEmbedding-320"><span class="linenos">320</span></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>The [CLS]-token embedding for the Transformer backbone.</p>

<p>The module prepends the same trainable token embedding to
all objects in the batch.</p>

<p><strong>Shape</strong></p>

<ul>
<li>Input: <code>(batch_size, n_tokens, d_embedding)</code></li>
<li>Output: <code>(batch_size, 1 + n_tokens, d_embedding)</code></li>
</ul>
</div>


                            <div id="CLSEmbedding.__init__" class="classattr">
                                        <input id="CLSEmbedding.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">CLSEmbedding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

                <label class="view-source-button" for="CLSEmbedding.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLSEmbedding.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLSEmbedding.__init__-298"><a href="#CLSEmbedding.__init__-298"><span class="linenos">298</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLSEmbedding.__init__-299"><a href="#CLSEmbedding.__init__-299"><span class="linenos">299</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding.__init__-300"><a href="#CLSEmbedding.__init__-300"><span class="linenos">300</span></a><span class="sd">        Args:</span>
</span><span id="CLSEmbedding.__init__-301"><a href="#CLSEmbedding.__init__-301"><span class="linenos">301</span></a><span class="sd">            d_embedding: the size of one token embedding</span>
</span><span id="CLSEmbedding.__init__-302"><a href="#CLSEmbedding.__init__-302"><span class="linenos">302</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CLSEmbedding.__init__-303"><a href="#CLSEmbedding.__init__-303"><span class="linenos">303</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="CLSEmbedding.__init__-304"><a href="#CLSEmbedding.__init__-304"><span class="linenos">304</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">))</span>
</span><span id="CLSEmbedding.__init__-305"><a href="#CLSEmbedding.__init__-305"><span class="linenos">305</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>d_embedding:</strong>  the size of one token embedding</li>
</ul>
</div>


                            </div>
                            <div id="CLSEmbedding.weight" class="classattr">
                                <div class="attr variable">
            <span class="name">weight</span>

        
    </div>
    <a class="headerlink" href="#CLSEmbedding.weight"></a>
    
    

                            </div>
                            <div id="CLSEmbedding.d_embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">d_embedding</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#CLSEmbedding.d_embedding"></a>
    
            <div class="docstring"><p>The embedding size.</p>
</div>


                            </div>
                            <div id="CLSEmbedding.reset_parameters" class="classattr">
                                        <input id="CLSEmbedding.reset_parameters-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">reset_parameters</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <label class="view-source-button" for="CLSEmbedding.reset_parameters-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLSEmbedding.reset_parameters"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLSEmbedding.reset_parameters-312"><a href="#CLSEmbedding.reset_parameters-312"><span class="linenos">312</span></a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="CLSEmbedding.reset_parameters-313"><a href="#CLSEmbedding.reset_parameters-313"><span class="linenos">313</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize all parameters.&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding.reset_parameters-314"><a href="#CLSEmbedding.reset_parameters-314"><span class="linenos">314</span></a>        <span class="n">_init_uniform_rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Reinitialize all parameters.</p>
</div>


                            </div>
                            <div id="CLSEmbedding.forward" class="classattr">
                                        <input id="CLSEmbedding.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="CLSEmbedding.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CLSEmbedding.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CLSEmbedding.forward-316"><a href="#CLSEmbedding.forward-316"><span class="linenos">316</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="CLSEmbedding.forward-317"><a href="#CLSEmbedding.forward-317"><span class="linenos">317</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="CLSEmbedding.forward-318"><a href="#CLSEmbedding.forward-318"><span class="linenos">318</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
</span><span id="CLSEmbedding.forward-319"><a href="#CLSEmbedding.forward-319"><span class="linenos">319</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_embedding</span>
</span><span id="CLSEmbedding.forward-320"><a href="#CLSEmbedding.forward-320"><span class="linenos">320</span></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="CLSEmbedding.dump_patches" class="variable">dump_patches</dd>
                <dd id="CLSEmbedding.training" class="variable">training</dd>
                <dd id="CLSEmbedding.register_buffer" class="function">register_buffer</dd>
                <dd id="CLSEmbedding.register_parameter" class="function">register_parameter</dd>
                <dd id="CLSEmbedding.add_module" class="function">add_module</dd>
                <dd id="CLSEmbedding.apply" class="function">apply</dd>
                <dd id="CLSEmbedding.cuda" class="function">cuda</dd>
                <dd id="CLSEmbedding.xpu" class="function">xpu</dd>
                <dd id="CLSEmbedding.cpu" class="function">cpu</dd>
                <dd id="CLSEmbedding.type" class="function">type</dd>
                <dd id="CLSEmbedding.float" class="function">float</dd>
                <dd id="CLSEmbedding.double" class="function">double</dd>
                <dd id="CLSEmbedding.half" class="function">half</dd>
                <dd id="CLSEmbedding.bfloat16" class="function">bfloat16</dd>
                <dd id="CLSEmbedding.to" class="function">to</dd>
                <dd id="CLSEmbedding.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="CLSEmbedding.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="CLSEmbedding.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="CLSEmbedding.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="CLSEmbedding.state_dict" class="function">state_dict</dd>
                <dd id="CLSEmbedding.load_state_dict" class="function">load_state_dict</dd>
                <dd id="CLSEmbedding.parameters" class="function">parameters</dd>
                <dd id="CLSEmbedding.named_parameters" class="function">named_parameters</dd>
                <dd id="CLSEmbedding.buffers" class="function">buffers</dd>
                <dd id="CLSEmbedding.named_buffers" class="function">named_buffers</dd>
                <dd id="CLSEmbedding.children" class="function">children</dd>
                <dd id="CLSEmbedding.named_children" class="function">named_children</dd>
                <dd id="CLSEmbedding.modules" class="function">modules</dd>
                <dd id="CLSEmbedding.named_modules" class="function">named_modules</dd>
                <dd id="CLSEmbedding.train" class="function">train</dd>
                <dd id="CLSEmbedding.eval" class="function">eval</dd>
                <dd id="CLSEmbedding.requires_grad_" class="function">requires_grad_</dd>
                <dd id="CLSEmbedding.zero_grad" class="function">zero_grad</dd>
                <dd id="CLSEmbedding.share_memory" class="function">share_memory</dd>
                <dd id="CLSEmbedding.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="MultiheadAttention">
                            <input id="MultiheadAttention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MultiheadAttention</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MultiheadAttention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiheadAttention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiheadAttention-470"><a href="#MultiheadAttention-470"><span class="linenos">470</span></a><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MultiheadAttention-471"><a href="#MultiheadAttention-471"><span class="linenos">471</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Multihead Attention (self-/cross-) with an optional linear attention.</span>
</span><span id="MultiheadAttention-472"><a href="#MultiheadAttention-472"><span class="linenos">472</span></a>
</span><span id="MultiheadAttention-473"><a href="#MultiheadAttention-473"><span class="linenos">473</span></a><span class="sd">    - To learn more about Multihead Attention, see</span>
</span><span id="MultiheadAttention-474"><a href="#MultiheadAttention-474"><span class="linenos">474</span></a><span class="sd">      [&quot;Attention Is All You Need&quot;](https://arxiv.org/abs/1706.03762)</span>
</span><span id="MultiheadAttention-475"><a href="#MultiheadAttention-475"><span class="linenos">475</span></a><span class="sd">    - To learn about the linear attention supported by this module, see</span>
</span><span id="MultiheadAttention-476"><a href="#MultiheadAttention-476"><span class="linenos">476</span></a><span class="sd">      [&quot;Linformer: Self-Attention with Linear Complexity&quot;](https://arxiv.org/abs/2006.04768),</span>
</span><span id="MultiheadAttention-477"><a href="#MultiheadAttention-477"><span class="linenos">477</span></a>
</span><span id="MultiheadAttention-478"><a href="#MultiheadAttention-478"><span class="linenos">478</span></a><span class="sd">    **Shape**</span>
</span><span id="MultiheadAttention-479"><a href="#MultiheadAttention-479"><span class="linenos">479</span></a>
</span><span id="MultiheadAttention-480"><a href="#MultiheadAttention-480"><span class="linenos">480</span></a><span class="sd">    - Input:</span>
</span><span id="MultiheadAttention-481"><a href="#MultiheadAttention-481"><span class="linenos">481</span></a><span class="sd">        - `x_q  ~ (batch_size, n_q_tokes,  d_embedding)`</span>
</span><span id="MultiheadAttention-482"><a href="#MultiheadAttention-482"><span class="linenos">482</span></a><span class="sd">        - `x_kv ~ (batch_size, n_kv_tokes, d_embedding)`</span>
</span><span id="MultiheadAttention-483"><a href="#MultiheadAttention-483"><span class="linenos">483</span></a><span class="sd">    - Output: `(batch_size, n_q_tokes, d_embedding)`</span>
</span><span id="MultiheadAttention-484"><a href="#MultiheadAttention-484"><span class="linenos">484</span></a>
</span><span id="MultiheadAttention-485"><a href="#MultiheadAttention-485"><span class="linenos">485</span></a><span class="sd">    **Examples**</span>
</span><span id="MultiheadAttention-486"><a href="#MultiheadAttention-486"><span class="linenos">486</span></a>
</span><span id="MultiheadAttention-487"><a href="#MultiheadAttention-487"><span class="linenos">487</span></a><span class="sd">    &gt;&gt;&gt; batch_size, n_tokens, d_embedding = 2, 3, 16</span>
</span><span id="MultiheadAttention-488"><a href="#MultiheadAttention-488"><span class="linenos">488</span></a><span class="sd">    &gt;&gt;&gt; n_heads = 8</span>
</span><span id="MultiheadAttention-489"><a href="#MultiheadAttention-489"><span class="linenos">489</span></a><span class="sd">    &gt;&gt;&gt; a = torch.randn(batch_size, n_tokens, d_embedding)</span>
</span><span id="MultiheadAttention-490"><a href="#MultiheadAttention-490"><span class="linenos">490</span></a><span class="sd">    &gt;&gt;&gt; b = torch.randn(batch_size, n_tokens * 2, d_embedding)</span>
</span><span id="MultiheadAttention-491"><a href="#MultiheadAttention-491"><span class="linenos">491</span></a><span class="sd">    &gt;&gt;&gt; m = MultiheadAttention(</span>
</span><span id="MultiheadAttention-492"><a href="#MultiheadAttention-492"><span class="linenos">492</span></a><span class="sd">    ...     d_embedding=d_embedding, n_heads=n_heads, dropout=0.2</span>
</span><span id="MultiheadAttention-493"><a href="#MultiheadAttention-493"><span class="linenos">493</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="MultiheadAttention-494"><a href="#MultiheadAttention-494"><span class="linenos">494</span></a><span class="sd">    &gt;&gt;&gt;</span>
</span><span id="MultiheadAttention-495"><a href="#MultiheadAttention-495"><span class="linenos">495</span></a><span class="sd">    &gt;&gt;&gt; # self-attention</span>
</span><span id="MultiheadAttention-496"><a href="#MultiheadAttention-496"><span class="linenos">496</span></a><span class="sd">    &gt;&gt;&gt; assert m(a, a).shape == a.shape</span>
</span><span id="MultiheadAttention-497"><a href="#MultiheadAttention-497"><span class="linenos">497</span></a><span class="sd">    &gt;&gt;&gt;</span>
</span><span id="MultiheadAttention-498"><a href="#MultiheadAttention-498"><span class="linenos">498</span></a><span class="sd">    &gt;&gt;&gt; # cross-attention</span>
</span><span id="MultiheadAttention-499"><a href="#MultiheadAttention-499"><span class="linenos">499</span></a><span class="sd">    &gt;&gt;&gt; assert m(a, b).shape == a.shape</span>
</span><span id="MultiheadAttention-500"><a href="#MultiheadAttention-500"><span class="linenos">500</span></a><span class="sd">    &gt;&gt;&gt;</span>
</span><span id="MultiheadAttention-501"><a href="#MultiheadAttention-501"><span class="linenos">501</span></a><span class="sd">    &gt;&gt;&gt; # Linformer attention</span>
</span><span id="MultiheadAttention-502"><a href="#MultiheadAttention-502"><span class="linenos">502</span></a><span class="sd">    &gt;&gt;&gt; m = MultiheadAttention(</span>
</span><span id="MultiheadAttention-503"><a href="#MultiheadAttention-503"><span class="linenos">503</span></a><span class="sd">    ...     d_embedding=d_embedding,</span>
</span><span id="MultiheadAttention-504"><a href="#MultiheadAttention-504"><span class="linenos">504</span></a><span class="sd">    ...     n_heads=n_heads,</span>
</span><span id="MultiheadAttention-505"><a href="#MultiheadAttention-505"><span class="linenos">505</span></a><span class="sd">    ...     dropout=0.2,</span>
</span><span id="MultiheadAttention-506"><a href="#MultiheadAttention-506"><span class="linenos">506</span></a><span class="sd">    ...     n_tokens=n_tokens,</span>
</span><span id="MultiheadAttention-507"><a href="#MultiheadAttention-507"><span class="linenos">507</span></a><span class="sd">    ...     kv_compression_ratio=0.5,</span>
</span><span id="MultiheadAttention-508"><a href="#MultiheadAttention-508"><span class="linenos">508</span></a><span class="sd">    ...     kv_compression_sharing=&#39;headwise&#39;,</span>
</span><span id="MultiheadAttention-509"><a href="#MultiheadAttention-509"><span class="linenos">509</span></a><span class="sd">    &gt;&gt;&gt; )</span>
</span><span id="MultiheadAttention-510"><a href="#MultiheadAttention-510"><span class="linenos">510</span></a><span class="sd">    &gt;&gt;&gt; assert m(a, a).shape == a.shape</span>
</span><span id="MultiheadAttention-511"><a href="#MultiheadAttention-511"><span class="linenos">511</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-512"><a href="#MultiheadAttention-512"><span class="linenos">512</span></a>
</span><span id="MultiheadAttention-513"><a href="#MultiheadAttention-513"><span class="linenos">513</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="MultiheadAttention-514"><a href="#MultiheadAttention-514"><span class="linenos">514</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MultiheadAttention-515"><a href="#MultiheadAttention-515"><span class="linenos">515</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="MultiheadAttention-516"><a href="#MultiheadAttention-516"><span class="linenos">516</span></a>        <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MultiheadAttention-517"><a href="#MultiheadAttention-517"><span class="linenos">517</span></a>        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MultiheadAttention-518"><a href="#MultiheadAttention-518"><span class="linenos">518</span></a>        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="MultiheadAttention-519"><a href="#MultiheadAttention-519"><span class="linenos">519</span></a>        <span class="c1"># Linformer arguments.</span>
</span><span id="MultiheadAttention-520"><a href="#MultiheadAttention-520"><span class="linenos">520</span></a>        <span class="n">n_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MultiheadAttention-521"><a href="#MultiheadAttention-521"><span class="linenos">521</span></a>        <span class="n">kv_compression_ratio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MultiheadAttention-522"><a href="#MultiheadAttention-522"><span class="linenos">522</span></a>        <span class="n">kv_compression_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MultiheadAttention-523"><a href="#MultiheadAttention-523"><span class="linenos">523</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention-524"><a href="#MultiheadAttention-524"><span class="linenos">524</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-525"><a href="#MultiheadAttention-525"><span class="linenos">525</span></a><span class="sd">        Args:</span>
</span><span id="MultiheadAttention-526"><a href="#MultiheadAttention-526"><span class="linenos">526</span></a><span class="sd">            d_embedding: the embedding size for one token.</span>
</span><span id="MultiheadAttention-527"><a href="#MultiheadAttention-527"><span class="linenos">527</span></a><span class="sd">                Must be a multiple of `n_heads`.</span>
</span><span id="MultiheadAttention-528"><a href="#MultiheadAttention-528"><span class="linenos">528</span></a><span class="sd">            n_heads: the number of heads. If greater than 1, then the module will have</span>
</span><span id="MultiheadAttention-529"><a href="#MultiheadAttention-529"><span class="linenos">529</span></a><span class="sd">                an additional output layer (the so called &quot;mixing&quot; layer).</span>
</span><span id="MultiheadAttention-530"><a href="#MultiheadAttention-530"><span class="linenos">530</span></a><span class="sd">            dropout: the dropout rate for the attention probability map.</span>
</span><span id="MultiheadAttention-531"><a href="#MultiheadAttention-531"><span class="linenos">531</span></a><span class="sd">            n_tokens: the number of tokens</span>
</span><span id="MultiheadAttention-532"><a href="#MultiheadAttention-532"><span class="linenos">532</span></a><span class="sd">                (must be provided if `kv_compression_ratio` is not None)</span>
</span><span id="MultiheadAttention-533"><a href="#MultiheadAttention-533"><span class="linenos">533</span></a><span class="sd">            kv_compression_ratio: Linformer-style compression rate.</span>
</span><span id="MultiheadAttention-534"><a href="#MultiheadAttention-534"><span class="linenos">534</span></a><span class="sd">                Must be within the interval `(0.0, 1.0)`.</span>
</span><span id="MultiheadAttention-535"><a href="#MultiheadAttention-535"><span class="linenos">535</span></a><span class="sd">            kv_compression_sharing: Linformer compression sharing policy.</span>
</span><span id="MultiheadAttention-536"><a href="#MultiheadAttention-536"><span class="linenos">536</span></a><span class="sd">                Must be provided if `kv_compression_ratio` is not None.</span>
</span><span id="MultiheadAttention-537"><a href="#MultiheadAttention-537"><span class="linenos">537</span></a><span class="sd">                (non-shared Linformer compression is not supported; the &quot;layerwise&quot;</span>
</span><span id="MultiheadAttention-538"><a href="#MultiheadAttention-538"><span class="linenos">538</span></a><span class="sd">                sharing policy is not supported).</span>
</span><span id="MultiheadAttention-539"><a href="#MultiheadAttention-539"><span class="linenos">539</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-540"><a href="#MultiheadAttention-540"><span class="linenos">540</span></a>        <span class="k">if</span> <span class="n">n_heads</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="MultiheadAttention-541"><a href="#MultiheadAttention-541"><span class="linenos">541</span></a>            <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="MultiheadAttention-542"><a href="#MultiheadAttention-542"><span class="linenos">542</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MultiheadAttention-543"><a href="#MultiheadAttention-543"><span class="linenos">543</span></a>
</span><span id="MultiheadAttention-544"><a href="#MultiheadAttention-544"><span class="linenos">544</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="MultiheadAttention-545"><a href="#MultiheadAttention-545"><span class="linenos">545</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The query projection layer.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-546"><a href="#MultiheadAttention-546"><span class="linenos">546</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="MultiheadAttention-547"><a href="#MultiheadAttention-547"><span class="linenos">547</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The key projection layer.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-548"><a href="#MultiheadAttention-548"><span class="linenos">548</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="MultiheadAttention-549"><a href="#MultiheadAttention-549"><span class="linenos">549</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The value projection layer.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-550"><a href="#MultiheadAttention-550"><span class="linenos">550</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span> <span class="k">if</span> <span class="n">n_heads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-551"><a href="#MultiheadAttention-551"><span class="linenos">551</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output mixing layer (presented if `n_heads &gt; 1`).&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-552"><a href="#MultiheadAttention-552"><span class="linenos">552</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span><span id="MultiheadAttention-553"><a href="#MultiheadAttention-553"><span class="linenos">553</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span> <span class="k">if</span> <span class="n">dropout</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-554"><a href="#MultiheadAttention-554"><span class="linenos">554</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The dropout for the attention probability map.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-555"><a href="#MultiheadAttention-555"><span class="linenos">555</span></a>
</span><span id="MultiheadAttention-556"><a href="#MultiheadAttention-556"><span class="linenos">556</span></a>        <span class="k">if</span> <span class="n">kv_compression_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention-557"><a href="#MultiheadAttention-557"><span class="linenos">557</span></a>            <span class="k">assert</span> <span class="n">n_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-558"><a href="#MultiheadAttention-558"><span class="linenos">558</span></a>            <span class="k">assert</span> <span class="n">kv_compression_sharing</span> <span class="ow">in</span> <span class="n">typing</span><span class="o">.</span><span class="n">get_args</span><span class="p">(</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">)</span>
</span><span id="MultiheadAttention-559"><a href="#MultiheadAttention-559"><span class="linenos">559</span></a>            <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">kv_compression_ratio</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
</span><span id="MultiheadAttention-560"><a href="#MultiheadAttention-560"><span class="linenos">560</span></a>
</span><span id="MultiheadAttention-561"><a href="#MultiheadAttention-561"><span class="linenos">561</span></a>            <span class="k">def</span> <span class="nf">make_kv_compression</span><span class="p">():</span>
</span><span id="MultiheadAttention-562"><a href="#MultiheadAttention-562"><span class="linenos">562</span></a>                <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="MultiheadAttention-563"><a href="#MultiheadAttention-563"><span class="linenos">563</span></a>                    <span class="n">n_tokens</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n_tokens</span> <span class="o">*</span> <span class="n">kv_compression_ratio</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
</span><span id="MultiheadAttention-564"><a href="#MultiheadAttention-564"><span class="linenos">564</span></a>                <span class="p">)</span>
</span><span id="MultiheadAttention-565"><a href="#MultiheadAttention-565"><span class="linenos">565</span></a>
</span><span id="MultiheadAttention-566"><a href="#MultiheadAttention-566"><span class="linenos">566</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="o">=</span> <span class="n">make_kv_compression</span><span class="p">()</span>
</span><span id="MultiheadAttention-567"><a href="#MultiheadAttention-567"><span class="linenos">567</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiheadAttention-568"><a href="#MultiheadAttention-568"><span class="linenos">568</span></a>                <span class="n">make_kv_compression</span><span class="p">()</span> <span class="k">if</span> <span class="n">kv_compression_sharing</span> <span class="o">==</span> <span class="s1">&#39;headwise&#39;</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-569"><a href="#MultiheadAttention-569"><span class="linenos">569</span></a>            <span class="p">)</span>
</span><span id="MultiheadAttention-570"><a href="#MultiheadAttention-570"><span class="linenos">570</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="MultiheadAttention-571"><a href="#MultiheadAttention-571"><span class="linenos">571</span></a>            <span class="k">assert</span> <span class="n">n_tokens</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-572"><a href="#MultiheadAttention-572"><span class="linenos">572</span></a>            <span class="k">assert</span> <span class="n">kv_compression_sharing</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-573"><a href="#MultiheadAttention-573"><span class="linenos">573</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-574"><a href="#MultiheadAttention-574"><span class="linenos">574</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-575"><a href="#MultiheadAttention-575"><span class="linenos">575</span></a>
</span><span id="MultiheadAttention-576"><a href="#MultiheadAttention-576"><span class="linenos">576</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">]:</span>
</span><span id="MultiheadAttention-577"><a href="#MultiheadAttention-577"><span class="linenos">577</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="MultiheadAttention-578"><a href="#MultiheadAttention-578"><span class="linenos">578</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention-579"><a href="#MultiheadAttention-579"><span class="linenos">579</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="MultiheadAttention-580"><a href="#MultiheadAttention-580"><span class="linenos">580</span></a>
</span><span id="MultiheadAttention-581"><a href="#MultiheadAttention-581"><span class="linenos">581</span></a>    <span class="k">def</span> <span class="nf">_reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MultiheadAttention-582"><a href="#MultiheadAttention-582"><span class="linenos">582</span></a>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MultiheadAttention-583"><a href="#MultiheadAttention-583"><span class="linenos">583</span></a>        <span class="n">d_head</span> <span class="o">=</span> <span class="n">d</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="MultiheadAttention-584"><a href="#MultiheadAttention-584"><span class="linenos">584</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="MultiheadAttention-585"><a href="#MultiheadAttention-585"><span class="linenos">585</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)</span>
</span><span id="MultiheadAttention-586"><a href="#MultiheadAttention-586"><span class="linenos">586</span></a>            <span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention-587"><a href="#MultiheadAttention-587"><span class="linenos">587</span></a>            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)</span>
</span><span id="MultiheadAttention-588"><a href="#MultiheadAttention-588"><span class="linenos">588</span></a>        <span class="p">)</span>
</span><span id="MultiheadAttention-589"><a href="#MultiheadAttention-589"><span class="linenos">589</span></a>
</span><span id="MultiheadAttention-590"><a href="#MultiheadAttention-590"><span class="linenos">590</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_kv</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MultiheadAttention-591"><a href="#MultiheadAttention-591"><span class="linenos">591</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention-592"><a href="#MultiheadAttention-592"><span class="linenos">592</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">x_q</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">x_kv</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">x_kv</span><span class="p">)</span>
</span><span id="MultiheadAttention-593"><a href="#MultiheadAttention-593"><span class="linenos">593</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention-594"><a href="#MultiheadAttention-594"><span class="linenos">594</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention-595"><a href="#MultiheadAttention-595"><span class="linenos">595</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiheadAttention-596"><a href="#MultiheadAttention-596"><span class="linenos">596</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span>
</span><span id="MultiheadAttention-597"><a href="#MultiheadAttention-597"><span class="linenos">597</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="MultiheadAttention-598"><a href="#MultiheadAttention-598"><span class="linenos">598</span></a>                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span>
</span><span id="MultiheadAttention-599"><a href="#MultiheadAttention-599"><span class="linenos">599</span></a>            <span class="p">)(</span><span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention-600"><a href="#MultiheadAttention-600"><span class="linenos">600</span></a>
</span><span id="MultiheadAttention-601"><a href="#MultiheadAttention-601"><span class="linenos">601</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="MultiheadAttention-602"><a href="#MultiheadAttention-602"><span class="linenos">602</span></a>        <span class="n">d_head_key</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="MultiheadAttention-603"><a href="#MultiheadAttention-603"><span class="linenos">603</span></a>        <span class="n">d_head_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="MultiheadAttention-604"><a href="#MultiheadAttention-604"><span class="linenos">604</span></a>        <span class="n">n_q_tokens</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MultiheadAttention-605"><a href="#MultiheadAttention-605"><span class="linenos">605</span></a>
</span><span id="MultiheadAttention-606"><a href="#MultiheadAttention-606"><span class="linenos">606</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="MultiheadAttention-607"><a href="#MultiheadAttention-607"><span class="linenos">607</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span id="MultiheadAttention-608"><a href="#MultiheadAttention-608"><span class="linenos">608</span></a>        <span class="n">attention_logits</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_head_key</span><span class="p">)</span>
</span><span id="MultiheadAttention-609"><a href="#MultiheadAttention-609"><span class="linenos">609</span></a>        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiheadAttention-610"><a href="#MultiheadAttention-610"><span class="linenos">610</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention-611"><a href="#MultiheadAttention-611"><span class="linenos">611</span></a>            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
</span><span id="MultiheadAttention-612"><a href="#MultiheadAttention-612"><span class="linenos">612</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span id="MultiheadAttention-613"><a href="#MultiheadAttention-613"><span class="linenos">613</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiheadAttention-614"><a href="#MultiheadAttention-614"><span class="linenos">614</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">n_q_tokens</span><span class="p">,</span> <span class="n">d_head_value</span><span class="p">)</span>
</span><span id="MultiheadAttention-615"><a href="#MultiheadAttention-615"><span class="linenos">615</span></a>            <span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention-616"><a href="#MultiheadAttention-616"><span class="linenos">616</span></a>            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_q_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span> <span class="o">*</span> <span class="n">d_head_value</span><span class="p">)</span>
</span><span id="MultiheadAttention-617"><a href="#MultiheadAttention-617"><span class="linenos">617</span></a>        <span class="p">)</span>
</span><span id="MultiheadAttention-618"><a href="#MultiheadAttention-618"><span class="linenos">618</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention-619"><a href="#MultiheadAttention-619"><span class="linenos">619</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MultiheadAttention-620"><a href="#MultiheadAttention-620"><span class="linenos">620</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Multihead Attention (self-/cross-) with an optional linear attention.</p>

<ul>
<li>To learn more about Multihead Attention, see
<a href="https://arxiv.org/abs/1706.03762">"Attention Is All You Need"</a></li>
<li>To learn about the linear attention supported by this module, see
<a href="https://arxiv.org/abs/2006.04768">"Linformer: Self-Attention with Linear Complexity"</a>,</li>
</ul>

<p><strong>Shape</strong></p>

<ul>
<li>Input:
<ul>
<li><code>x_q  ~ (batch_size, n_q_tokes,  d_embedding)</code></li>
<li><code>x_kv ~ (batch_size, n_kv_tokes, d_embedding)</code></li>
</ul></li>
<li>Output: <code>(batch_size, n_q_tokes, d_embedding)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d_embedding</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">d_embedding</span><span class="o">=</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># self-attention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># cross-attention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Linformer attention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">d_embedding</span><span class="o">=</span><span class="n">d_embedding</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">kv_compression_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">kv_compression_sharing</span><span class="o">=</span><span class="s1">&#39;headwise&#39;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</code></pre>
</div>
</div>


                            <div id="MultiheadAttention.__init__" class="classattr">
                                        <input id="MultiheadAttention.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MultiheadAttention</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="o">*</span>,</span><span class="param">	<span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">n_tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">kv_compression_ratio</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">kv_compression_sharing</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;headwise&#39;</span><span class="p">,</span> <span class="s1">&#39;key-value&#39;</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="MultiheadAttention.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiheadAttention.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiheadAttention.__init__-513"><a href="#MultiheadAttention.__init__-513"><span class="linenos">513</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="MultiheadAttention.__init__-514"><a href="#MultiheadAttention.__init__-514"><span class="linenos">514</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-515"><a href="#MultiheadAttention.__init__-515"><span class="linenos">515</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-516"><a href="#MultiheadAttention.__init__-516"><span class="linenos">516</span></a>        <span class="n">d_embedding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-517"><a href="#MultiheadAttention.__init__-517"><span class="linenos">517</span></a>        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-518"><a href="#MultiheadAttention.__init__-518"><span class="linenos">518</span></a>        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-519"><a href="#MultiheadAttention.__init__-519"><span class="linenos">519</span></a>        <span class="c1"># Linformer arguments.</span>
</span><span id="MultiheadAttention.__init__-520"><a href="#MultiheadAttention.__init__-520"><span class="linenos">520</span></a>        <span class="n">n_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-521"><a href="#MultiheadAttention.__init__-521"><span class="linenos">521</span></a>        <span class="n">kv_compression_ratio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-522"><a href="#MultiheadAttention.__init__-522"><span class="linenos">522</span></a>        <span class="n">kv_compression_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="MultiheadAttention.__init__-523"><a href="#MultiheadAttention.__init__-523"><span class="linenos">523</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention.__init__-524"><a href="#MultiheadAttention.__init__-524"><span class="linenos">524</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-525"><a href="#MultiheadAttention.__init__-525"><span class="linenos">525</span></a><span class="sd">        Args:</span>
</span><span id="MultiheadAttention.__init__-526"><a href="#MultiheadAttention.__init__-526"><span class="linenos">526</span></a><span class="sd">            d_embedding: the embedding size for one token.</span>
</span><span id="MultiheadAttention.__init__-527"><a href="#MultiheadAttention.__init__-527"><span class="linenos">527</span></a><span class="sd">                Must be a multiple of `n_heads`.</span>
</span><span id="MultiheadAttention.__init__-528"><a href="#MultiheadAttention.__init__-528"><span class="linenos">528</span></a><span class="sd">            n_heads: the number of heads. If greater than 1, then the module will have</span>
</span><span id="MultiheadAttention.__init__-529"><a href="#MultiheadAttention.__init__-529"><span class="linenos">529</span></a><span class="sd">                an additional output layer (the so called &quot;mixing&quot; layer).</span>
</span><span id="MultiheadAttention.__init__-530"><a href="#MultiheadAttention.__init__-530"><span class="linenos">530</span></a><span class="sd">            dropout: the dropout rate for the attention probability map.</span>
</span><span id="MultiheadAttention.__init__-531"><a href="#MultiheadAttention.__init__-531"><span class="linenos">531</span></a><span class="sd">            n_tokens: the number of tokens</span>
</span><span id="MultiheadAttention.__init__-532"><a href="#MultiheadAttention.__init__-532"><span class="linenos">532</span></a><span class="sd">                (must be provided if `kv_compression_ratio` is not None)</span>
</span><span id="MultiheadAttention.__init__-533"><a href="#MultiheadAttention.__init__-533"><span class="linenos">533</span></a><span class="sd">            kv_compression_ratio: Linformer-style compression rate.</span>
</span><span id="MultiheadAttention.__init__-534"><a href="#MultiheadAttention.__init__-534"><span class="linenos">534</span></a><span class="sd">                Must be within the interval `(0.0, 1.0)`.</span>
</span><span id="MultiheadAttention.__init__-535"><a href="#MultiheadAttention.__init__-535"><span class="linenos">535</span></a><span class="sd">            kv_compression_sharing: Linformer compression sharing policy.</span>
</span><span id="MultiheadAttention.__init__-536"><a href="#MultiheadAttention.__init__-536"><span class="linenos">536</span></a><span class="sd">                Must be provided if `kv_compression_ratio` is not None.</span>
</span><span id="MultiheadAttention.__init__-537"><a href="#MultiheadAttention.__init__-537"><span class="linenos">537</span></a><span class="sd">                (non-shared Linformer compression is not supported; the &quot;layerwise&quot;</span>
</span><span id="MultiheadAttention.__init__-538"><a href="#MultiheadAttention.__init__-538"><span class="linenos">538</span></a><span class="sd">                sharing policy is not supported).</span>
</span><span id="MultiheadAttention.__init__-539"><a href="#MultiheadAttention.__init__-539"><span class="linenos">539</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-540"><a href="#MultiheadAttention.__init__-540"><span class="linenos">540</span></a>        <span class="k">if</span> <span class="n">n_heads</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="MultiheadAttention.__init__-541"><a href="#MultiheadAttention.__init__-541"><span class="linenos">541</span></a>            <span class="k">assert</span> <span class="n">d_embedding</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="MultiheadAttention.__init__-542"><a href="#MultiheadAttention.__init__-542"><span class="linenos">542</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MultiheadAttention.__init__-543"><a href="#MultiheadAttention.__init__-543"><span class="linenos">543</span></a>
</span><span id="MultiheadAttention.__init__-544"><a href="#MultiheadAttention.__init__-544"><span class="linenos">544</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="MultiheadAttention.__init__-545"><a href="#MultiheadAttention.__init__-545"><span class="linenos">545</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The query projection layer.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-546"><a href="#MultiheadAttention.__init__-546"><span class="linenos">546</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="MultiheadAttention.__init__-547"><a href="#MultiheadAttention.__init__-547"><span class="linenos">547</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The key projection layer.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-548"><a href="#MultiheadAttention.__init__-548"><span class="linenos">548</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span>
</span><span id="MultiheadAttention.__init__-549"><a href="#MultiheadAttention.__init__-549"><span class="linenos">549</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The value projection layer.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-550"><a href="#MultiheadAttention.__init__-550"><span class="linenos">550</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">)</span> <span class="k">if</span> <span class="n">n_heads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-551"><a href="#MultiheadAttention.__init__-551"><span class="linenos">551</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output mixing layer (presented if `n_heads &gt; 1`).&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-552"><a href="#MultiheadAttention.__init__-552"><span class="linenos">552</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span><span id="MultiheadAttention.__init__-553"><a href="#MultiheadAttention.__init__-553"><span class="linenos">553</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span> <span class="k">if</span> <span class="n">dropout</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-554"><a href="#MultiheadAttention.__init__-554"><span class="linenos">554</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The dropout for the attention probability map.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.__init__-555"><a href="#MultiheadAttention.__init__-555"><span class="linenos">555</span></a>
</span><span id="MultiheadAttention.__init__-556"><a href="#MultiheadAttention.__init__-556"><span class="linenos">556</span></a>        <span class="k">if</span> <span class="n">kv_compression_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention.__init__-557"><a href="#MultiheadAttention.__init__-557"><span class="linenos">557</span></a>            <span class="k">assert</span> <span class="n">n_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-558"><a href="#MultiheadAttention.__init__-558"><span class="linenos">558</span></a>            <span class="k">assert</span> <span class="n">kv_compression_sharing</span> <span class="ow">in</span> <span class="n">typing</span><span class="o">.</span><span class="n">get_args</span><span class="p">(</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">)</span>
</span><span id="MultiheadAttention.__init__-559"><a href="#MultiheadAttention.__init__-559"><span class="linenos">559</span></a>            <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">kv_compression_ratio</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
</span><span id="MultiheadAttention.__init__-560"><a href="#MultiheadAttention.__init__-560"><span class="linenos">560</span></a>
</span><span id="MultiheadAttention.__init__-561"><a href="#MultiheadAttention.__init__-561"><span class="linenos">561</span></a>            <span class="k">def</span> <span class="nf">make_kv_compression</span><span class="p">():</span>
</span><span id="MultiheadAttention.__init__-562"><a href="#MultiheadAttention.__init__-562"><span class="linenos">562</span></a>                <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="MultiheadAttention.__init__-563"><a href="#MultiheadAttention.__init__-563"><span class="linenos">563</span></a>                    <span class="n">n_tokens</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n_tokens</span> <span class="o">*</span> <span class="n">kv_compression_ratio</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
</span><span id="MultiheadAttention.__init__-564"><a href="#MultiheadAttention.__init__-564"><span class="linenos">564</span></a>                <span class="p">)</span>
</span><span id="MultiheadAttention.__init__-565"><a href="#MultiheadAttention.__init__-565"><span class="linenos">565</span></a>
</span><span id="MultiheadAttention.__init__-566"><a href="#MultiheadAttention.__init__-566"><span class="linenos">566</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="o">=</span> <span class="n">make_kv_compression</span><span class="p">()</span>
</span><span id="MultiheadAttention.__init__-567"><a href="#MultiheadAttention.__init__-567"><span class="linenos">567</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiheadAttention.__init__-568"><a href="#MultiheadAttention.__init__-568"><span class="linenos">568</span></a>                <span class="n">make_kv_compression</span><span class="p">()</span> <span class="k">if</span> <span class="n">kv_compression_sharing</span> <span class="o">==</span> <span class="s1">&#39;headwise&#39;</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-569"><a href="#MultiheadAttention.__init__-569"><span class="linenos">569</span></a>            <span class="p">)</span>
</span><span id="MultiheadAttention.__init__-570"><a href="#MultiheadAttention.__init__-570"><span class="linenos">570</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="MultiheadAttention.__init__-571"><a href="#MultiheadAttention.__init__-571"><span class="linenos">571</span></a>            <span class="k">assert</span> <span class="n">n_tokens</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-572"><a href="#MultiheadAttention.__init__-572"><span class="linenos">572</span></a>            <span class="k">assert</span> <span class="n">kv_compression_sharing</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-573"><a href="#MultiheadAttention.__init__-573"><span class="linenos">573</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-574"><a href="#MultiheadAttention.__init__-574"><span class="linenos">574</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.__init__-575"><a href="#MultiheadAttention.__init__-575"><span class="linenos">575</span></a>
</span><span id="MultiheadAttention.__init__-576"><a href="#MultiheadAttention.__init__-576"><span class="linenos">576</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">]:</span>
</span><span id="MultiheadAttention.__init__-577"><a href="#MultiheadAttention.__init__-577"><span class="linenos">577</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span><span id="MultiheadAttention.__init__-578"><a href="#MultiheadAttention.__init__-578"><span class="linenos">578</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention.__init__-579"><a href="#MultiheadAttention.__init__-579"><span class="linenos">579</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>d_embedding:</strong>  the embedding size for one token.
Must be a multiple of <code>n_heads</code>.</li>
<li><strong>n_heads:</strong>  the number of heads. If greater than 1, then the module will have
an additional output layer (the so called "mixing" layer).</li>
<li><strong>dropout:</strong>  the dropout rate for the attention probability map.</li>
<li><strong>n_tokens:</strong>  the number of tokens
(must be provided if <code>kv_compression_ratio</code> is not None)</li>
<li><strong>kv_compression_ratio:</strong>  Linformer-style compression rate.
Must be within the interval <code>(0.0, 1.0)</code>.</li>
<li><strong>kv_compression_sharing:</strong>  Linformer compression sharing policy.
Must be provided if <code>kv_compression_ratio</code> is not None.
(non-shared Linformer compression is not supported; the "layerwise"
sharing policy is not supported).</li>
</ul>
</div>


                            </div>
                            <div id="MultiheadAttention.W_q" class="classattr">
                                <div class="attr variable">
            <span class="name">W_q</span>

        
    </div>
    <a class="headerlink" href="#MultiheadAttention.W_q"></a>
    
            <div class="docstring"><p>The query projection layer.</p>
</div>


                            </div>
                            <div id="MultiheadAttention.W_k" class="classattr">
                                <div class="attr variable">
            <span class="name">W_k</span>

        
    </div>
    <a class="headerlink" href="#MultiheadAttention.W_k"></a>
    
            <div class="docstring"><p>The key projection layer.</p>
</div>


                            </div>
                            <div id="MultiheadAttention.W_v" class="classattr">
                                <div class="attr variable">
            <span class="name">W_v</span>

        
    </div>
    <a class="headerlink" href="#MultiheadAttention.W_v"></a>
    
            <div class="docstring"><p>The value projection layer.</p>
</div>


                            </div>
                            <div id="MultiheadAttention.W_out" class="classattr">
                                <div class="attr variable">
            <span class="name">W_out</span>

        
    </div>
    <a class="headerlink" href="#MultiheadAttention.W_out"></a>
    
            <div class="docstring"><p>The output mixing layer (presented if <code>n_heads &gt; 1</code>).</p>
</div>


                            </div>
                            <div id="MultiheadAttention.dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">dropout</span>

        
    </div>
    <a class="headerlink" href="#MultiheadAttention.dropout"></a>
    
            <div class="docstring"><p>The dropout for the attention probability map.</p>
</div>


                            </div>
                            <div id="MultiheadAttention.forward" class="classattr">
                                        <input id="MultiheadAttention.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x_q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>, </span><span class="param"><span class="n">x_kv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MultiheadAttention.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiheadAttention.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiheadAttention.forward-590"><a href="#MultiheadAttention.forward-590"><span class="linenos">590</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_kv</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MultiheadAttention.forward-591"><a href="#MultiheadAttention.forward-591"><span class="linenos">591</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="MultiheadAttention.forward-592"><a href="#MultiheadAttention.forward-592"><span class="linenos">592</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">x_q</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">x_kv</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">x_kv</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-593"><a href="#MultiheadAttention.forward-593"><span class="linenos">593</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention.forward-594"><a href="#MultiheadAttention.forward-594"><span class="linenos">594</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-595"><a href="#MultiheadAttention.forward-595"><span class="linenos">595</span></a>            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiheadAttention.forward-596"><a href="#MultiheadAttention.forward-596"><span class="linenos">596</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">key_compression</span>
</span><span id="MultiheadAttention.forward-597"><a href="#MultiheadAttention.forward-597"><span class="linenos">597</span></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="MultiheadAttention.forward-598"><a href="#MultiheadAttention.forward-598"><span class="linenos">598</span></a>                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_compression</span>
</span><span id="MultiheadAttention.forward-599"><a href="#MultiheadAttention.forward-599"><span class="linenos">599</span></a>            <span class="p">)(</span><span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-600"><a href="#MultiheadAttention.forward-600"><span class="linenos">600</span></a>
</span><span id="MultiheadAttention.forward-601"><a href="#MultiheadAttention.forward-601"><span class="linenos">601</span></a>        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-602"><a href="#MultiheadAttention.forward-602"><span class="linenos">602</span></a>        <span class="n">d_head_key</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="MultiheadAttention.forward-603"><a href="#MultiheadAttention.forward-603"><span class="linenos">603</span></a>        <span class="n">d_head_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span>
</span><span id="MultiheadAttention.forward-604"><a href="#MultiheadAttention.forward-604"><span class="linenos">604</span></a>        <span class="n">n_q_tokens</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MultiheadAttention.forward-605"><a href="#MultiheadAttention.forward-605"><span class="linenos">605</span></a>
</span><span id="MultiheadAttention.forward-606"><a href="#MultiheadAttention.forward-606"><span class="linenos">606</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-607"><a href="#MultiheadAttention.forward-607"><span class="linenos">607</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-608"><a href="#MultiheadAttention.forward-608"><span class="linenos">608</span></a>        <span class="n">attention_logits</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_head_key</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-609"><a href="#MultiheadAttention.forward-609"><span class="linenos">609</span></a>        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-610"><a href="#MultiheadAttention.forward-610"><span class="linenos">610</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention.forward-611"><a href="#MultiheadAttention.forward-611"><span class="linenos">611</span></a>            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-612"><a href="#MultiheadAttention.forward-612"><span class="linenos">612</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-613"><a href="#MultiheadAttention.forward-613"><span class="linenos">613</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="MultiheadAttention.forward-614"><a href="#MultiheadAttention.forward-614"><span class="linenos">614</span></a>            <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span><span class="p">,</span> <span class="n">n_q_tokens</span><span class="p">,</span> <span class="n">d_head_value</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-615"><a href="#MultiheadAttention.forward-615"><span class="linenos">615</span></a>            <span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-616"><a href="#MultiheadAttention.forward-616"><span class="linenos">616</span></a>            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_q_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_heads</span> <span class="o">*</span> <span class="n">d_head_value</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-617"><a href="#MultiheadAttention.forward-617"><span class="linenos">617</span></a>        <span class="p">)</span>
</span><span id="MultiheadAttention.forward-618"><a href="#MultiheadAttention.forward-618"><span class="linenos">618</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MultiheadAttention.forward-619"><a href="#MultiheadAttention.forward-619"><span class="linenos">619</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MultiheadAttention.forward-620"><a href="#MultiheadAttention.forward-620"><span class="linenos">620</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="MultiheadAttention.dump_patches" class="variable">dump_patches</dd>
                <dd id="MultiheadAttention.training" class="variable">training</dd>
                <dd id="MultiheadAttention.register_buffer" class="function">register_buffer</dd>
                <dd id="MultiheadAttention.register_parameter" class="function">register_parameter</dd>
                <dd id="MultiheadAttention.add_module" class="function">add_module</dd>
                <dd id="MultiheadAttention.apply" class="function">apply</dd>
                <dd id="MultiheadAttention.cuda" class="function">cuda</dd>
                <dd id="MultiheadAttention.xpu" class="function">xpu</dd>
                <dd id="MultiheadAttention.cpu" class="function">cpu</dd>
                <dd id="MultiheadAttention.type" class="function">type</dd>
                <dd id="MultiheadAttention.float" class="function">float</dd>
                <dd id="MultiheadAttention.double" class="function">double</dd>
                <dd id="MultiheadAttention.half" class="function">half</dd>
                <dd id="MultiheadAttention.bfloat16" class="function">bfloat16</dd>
                <dd id="MultiheadAttention.to" class="function">to</dd>
                <dd id="MultiheadAttention.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="MultiheadAttention.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="MultiheadAttention.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="MultiheadAttention.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="MultiheadAttention.state_dict" class="function">state_dict</dd>
                <dd id="MultiheadAttention.load_state_dict" class="function">load_state_dict</dd>
                <dd id="MultiheadAttention.parameters" class="function">parameters</dd>
                <dd id="MultiheadAttention.named_parameters" class="function">named_parameters</dd>
                <dd id="MultiheadAttention.buffers" class="function">buffers</dd>
                <dd id="MultiheadAttention.named_buffers" class="function">named_buffers</dd>
                <dd id="MultiheadAttention.children" class="function">children</dd>
                <dd id="MultiheadAttention.named_children" class="function">named_children</dd>
                <dd id="MultiheadAttention.modules" class="function">modules</dd>
                <dd id="MultiheadAttention.named_modules" class="function">named_modules</dd>
                <dd id="MultiheadAttention.train" class="function">train</dd>
                <dd id="MultiheadAttention.eval" class="function">eval</dd>
                <dd id="MultiheadAttention.requires_grad_" class="function">requires_grad_</dd>
                <dd id="MultiheadAttention.zero_grad" class="function">zero_grad</dd>
                <dd id="MultiheadAttention.share_memory" class="function">share_memory</dd>
                <dd id="MultiheadAttention.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="FTTransformerBackbone">
                            <input id="FTTransformerBackbone-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">FTTransformerBackbone</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="FTTransformerBackbone-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformerBackbone"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformerBackbone-630"><a href="#FTTransformerBackbone-630"><span class="linenos">630</span></a><span class="k">class</span> <span class="nc">FTTransformerBackbone</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="FTTransformerBackbone-631"><a href="#FTTransformerBackbone-631"><span class="linenos">631</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The backbone of FT-Transformer.</span>
</span><span id="FTTransformerBackbone-632"><a href="#FTTransformerBackbone-632"><span class="linenos">632</span></a>
</span><span id="FTTransformerBackbone-633"><a href="#FTTransformerBackbone-633"><span class="linenos">633</span></a><span class="sd">    For the illustration, see `FTTransformer`.</span>
</span><span id="FTTransformerBackbone-634"><a href="#FTTransformerBackbone-634"><span class="linenos">634</span></a>
</span><span id="FTTransformerBackbone-635"><a href="#FTTransformerBackbone-635"><span class="linenos">635</span></a><span class="sd">    In fact, it is almost idential to Transformer from the paper</span>
</span><span id="FTTransformerBackbone-636"><a href="#FTTransformerBackbone-636"><span class="linenos">636</span></a><span class="sd">    [&quot;Attention Is All You Need&quot;](https://arxiv.org/abs/1706.03762).</span>
</span><span id="FTTransformerBackbone-637"><a href="#FTTransformerBackbone-637"><span class="linenos">637</span></a><span class="sd">    The differences are as follows:</span>
</span><span id="FTTransformerBackbone-638"><a href="#FTTransformerBackbone-638"><span class="linenos">638</span></a><span class="sd">    - the so called &quot;PreNorm&quot; variation is used</span>
</span><span id="FTTransformerBackbone-639"><a href="#FTTransformerBackbone-639"><span class="linenos">639</span></a><span class="sd">      (`norm_first=True` in terms of `torch.nn.TransformerEncoderLayer`)</span>
</span><span id="FTTransformerBackbone-640"><a href="#FTTransformerBackbone-640"><span class="linenos">640</span></a><span class="sd">    - the very first normalization is skipped. This is **CRUCIAL** for FT-Transformer.</span>
</span><span id="FTTransformerBackbone-641"><a href="#FTTransformerBackbone-641"><span class="linenos">641</span></a><span class="sd">    - the ReGLU activation is used in the feed-forward blocks. This is unlikely to be</span>
</span><span id="FTTransformerBackbone-642"><a href="#FTTransformerBackbone-642"><span class="linenos">642</span></a><span class="sd">      crucial, but this is what we used in the paper.</span>
</span><span id="FTTransformerBackbone-643"><a href="#FTTransformerBackbone-643"><span class="linenos">643</span></a>
</span><span id="FTTransformerBackbone-644"><a href="#FTTransformerBackbone-644"><span class="linenos">644</span></a><span class="sd">    **Shape**</span>
</span><span id="FTTransformerBackbone-645"><a href="#FTTransformerBackbone-645"><span class="linenos">645</span></a>
</span><span id="FTTransformerBackbone-646"><a href="#FTTransformerBackbone-646"><span class="linenos">646</span></a><span class="sd">    - Input: `(batch_size, n_tokens, d_block)`</span>
</span><span id="FTTransformerBackbone-647"><a href="#FTTransformerBackbone-647"><span class="linenos">647</span></a><span class="sd">    - Output: `(batch_size, d_out or d_block)`</span>
</span><span id="FTTransformerBackbone-648"><a href="#FTTransformerBackbone-648"><span class="linenos">648</span></a>
</span><span id="FTTransformerBackbone-649"><a href="#FTTransformerBackbone-649"><span class="linenos">649</span></a><span class="sd">    **Examples**</span>
</span><span id="FTTransformerBackbone-650"><a href="#FTTransformerBackbone-650"><span class="linenos">650</span></a>
</span><span id="FTTransformerBackbone-651"><a href="#FTTransformerBackbone-651"><span class="linenos">651</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="FTTransformerBackbone-652"><a href="#FTTransformerBackbone-652"><span class="linenos">652</span></a><span class="sd">    &gt;&gt;&gt; n_tokens = 3</span>
</span><span id="FTTransformerBackbone-653"><a href="#FTTransformerBackbone-653"><span class="linenos">653</span></a><span class="sd">    &gt;&gt;&gt; d_block = 16</span>
</span><span id="FTTransformerBackbone-654"><a href="#FTTransformerBackbone-654"><span class="linenos">654</span></a><span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, n_tokens, d_block)</span>
</span><span id="FTTransformerBackbone-655"><a href="#FTTransformerBackbone-655"><span class="linenos">655</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="FTTransformerBackbone-656"><a href="#FTTransformerBackbone-656"><span class="linenos">656</span></a><span class="sd">    &gt;&gt;&gt; m = Transformer(</span>
</span><span id="FTTransformerBackbone-657"><a href="#FTTransformerBackbone-657"><span class="linenos">657</span></a><span class="sd">    ...     d_out=d_out,</span>
</span><span id="FTTransformerBackbone-658"><a href="#FTTransformerBackbone-658"><span class="linenos">658</span></a><span class="sd">    ...     n_blocks=2,</span>
</span><span id="FTTransformerBackbone-659"><a href="#FTTransformerBackbone-659"><span class="linenos">659</span></a><span class="sd">    ...     d_block=d_block,</span>
</span><span id="FTTransformerBackbone-660"><a href="#FTTransformerBackbone-660"><span class="linenos">660</span></a><span class="sd">    ...     attention_n_heads=8,</span>
</span><span id="FTTransformerBackbone-661"><a href="#FTTransformerBackbone-661"><span class="linenos">661</span></a><span class="sd">    ...     attention_dropout=0.2,</span>
</span><span id="FTTransformerBackbone-662"><a href="#FTTransformerBackbone-662"><span class="linenos">662</span></a><span class="sd">    ...     ffn_d_hidden=None,</span>
</span><span id="FTTransformerBackbone-663"><a href="#FTTransformerBackbone-663"><span class="linenos">663</span></a><span class="sd">    ...     ffn_d_hidden_multiplier=4 / 3,</span>
</span><span id="FTTransformerBackbone-664"><a href="#FTTransformerBackbone-664"><span class="linenos">664</span></a><span class="sd">    ...     ffn_dropout=0.1,</span>
</span><span id="FTTransformerBackbone-665"><a href="#FTTransformerBackbone-665"><span class="linenos">665</span></a><span class="sd">    ...     residual_dropout=0.0,</span>
</span><span id="FTTransformerBackbone-666"><a href="#FTTransformerBackbone-666"><span class="linenos">666</span></a><span class="sd">    ... )</span>
</span><span id="FTTransformerBackbone-667"><a href="#FTTransformerBackbone-667"><span class="linenos">667</span></a><span class="sd">    &gt;&gt;&gt; assert m(x).shape == (batch_size, d_out)</span>
</span><span id="FTTransformerBackbone-668"><a href="#FTTransformerBackbone-668"><span class="linenos">668</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-669"><a href="#FTTransformerBackbone-669"><span class="linenos">669</span></a>
</span><span id="FTTransformerBackbone-670"><a href="#FTTransformerBackbone-670"><span class="linenos">670</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-671"><a href="#FTTransformerBackbone-671"><span class="linenos">671</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-672"><a href="#FTTransformerBackbone-672"><span class="linenos">672</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-673"><a href="#FTTransformerBackbone-673"><span class="linenos">673</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="FTTransformerBackbone-674"><a href="#FTTransformerBackbone-674"><span class="linenos">674</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-675"><a href="#FTTransformerBackbone-675"><span class="linenos">675</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-676"><a href="#FTTransformerBackbone-676"><span class="linenos">676</span></a>        <span class="n">attention_n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-677"><a href="#FTTransformerBackbone-677"><span class="linenos">677</span></a>        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-678"><a href="#FTTransformerBackbone-678"><span class="linenos">678</span></a>        <span class="n">ffn_d_hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="FTTransformerBackbone-679"><a href="#FTTransformerBackbone-679"><span class="linenos">679</span></a>        <span class="n">ffn_d_hidden_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="FTTransformerBackbone-680"><a href="#FTTransformerBackbone-680"><span class="linenos">680</span></a>        <span class="n">ffn_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-681"><a href="#FTTransformerBackbone-681"><span class="linenos">681</span></a>        <span class="n">residual_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-682"><a href="#FTTransformerBackbone-682"><span class="linenos">682</span></a>        <span class="n">n_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-683"><a href="#FTTransformerBackbone-683"><span class="linenos">683</span></a>        <span class="n">attention_kv_compression_ratio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-684"><a href="#FTTransformerBackbone-684"><span class="linenos">684</span></a>        <span class="n">attention_kv_compression_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-685"><a href="#FTTransformerBackbone-685"><span class="linenos">685</span></a>    <span class="p">):</span>
</span><span id="FTTransformerBackbone-686"><a href="#FTTransformerBackbone-686"><span class="linenos">686</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-687"><a href="#FTTransformerBackbone-687"><span class="linenos">687</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformerBackbone-688"><a href="#FTTransformerBackbone-688"><span class="linenos">688</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="FTTransformerBackbone-689"><a href="#FTTransformerBackbone-689"><span class="linenos">689</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="FTTransformerBackbone-690"><a href="#FTTransformerBackbone-690"><span class="linenos">690</span></a><span class="sd">            d_block: the block width</span>
</span><span id="FTTransformerBackbone-691"><a href="#FTTransformerBackbone-691"><span class="linenos">691</span></a><span class="sd">                (or, equivalently, the embedding size of each feature).</span>
</span><span id="FTTransformerBackbone-692"><a href="#FTTransformerBackbone-692"><span class="linenos">692</span></a><span class="sd">                Must be a multiple of `attention_n_heads`.</span>
</span><span id="FTTransformerBackbone-693"><a href="#FTTransformerBackbone-693"><span class="linenos">693</span></a><span class="sd">            attention_n_heads: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone-694"><a href="#FTTransformerBackbone-694"><span class="linenos">694</span></a><span class="sd">            attention_dropout: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone-695"><a href="#FTTransformerBackbone-695"><span class="linenos">695</span></a><span class="sd">            ffn_d_hidden: the hidden representation size after the activation in the</span>
</span><span id="FTTransformerBackbone-696"><a href="#FTTransformerBackbone-696"><span class="linenos">696</span></a><span class="sd">                feed-forward blocks (or, equivalently, the *input* size of the *second*</span>
</span><span id="FTTransformerBackbone-697"><a href="#FTTransformerBackbone-697"><span class="linenos">697</span></a><span class="sd">                linear layer in the feed-forward blocks). Since `Transformer` uses ReGLU</span>
</span><span id="FTTransformerBackbone-698"><a href="#FTTransformerBackbone-698"><span class="linenos">698</span></a><span class="sd">                activation function, the *output* size of the *first*</span>
</span><span id="FTTransformerBackbone-699"><a href="#FTTransformerBackbone-699"><span class="linenos">699</span></a><span class="sd">                linear layer will be `2 * ffn_d_hidden`.</span>
</span><span id="FTTransformerBackbone-700"><a href="#FTTransformerBackbone-700"><span class="linenos">700</span></a><span class="sd">            ffn_d_hidden_multiplier: the alternative way to set `ffn_d_hidden` as</span>
</span><span id="FTTransformerBackbone-701"><a href="#FTTransformerBackbone-701"><span class="linenos">701</span></a><span class="sd">                `int(d_block * ffn_d_hidden_multiplier)`</span>
</span><span id="FTTransformerBackbone-702"><a href="#FTTransformerBackbone-702"><span class="linenos">702</span></a><span class="sd">            ffn_dropout: the dropout rate for the hidden representation</span>
</span><span id="FTTransformerBackbone-703"><a href="#FTTransformerBackbone-703"><span class="linenos">703</span></a><span class="sd">                in the feed-forward blocks.</span>
</span><span id="FTTransformerBackbone-704"><a href="#FTTransformerBackbone-704"><span class="linenos">704</span></a><span class="sd">            residual_dropout: the dropout rate for all residual branches.</span>
</span><span id="FTTransformerBackbone-705"><a href="#FTTransformerBackbone-705"><span class="linenos">705</span></a><span class="sd">            n_tokens: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone-706"><a href="#FTTransformerBackbone-706"><span class="linenos">706</span></a><span class="sd">            attention_kv_compression_ratio: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone-707"><a href="#FTTransformerBackbone-707"><span class="linenos">707</span></a><span class="sd">                Use this option with caution:</span>
</span><span id="FTTransformerBackbone-708"><a href="#FTTransformerBackbone-708"><span class="linenos">708</span></a><span class="sd">                - it can affect task performance in an unpredictable way</span>
</span><span id="FTTransformerBackbone-709"><a href="#FTTransformerBackbone-709"><span class="linenos">709</span></a><span class="sd">                - it can make things *slower* when the number of features</span>
</span><span id="FTTransformerBackbone-710"><a href="#FTTransformerBackbone-710"><span class="linenos">710</span></a><span class="sd">                  is not large enough</span>
</span><span id="FTTransformerBackbone-711"><a href="#FTTransformerBackbone-711"><span class="linenos">711</span></a><span class="sd">            attention_kv_compression_sharing: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone-712"><a href="#FTTransformerBackbone-712"><span class="linenos">712</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-713"><a href="#FTTransformerBackbone-713"><span class="linenos">713</span></a>        <span class="k">if</span> <span class="n">attention_kv_compression_sharing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformerBackbone-714"><a href="#FTTransformerBackbone-714"><span class="linenos">714</span></a>            <span class="k">assert</span> <span class="n">attention_kv_compression_sharing</span> <span class="ow">in</span> <span class="n">typing</span><span class="o">.</span><span class="n">get_args</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-715"><a href="#FTTransformerBackbone-715"><span class="linenos">715</span></a>                <span class="n">_KV_COMPRESSION_SHARING</span>
</span><span id="FTTransformerBackbone-716"><a href="#FTTransformerBackbone-716"><span class="linenos">716</span></a>            <span class="p">)</span>
</span><span id="FTTransformerBackbone-717"><a href="#FTTransformerBackbone-717"><span class="linenos">717</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">ffn_d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">ffn_d_hidden_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-718"><a href="#FTTransformerBackbone-718"><span class="linenos">718</span></a>        <span class="k">if</span> <span class="n">ffn_d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformerBackbone-719"><a href="#FTTransformerBackbone-719"><span class="linenos">719</span></a>            <span class="n">ffn_d_hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_block</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">ffn_d_hidden_multiplier</span><span class="p">))</span>
</span><span id="FTTransformerBackbone-720"><a href="#FTTransformerBackbone-720"><span class="linenos">720</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="FTTransformerBackbone-721"><a href="#FTTransformerBackbone-721"><span class="linenos">721</span></a>
</span><span id="FTTransformerBackbone-722"><a href="#FTTransformerBackbone-722"><span class="linenos">722</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cls_embedding</span> <span class="o">=</span> <span class="n">CLSEmbedding</span><span class="p">(</span><span class="n">d_block</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-723"><a href="#FTTransformerBackbone-723"><span class="linenos">723</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The [CLS]-token embedding.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-724"><a href="#FTTransformerBackbone-724"><span class="linenos">724</span></a>
</span><span id="FTTransformerBackbone-725"><a href="#FTTransformerBackbone-725"><span class="linenos">725</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-726"><a href="#FTTransformerBackbone-726"><span class="linenos">726</span></a>            <span class="p">[</span>
</span><span id="FTTransformerBackbone-727"><a href="#FTTransformerBackbone-727"><span class="linenos">727</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-728"><a href="#FTTransformerBackbone-728"><span class="linenos">728</span></a>                    <span class="p">{</span>
</span><span id="FTTransformerBackbone-729"><a href="#FTTransformerBackbone-729"><span class="linenos">729</span></a>                        <span class="c1"># &gt;&gt;&gt; attention</span>
</span><span id="FTTransformerBackbone-730"><a href="#FTTransformerBackbone-730"><span class="linenos">730</span></a>                        <span class="s1">&#39;attention&#39;</span><span class="p">:</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-731"><a href="#FTTransformerBackbone-731"><span class="linenos">731</span></a>                            <span class="n">d_embedding</span><span class="o">=</span><span class="n">d_block</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-732"><a href="#FTTransformerBackbone-732"><span class="linenos">732</span></a>                            <span class="n">n_heads</span><span class="o">=</span><span class="n">attention_n_heads</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-733"><a href="#FTTransformerBackbone-733"><span class="linenos">733</span></a>                            <span class="n">dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-734"><a href="#FTTransformerBackbone-734"><span class="linenos">734</span></a>                            <span class="n">n_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-735"><a href="#FTTransformerBackbone-735"><span class="linenos">735</span></a>                            <span class="n">kv_compression_ratio</span><span class="o">=</span><span class="n">attention_kv_compression_ratio</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-736"><a href="#FTTransformerBackbone-736"><span class="linenos">736</span></a>                            <span class="n">kv_compression_sharing</span><span class="o">=</span><span class="n">attention_kv_compression_sharing</span><span class="p">,</span>
</span><span id="FTTransformerBackbone-737"><a href="#FTTransformerBackbone-737"><span class="linenos">737</span></a>                        <span class="p">),</span>
</span><span id="FTTransformerBackbone-738"><a href="#FTTransformerBackbone-738"><span class="linenos">738</span></a>                        <span class="s1">&#39;attention_residual_dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">residual_dropout</span><span class="p">),</span>
</span><span id="FTTransformerBackbone-739"><a href="#FTTransformerBackbone-739"><span class="linenos">739</span></a>                        <span class="c1"># &gt;&gt;&gt; feed-forward</span>
</span><span id="FTTransformerBackbone-740"><a href="#FTTransformerBackbone-740"><span class="linenos">740</span></a>                        <span class="s1">&#39;ffn_normalization&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">),</span>
</span><span id="FTTransformerBackbone-741"><a href="#FTTransformerBackbone-741"><span class="linenos">741</span></a>                        <span class="s1">&#39;ffn&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-742"><a href="#FTTransformerBackbone-742"><span class="linenos">742</span></a>                            <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-743"><a href="#FTTransformerBackbone-743"><span class="linenos">743</span></a>                                <span class="p">[</span>
</span><span id="FTTransformerBackbone-744"><a href="#FTTransformerBackbone-744"><span class="linenos">744</span></a>                                    <span class="c1"># Multiplying dimension by 2 to compensate for</span>
</span><span id="FTTransformerBackbone-745"><a href="#FTTransformerBackbone-745"><span class="linenos">745</span></a>                                    <span class="c1"># ReGLU which (internally) divides dimension by 2.</span>
</span><span id="FTTransformerBackbone-746"><a href="#FTTransformerBackbone-746"><span class="linenos">746</span></a>                                    <span class="p">(</span><span class="s1">&#39;linear1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">ffn_d_hidden</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone-747"><a href="#FTTransformerBackbone-747"><span class="linenos">747</span></a>                                    <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">_ReGLU</span><span class="p">()),</span>
</span><span id="FTTransformerBackbone-748"><a href="#FTTransformerBackbone-748"><span class="linenos">748</span></a>                                    <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">ffn_dropout</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone-749"><a href="#FTTransformerBackbone-749"><span class="linenos">749</span></a>                                    <span class="p">(</span><span class="s1">&#39;linear2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ffn_d_hidden</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone-750"><a href="#FTTransformerBackbone-750"><span class="linenos">750</span></a>                                <span class="p">]</span>
</span><span id="FTTransformerBackbone-751"><a href="#FTTransformerBackbone-751"><span class="linenos">751</span></a>                            <span class="p">)</span>
</span><span id="FTTransformerBackbone-752"><a href="#FTTransformerBackbone-752"><span class="linenos">752</span></a>                        <span class="p">),</span>
</span><span id="FTTransformerBackbone-753"><a href="#FTTransformerBackbone-753"><span class="linenos">753</span></a>                        <span class="s1">&#39;ffn_residual_dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">residual_dropout</span><span class="p">),</span>
</span><span id="FTTransformerBackbone-754"><a href="#FTTransformerBackbone-754"><span class="linenos">754</span></a>                        <span class="c1"># &gt;&gt;&gt; output (for hooks-based introspection)</span>
</span><span id="FTTransformerBackbone-755"><a href="#FTTransformerBackbone-755"><span class="linenos">755</span></a>                        <span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
</span><span id="FTTransformerBackbone-756"><a href="#FTTransformerBackbone-756"><span class="linenos">756</span></a>                        <span class="c1"># &gt;&gt;&gt; the very first normalization</span>
</span><span id="FTTransformerBackbone-757"><a href="#FTTransformerBackbone-757"><span class="linenos">757</span></a>                        <span class="o">**</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-758"><a href="#FTTransformerBackbone-758"><span class="linenos">758</span></a>                            <span class="p">{}</span>
</span><span id="FTTransformerBackbone-759"><a href="#FTTransformerBackbone-759"><span class="linenos">759</span></a>                            <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="FTTransformerBackbone-760"><a href="#FTTransformerBackbone-760"><span class="linenos">760</span></a>                            <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;attention_normalization&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">)}</span>
</span><span id="FTTransformerBackbone-761"><a href="#FTTransformerBackbone-761"><span class="linenos">761</span></a>                        <span class="p">),</span>
</span><span id="FTTransformerBackbone-762"><a href="#FTTransformerBackbone-762"><span class="linenos">762</span></a>                    <span class="p">}</span>
</span><span id="FTTransformerBackbone-763"><a href="#FTTransformerBackbone-763"><span class="linenos">763</span></a>                <span class="p">)</span>
</span><span id="FTTransformerBackbone-764"><a href="#FTTransformerBackbone-764"><span class="linenos">764</span></a>                <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-765"><a href="#FTTransformerBackbone-765"><span class="linenos">765</span></a>            <span class="p">]</span>
</span><span id="FTTransformerBackbone-766"><a href="#FTTransformerBackbone-766"><span class="linenos">766</span></a>        <span class="p">)</span>
</span><span id="FTTransformerBackbone-767"><a href="#FTTransformerBackbone-767"><span class="linenos">767</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-768"><a href="#FTTransformerBackbone-768"><span class="linenos">768</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformerBackbone-769"><a href="#FTTransformerBackbone-769"><span class="linenos">769</span></a>            <span class="kc">None</span>
</span><span id="FTTransformerBackbone-770"><a href="#FTTransformerBackbone-770"><span class="linenos">770</span></a>            <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="FTTransformerBackbone-771"><a href="#FTTransformerBackbone-771"><span class="linenos">771</span></a>            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-772"><a href="#FTTransformerBackbone-772"><span class="linenos">772</span></a>                <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="FTTransformerBackbone-773"><a href="#FTTransformerBackbone-773"><span class="linenos">773</span></a>                    <span class="p">[</span>
</span><span id="FTTransformerBackbone-774"><a href="#FTTransformerBackbone-774"><span class="linenos">774</span></a>                        <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone-775"><a href="#FTTransformerBackbone-775"><span class="linenos">775</span></a>                        <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="FTTransformerBackbone-776"><a href="#FTTransformerBackbone-776"><span class="linenos">776</span></a>                        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone-777"><a href="#FTTransformerBackbone-777"><span class="linenos">777</span></a>                    <span class="p">]</span>
</span><span id="FTTransformerBackbone-778"><a href="#FTTransformerBackbone-778"><span class="linenos">778</span></a>                <span class="p">)</span>
</span><span id="FTTransformerBackbone-779"><a href="#FTTransformerBackbone-779"><span class="linenos">779</span></a>            <span class="p">)</span>
</span><span id="FTTransformerBackbone-780"><a href="#FTTransformerBackbone-780"><span class="linenos">780</span></a>        <span class="p">)</span>
</span><span id="FTTransformerBackbone-781"><a href="#FTTransformerBackbone-781"><span class="linenos">781</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-782"><a href="#FTTransformerBackbone-782"><span class="linenos">782</span></a>
</span><span id="FTTransformerBackbone-783"><a href="#FTTransformerBackbone-783"><span class="linenos">783</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FTTransformerBackbone-784"><a href="#FTTransformerBackbone-784"><span class="linenos">784</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone-785"><a href="#FTTransformerBackbone-785"><span class="linenos">785</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
</span><span id="FTTransformerBackbone-786"><a href="#FTTransformerBackbone-786"><span class="linenos">786</span></a>
</span><span id="FTTransformerBackbone-787"><a href="#FTTransformerBackbone-787"><span class="linenos">787</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-788"><a href="#FTTransformerBackbone-788"><span class="linenos">788</span></a>
</span><span id="FTTransformerBackbone-789"><a href="#FTTransformerBackbone-789"><span class="linenos">789</span></a>        <span class="n">n_blocks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-790"><a href="#FTTransformerBackbone-790"><span class="linenos">790</span></a>        <span class="k">for</span> <span class="n">i_block</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span><span id="FTTransformerBackbone-791"><a href="#FTTransformerBackbone-791"><span class="linenos">791</span></a>            <span class="n">block</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">,</span> <span class="n">block</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-792"><a href="#FTTransformerBackbone-792"><span class="linenos">792</span></a>
</span><span id="FTTransformerBackbone-793"><a href="#FTTransformerBackbone-793"><span class="linenos">793</span></a>            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone-794"><a href="#FTTransformerBackbone-794"><span class="linenos">794</span></a>            <span class="k">if</span> <span class="s1">&#39;attention_normalization&#39;</span> <span class="ow">in</span> <span class="n">block</span><span class="p">:</span>
</span><span id="FTTransformerBackbone-795"><a href="#FTTransformerBackbone-795"><span class="linenos">795</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention_normalization&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-796"><a href="#FTTransformerBackbone-796"><span class="linenos">796</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i_block</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-797"><a href="#FTTransformerBackbone-797"><span class="linenos">797</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention_residual_dropout&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-798"><a href="#FTTransformerBackbone-798"><span class="linenos">798</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x_identity</span> <span class="o">+</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone-799"><a href="#FTTransformerBackbone-799"><span class="linenos">799</span></a>
</span><span id="FTTransformerBackbone-800"><a href="#FTTransformerBackbone-800"><span class="linenos">800</span></a>            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone-801"><a href="#FTTransformerBackbone-801"><span class="linenos">801</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn_normalization&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-802"><a href="#FTTransformerBackbone-802"><span class="linenos">802</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-803"><a href="#FTTransformerBackbone-803"><span class="linenos">803</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn_residual_dropout&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-804"><a href="#FTTransformerBackbone-804"><span class="linenos">804</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x_identity</span> <span class="o">+</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone-805"><a href="#FTTransformerBackbone-805"><span class="linenos">805</span></a>
</span><span id="FTTransformerBackbone-806"><a href="#FTTransformerBackbone-806"><span class="linenos">806</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-807"><a href="#FTTransformerBackbone-807"><span class="linenos">807</span></a>
</span><span id="FTTransformerBackbone-808"><a href="#FTTransformerBackbone-808"><span class="linenos">808</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># The representation of [CLS]-token.</span>
</span><span id="FTTransformerBackbone-809"><a href="#FTTransformerBackbone-809"><span class="linenos">809</span></a>
</span><span id="FTTransformerBackbone-810"><a href="#FTTransformerBackbone-810"><span class="linenos">810</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformerBackbone-811"><a href="#FTTransformerBackbone-811"><span class="linenos">811</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone-812"><a href="#FTTransformerBackbone-812"><span class="linenos">812</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>The backbone of FT-Transformer.</p>

<p>For the illustration, see <code><a href="#FTTransformer">FTTransformer</a></code>.</p>

<p>In fact, it is almost idential to Transformer from the paper
<a href="https://arxiv.org/abs/1706.03762">"Attention Is All You Need"</a>.
The differences are as follows:</p>

<ul>
<li>the so called "PreNorm" variation is used
(<code>norm_first=True</code> in terms of <code>torch.nn.TransformerEncoderLayer</code>)</li>
<li>the very first normalization is skipped. This is <strong>CRUCIAL</strong> for FT-Transformer.</li>
<li>the ReGLU activation is used in the feed-forward blocks. This is unlikely to be
crucial, but this is what we used in the paper.</li>
</ul>

<p><strong>Shape</strong></p>

<ul>
<li>Input: <code>(batch_size, n_tokens, d_block)</code></li>
<li>Output: <code>(batch_size, d_out or d_block)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_block</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_out</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">d_out</span><span class="o">=</span><span class="n">d_out</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_block</span><span class="o">=</span><span class="n">d_block</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">attention_n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ffn_d_hidden</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ffn_d_hidden_multiplier</span><span class="o">=</span><span class="mi">4</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ffn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">residual_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</code></pre>
</div>
</div>


                            <div id="FTTransformerBackbone.__init__" class="classattr">
                                        <input id="FTTransformerBackbone.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">FTTransformerBackbone</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="o">*</span>,</span><span class="param">	<span class="n">d_out</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">attention_n_heads</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">ffn_d_hidden</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">ffn_d_hidden_multiplier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">ffn_dropout</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">residual_dropout</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">n_tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">attention_kv_compression_ratio</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">attention_kv_compression_sharing</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;headwise&#39;</span><span class="p">,</span> <span class="s1">&#39;key-value&#39;</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="FTTransformerBackbone.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformerBackbone.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformerBackbone.__init__-670"><a href="#FTTransformerBackbone.__init__-670"><span class="linenos">670</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-671"><a href="#FTTransformerBackbone.__init__-671"><span class="linenos">671</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-672"><a href="#FTTransformerBackbone.__init__-672"><span class="linenos">672</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-673"><a href="#FTTransformerBackbone.__init__-673"><span class="linenos">673</span></a>        <span class="n">d_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="FTTransformerBackbone.__init__-674"><a href="#FTTransformerBackbone.__init__-674"><span class="linenos">674</span></a>        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-675"><a href="#FTTransformerBackbone.__init__-675"><span class="linenos">675</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-676"><a href="#FTTransformerBackbone.__init__-676"><span class="linenos">676</span></a>        <span class="n">attention_n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-677"><a href="#FTTransformerBackbone.__init__-677"><span class="linenos">677</span></a>        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-678"><a href="#FTTransformerBackbone.__init__-678"><span class="linenos">678</span></a>        <span class="n">ffn_d_hidden</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="FTTransformerBackbone.__init__-679"><a href="#FTTransformerBackbone.__init__-679"><span class="linenos">679</span></a>        <span class="n">ffn_d_hidden_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="FTTransformerBackbone.__init__-680"><a href="#FTTransformerBackbone.__init__-680"><span class="linenos">680</span></a>        <span class="n">ffn_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-681"><a href="#FTTransformerBackbone.__init__-681"><span class="linenos">681</span></a>        <span class="n">residual_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-682"><a href="#FTTransformerBackbone.__init__-682"><span class="linenos">682</span></a>        <span class="n">n_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-683"><a href="#FTTransformerBackbone.__init__-683"><span class="linenos">683</span></a>        <span class="n">attention_kv_compression_ratio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-684"><a href="#FTTransformerBackbone.__init__-684"><span class="linenos">684</span></a>        <span class="n">attention_kv_compression_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_KV_COMPRESSION_SHARING</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-685"><a href="#FTTransformerBackbone.__init__-685"><span class="linenos">685</span></a>    <span class="p">):</span>
</span><span id="FTTransformerBackbone.__init__-686"><a href="#FTTransformerBackbone.__init__-686"><span class="linenos">686</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone.__init__-687"><a href="#FTTransformerBackbone.__init__-687"><span class="linenos">687</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformerBackbone.__init__-688"><a href="#FTTransformerBackbone.__init__-688"><span class="linenos">688</span></a><span class="sd">            d_out: the output size.</span>
</span><span id="FTTransformerBackbone.__init__-689"><a href="#FTTransformerBackbone.__init__-689"><span class="linenos">689</span></a><span class="sd">            n_blocks: the number of blocks.</span>
</span><span id="FTTransformerBackbone.__init__-690"><a href="#FTTransformerBackbone.__init__-690"><span class="linenos">690</span></a><span class="sd">            d_block: the block width</span>
</span><span id="FTTransformerBackbone.__init__-691"><a href="#FTTransformerBackbone.__init__-691"><span class="linenos">691</span></a><span class="sd">                (or, equivalently, the embedding size of each feature).</span>
</span><span id="FTTransformerBackbone.__init__-692"><a href="#FTTransformerBackbone.__init__-692"><span class="linenos">692</span></a><span class="sd">                Must be a multiple of `attention_n_heads`.</span>
</span><span id="FTTransformerBackbone.__init__-693"><a href="#FTTransformerBackbone.__init__-693"><span class="linenos">693</span></a><span class="sd">            attention_n_heads: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone.__init__-694"><a href="#FTTransformerBackbone.__init__-694"><span class="linenos">694</span></a><span class="sd">            attention_dropout: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone.__init__-695"><a href="#FTTransformerBackbone.__init__-695"><span class="linenos">695</span></a><span class="sd">            ffn_d_hidden: the hidden representation size after the activation in the</span>
</span><span id="FTTransformerBackbone.__init__-696"><a href="#FTTransformerBackbone.__init__-696"><span class="linenos">696</span></a><span class="sd">                feed-forward blocks (or, equivalently, the *input* size of the *second*</span>
</span><span id="FTTransformerBackbone.__init__-697"><a href="#FTTransformerBackbone.__init__-697"><span class="linenos">697</span></a><span class="sd">                linear layer in the feed-forward blocks). Since `Transformer` uses ReGLU</span>
</span><span id="FTTransformerBackbone.__init__-698"><a href="#FTTransformerBackbone.__init__-698"><span class="linenos">698</span></a><span class="sd">                activation function, the *output* size of the *first*</span>
</span><span id="FTTransformerBackbone.__init__-699"><a href="#FTTransformerBackbone.__init__-699"><span class="linenos">699</span></a><span class="sd">                linear layer will be `2 * ffn_d_hidden`.</span>
</span><span id="FTTransformerBackbone.__init__-700"><a href="#FTTransformerBackbone.__init__-700"><span class="linenos">700</span></a><span class="sd">            ffn_d_hidden_multiplier: the alternative way to set `ffn_d_hidden` as</span>
</span><span id="FTTransformerBackbone.__init__-701"><a href="#FTTransformerBackbone.__init__-701"><span class="linenos">701</span></a><span class="sd">                `int(d_block * ffn_d_hidden_multiplier)`</span>
</span><span id="FTTransformerBackbone.__init__-702"><a href="#FTTransformerBackbone.__init__-702"><span class="linenos">702</span></a><span class="sd">            ffn_dropout: the dropout rate for the hidden representation</span>
</span><span id="FTTransformerBackbone.__init__-703"><a href="#FTTransformerBackbone.__init__-703"><span class="linenos">703</span></a><span class="sd">                in the feed-forward blocks.</span>
</span><span id="FTTransformerBackbone.__init__-704"><a href="#FTTransformerBackbone.__init__-704"><span class="linenos">704</span></a><span class="sd">            residual_dropout: the dropout rate for all residual branches.</span>
</span><span id="FTTransformerBackbone.__init__-705"><a href="#FTTransformerBackbone.__init__-705"><span class="linenos">705</span></a><span class="sd">            n_tokens: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone.__init__-706"><a href="#FTTransformerBackbone.__init__-706"><span class="linenos">706</span></a><span class="sd">            attention_kv_compression_ratio: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone.__init__-707"><a href="#FTTransformerBackbone.__init__-707"><span class="linenos">707</span></a><span class="sd">                Use this option with caution:</span>
</span><span id="FTTransformerBackbone.__init__-708"><a href="#FTTransformerBackbone.__init__-708"><span class="linenos">708</span></a><span class="sd">                - it can affect task performance in an unpredictable way</span>
</span><span id="FTTransformerBackbone.__init__-709"><a href="#FTTransformerBackbone.__init__-709"><span class="linenos">709</span></a><span class="sd">                - it can make things *slower* when the number of features</span>
</span><span id="FTTransformerBackbone.__init__-710"><a href="#FTTransformerBackbone.__init__-710"><span class="linenos">710</span></a><span class="sd">                  is not large enough</span>
</span><span id="FTTransformerBackbone.__init__-711"><a href="#FTTransformerBackbone.__init__-711"><span class="linenos">711</span></a><span class="sd">            attention_kv_compression_sharing: the argument for `MultiheadAttention`.</span>
</span><span id="FTTransformerBackbone.__init__-712"><a href="#FTTransformerBackbone.__init__-712"><span class="linenos">712</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone.__init__-713"><a href="#FTTransformerBackbone.__init__-713"><span class="linenos">713</span></a>        <span class="k">if</span> <span class="n">attention_kv_compression_sharing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformerBackbone.__init__-714"><a href="#FTTransformerBackbone.__init__-714"><span class="linenos">714</span></a>            <span class="k">assert</span> <span class="n">attention_kv_compression_sharing</span> <span class="ow">in</span> <span class="n">typing</span><span class="o">.</span><span class="n">get_args</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-715"><a href="#FTTransformerBackbone.__init__-715"><span class="linenos">715</span></a>                <span class="n">_KV_COMPRESSION_SHARING</span>
</span><span id="FTTransformerBackbone.__init__-716"><a href="#FTTransformerBackbone.__init__-716"><span class="linenos">716</span></a>            <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-717"><a href="#FTTransformerBackbone.__init__-717"><span class="linenos">717</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">ffn_d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">ffn_d_hidden_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-718"><a href="#FTTransformerBackbone.__init__-718"><span class="linenos">718</span></a>        <span class="k">if</span> <span class="n">ffn_d_hidden</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformerBackbone.__init__-719"><a href="#FTTransformerBackbone.__init__-719"><span class="linenos">719</span></a>            <span class="n">ffn_d_hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_block</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">ffn_d_hidden_multiplier</span><span class="p">))</span>
</span><span id="FTTransformerBackbone.__init__-720"><a href="#FTTransformerBackbone.__init__-720"><span class="linenos">720</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="FTTransformerBackbone.__init__-721"><a href="#FTTransformerBackbone.__init__-721"><span class="linenos">721</span></a>
</span><span id="FTTransformerBackbone.__init__-722"><a href="#FTTransformerBackbone.__init__-722"><span class="linenos">722</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cls_embedding</span> <span class="o">=</span> <span class="n">CLSEmbedding</span><span class="p">(</span><span class="n">d_block</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-723"><a href="#FTTransformerBackbone.__init__-723"><span class="linenos">723</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The [CLS]-token embedding.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone.__init__-724"><a href="#FTTransformerBackbone.__init__-724"><span class="linenos">724</span></a>
</span><span id="FTTransformerBackbone.__init__-725"><a href="#FTTransformerBackbone.__init__-725"><span class="linenos">725</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-726"><a href="#FTTransformerBackbone.__init__-726"><span class="linenos">726</span></a>            <span class="p">[</span>
</span><span id="FTTransformerBackbone.__init__-727"><a href="#FTTransformerBackbone.__init__-727"><span class="linenos">727</span></a>                <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-728"><a href="#FTTransformerBackbone.__init__-728"><span class="linenos">728</span></a>                    <span class="p">{</span>
</span><span id="FTTransformerBackbone.__init__-729"><a href="#FTTransformerBackbone.__init__-729"><span class="linenos">729</span></a>                        <span class="c1"># &gt;&gt;&gt; attention</span>
</span><span id="FTTransformerBackbone.__init__-730"><a href="#FTTransformerBackbone.__init__-730"><span class="linenos">730</span></a>                        <span class="s1">&#39;attention&#39;</span><span class="p">:</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-731"><a href="#FTTransformerBackbone.__init__-731"><span class="linenos">731</span></a>                            <span class="n">d_embedding</span><span class="o">=</span><span class="n">d_block</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-732"><a href="#FTTransformerBackbone.__init__-732"><span class="linenos">732</span></a>                            <span class="n">n_heads</span><span class="o">=</span><span class="n">attention_n_heads</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-733"><a href="#FTTransformerBackbone.__init__-733"><span class="linenos">733</span></a>                            <span class="n">dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-734"><a href="#FTTransformerBackbone.__init__-734"><span class="linenos">734</span></a>                            <span class="n">n_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-735"><a href="#FTTransformerBackbone.__init__-735"><span class="linenos">735</span></a>                            <span class="n">kv_compression_ratio</span><span class="o">=</span><span class="n">attention_kv_compression_ratio</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-736"><a href="#FTTransformerBackbone.__init__-736"><span class="linenos">736</span></a>                            <span class="n">kv_compression_sharing</span><span class="o">=</span><span class="n">attention_kv_compression_sharing</span><span class="p">,</span>
</span><span id="FTTransformerBackbone.__init__-737"><a href="#FTTransformerBackbone.__init__-737"><span class="linenos">737</span></a>                        <span class="p">),</span>
</span><span id="FTTransformerBackbone.__init__-738"><a href="#FTTransformerBackbone.__init__-738"><span class="linenos">738</span></a>                        <span class="s1">&#39;attention_residual_dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">residual_dropout</span><span class="p">),</span>
</span><span id="FTTransformerBackbone.__init__-739"><a href="#FTTransformerBackbone.__init__-739"><span class="linenos">739</span></a>                        <span class="c1"># &gt;&gt;&gt; feed-forward</span>
</span><span id="FTTransformerBackbone.__init__-740"><a href="#FTTransformerBackbone.__init__-740"><span class="linenos">740</span></a>                        <span class="s1">&#39;ffn_normalization&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">),</span>
</span><span id="FTTransformerBackbone.__init__-741"><a href="#FTTransformerBackbone.__init__-741"><span class="linenos">741</span></a>                        <span class="s1">&#39;ffn&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-742"><a href="#FTTransformerBackbone.__init__-742"><span class="linenos">742</span></a>                            <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-743"><a href="#FTTransformerBackbone.__init__-743"><span class="linenos">743</span></a>                                <span class="p">[</span>
</span><span id="FTTransformerBackbone.__init__-744"><a href="#FTTransformerBackbone.__init__-744"><span class="linenos">744</span></a>                                    <span class="c1"># Multiplying dimension by 2 to compensate for</span>
</span><span id="FTTransformerBackbone.__init__-745"><a href="#FTTransformerBackbone.__init__-745"><span class="linenos">745</span></a>                                    <span class="c1"># ReGLU which (internally) divides dimension by 2.</span>
</span><span id="FTTransformerBackbone.__init__-746"><a href="#FTTransformerBackbone.__init__-746"><span class="linenos">746</span></a>                                    <span class="p">(</span><span class="s1">&#39;linear1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">ffn_d_hidden</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone.__init__-747"><a href="#FTTransformerBackbone.__init__-747"><span class="linenos">747</span></a>                                    <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">_ReGLU</span><span class="p">()),</span>
</span><span id="FTTransformerBackbone.__init__-748"><a href="#FTTransformerBackbone.__init__-748"><span class="linenos">748</span></a>                                    <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">ffn_dropout</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone.__init__-749"><a href="#FTTransformerBackbone.__init__-749"><span class="linenos">749</span></a>                                    <span class="p">(</span><span class="s1">&#39;linear2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ffn_d_hidden</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone.__init__-750"><a href="#FTTransformerBackbone.__init__-750"><span class="linenos">750</span></a>                                <span class="p">]</span>
</span><span id="FTTransformerBackbone.__init__-751"><a href="#FTTransformerBackbone.__init__-751"><span class="linenos">751</span></a>                            <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-752"><a href="#FTTransformerBackbone.__init__-752"><span class="linenos">752</span></a>                        <span class="p">),</span>
</span><span id="FTTransformerBackbone.__init__-753"><a href="#FTTransformerBackbone.__init__-753"><span class="linenos">753</span></a>                        <span class="s1">&#39;ffn_residual_dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">residual_dropout</span><span class="p">),</span>
</span><span id="FTTransformerBackbone.__init__-754"><a href="#FTTransformerBackbone.__init__-754"><span class="linenos">754</span></a>                        <span class="c1"># &gt;&gt;&gt; output (for hooks-based introspection)</span>
</span><span id="FTTransformerBackbone.__init__-755"><a href="#FTTransformerBackbone.__init__-755"><span class="linenos">755</span></a>                        <span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
</span><span id="FTTransformerBackbone.__init__-756"><a href="#FTTransformerBackbone.__init__-756"><span class="linenos">756</span></a>                        <span class="c1"># &gt;&gt;&gt; the very first normalization</span>
</span><span id="FTTransformerBackbone.__init__-757"><a href="#FTTransformerBackbone.__init__-757"><span class="linenos">757</span></a>                        <span class="o">**</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-758"><a href="#FTTransformerBackbone.__init__-758"><span class="linenos">758</span></a>                            <span class="p">{}</span>
</span><span id="FTTransformerBackbone.__init__-759"><a href="#FTTransformerBackbone.__init__-759"><span class="linenos">759</span></a>                            <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="FTTransformerBackbone.__init__-760"><a href="#FTTransformerBackbone.__init__-760"><span class="linenos">760</span></a>                            <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;attention_normalization&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">)}</span>
</span><span id="FTTransformerBackbone.__init__-761"><a href="#FTTransformerBackbone.__init__-761"><span class="linenos">761</span></a>                        <span class="p">),</span>
</span><span id="FTTransformerBackbone.__init__-762"><a href="#FTTransformerBackbone.__init__-762"><span class="linenos">762</span></a>                    <span class="p">}</span>
</span><span id="FTTransformerBackbone.__init__-763"><a href="#FTTransformerBackbone.__init__-763"><span class="linenos">763</span></a>                <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-764"><a href="#FTTransformerBackbone.__init__-764"><span class="linenos">764</span></a>                <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-765"><a href="#FTTransformerBackbone.__init__-765"><span class="linenos">765</span></a>            <span class="p">]</span>
</span><span id="FTTransformerBackbone.__init__-766"><a href="#FTTransformerBackbone.__init__-766"><span class="linenos">766</span></a>        <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-767"><a href="#FTTransformerBackbone.__init__-767"><span class="linenos">767</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The blocks.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone.__init__-768"><a href="#FTTransformerBackbone.__init__-768"><span class="linenos">768</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-769"><a href="#FTTransformerBackbone.__init__-769"><span class="linenos">769</span></a>            <span class="kc">None</span>
</span><span id="FTTransformerBackbone.__init__-770"><a href="#FTTransformerBackbone.__init__-770"><span class="linenos">770</span></a>            <span class="k">if</span> <span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span>
</span><span id="FTTransformerBackbone.__init__-771"><a href="#FTTransformerBackbone.__init__-771"><span class="linenos">771</span></a>            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-772"><a href="#FTTransformerBackbone.__init__-772"><span class="linenos">772</span></a>                <span class="n">OrderedDict</span><span class="p">(</span>
</span><span id="FTTransformerBackbone.__init__-773"><a href="#FTTransformerBackbone.__init__-773"><span class="linenos">773</span></a>                    <span class="p">[</span>
</span><span id="FTTransformerBackbone.__init__-774"><a href="#FTTransformerBackbone.__init__-774"><span class="linenos">774</span></a>                        <span class="p">(</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_block</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone.__init__-775"><a href="#FTTransformerBackbone.__init__-775"><span class="linenos">775</span></a>                        <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
</span><span id="FTTransformerBackbone.__init__-776"><a href="#FTTransformerBackbone.__init__-776"><span class="linenos">776</span></a>                        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_block</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)),</span>
</span><span id="FTTransformerBackbone.__init__-777"><a href="#FTTransformerBackbone.__init__-777"><span class="linenos">777</span></a>                    <span class="p">]</span>
</span><span id="FTTransformerBackbone.__init__-778"><a href="#FTTransformerBackbone.__init__-778"><span class="linenos">778</span></a>                <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-779"><a href="#FTTransformerBackbone.__init__-779"><span class="linenos">779</span></a>            <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-780"><a href="#FTTransformerBackbone.__init__-780"><span class="linenos">780</span></a>        <span class="p">)</span>
</span><span id="FTTransformerBackbone.__init__-781"><a href="#FTTransformerBackbone.__init__-781"><span class="linenos">781</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The output module.&quot;&quot;&quot;</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>d_out:</strong>  the output size.</li>
<li><strong>n_blocks:</strong>  the number of blocks.</li>
<li><strong>d_block:</strong>  the block width
(or, equivalently, the embedding size of each feature).
Must be a multiple of <code>attention_n_heads</code>.</li>
<li><strong>attention_n_heads:</strong>  the argument for <code><a href="#MultiheadAttention">MultiheadAttention</a></code>.</li>
<li><strong>attention_dropout:</strong>  the argument for <code><a href="#MultiheadAttention">MultiheadAttention</a></code>.</li>
<li><strong>ffn_d_hidden:</strong>  the hidden representation size after the activation in the
feed-forward blocks (or, equivalently, the <em>input</em> size of the <em>second</em>
linear layer in the feed-forward blocks). Since <code>Transformer</code> uses ReGLU
activation function, the <em>output</em> size of the <em>first</em>
linear layer will be <code>2 * ffn_d_hidden</code>.</li>
<li><strong>ffn_d_hidden_multiplier:</strong>  the alternative way to set <code>ffn_d_hidden</code> as
<code>int(d_block * ffn_d_hidden_multiplier)</code></li>
<li><strong>ffn_dropout:</strong>  the dropout rate for the hidden representation
in the feed-forward blocks.</li>
<li><strong>residual_dropout:</strong>  the dropout rate for all residual branches.</li>
<li><strong>n_tokens:</strong>  the argument for <code><a href="#MultiheadAttention">MultiheadAttention</a></code>.</li>
<li><strong>attention_kv_compression_ratio:</strong>  the argument for <code><a href="#MultiheadAttention">MultiheadAttention</a></code>.
Use this option with caution:
<ul>
<li>it can affect task performance in an unpredictable way</li>
<li>it can make things <em>slower</em> when the number of features
is not large enough</li>
</ul></li>
<li><strong>attention_kv_compression_sharing:</strong>  the argument for <code><a href="#MultiheadAttention">MultiheadAttention</a></code>.</li>
</ul>
</div>


                            </div>
                            <div id="FTTransformerBackbone.cls_embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">cls_embedding</span>

        
    </div>
    <a class="headerlink" href="#FTTransformerBackbone.cls_embedding"></a>
    
            <div class="docstring"><p>The [CLS]-token embedding.</p>
</div>


                            </div>
                            <div id="FTTransformerBackbone.blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">blocks</span>

        
    </div>
    <a class="headerlink" href="#FTTransformerBackbone.blocks"></a>
    
            <div class="docstring"><p>The blocks.</p>
</div>


                            </div>
                            <div id="FTTransformerBackbone.output" class="classattr">
                                <div class="attr variable">
            <span class="name">output</span>

        
    </div>
    <a class="headerlink" href="#FTTransformerBackbone.output"></a>
    
            <div class="docstring"><p>The output module.</p>
</div>


                            </div>
                            <div id="FTTransformerBackbone.forward" class="classattr">
                                        <input id="FTTransformerBackbone.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FTTransformerBackbone.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformerBackbone.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformerBackbone.forward-783"><a href="#FTTransformerBackbone.forward-783"><span class="linenos">783</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FTTransformerBackbone.forward-784"><a href="#FTTransformerBackbone.forward-784"><span class="linenos">784</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.&quot;&quot;&quot;</span>
</span><span id="FTTransformerBackbone.forward-785"><a href="#FTTransformerBackbone.forward-785"><span class="linenos">785</span></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
</span><span id="FTTransformerBackbone.forward-786"><a href="#FTTransformerBackbone.forward-786"><span class="linenos">786</span></a>
</span><span id="FTTransformerBackbone.forward-787"><a href="#FTTransformerBackbone.forward-787"><span class="linenos">787</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-788"><a href="#FTTransformerBackbone.forward-788"><span class="linenos">788</span></a>
</span><span id="FTTransformerBackbone.forward-789"><a href="#FTTransformerBackbone.forward-789"><span class="linenos">789</span></a>        <span class="n">n_blocks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-790"><a href="#FTTransformerBackbone.forward-790"><span class="linenos">790</span></a>        <span class="k">for</span> <span class="n">i_block</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span><span id="FTTransformerBackbone.forward-791"><a href="#FTTransformerBackbone.forward-791"><span class="linenos">791</span></a>            <span class="n">block</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">,</span> <span class="n">block</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-792"><a href="#FTTransformerBackbone.forward-792"><span class="linenos">792</span></a>
</span><span id="FTTransformerBackbone.forward-793"><a href="#FTTransformerBackbone.forward-793"><span class="linenos">793</span></a>            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone.forward-794"><a href="#FTTransformerBackbone.forward-794"><span class="linenos">794</span></a>            <span class="k">if</span> <span class="s1">&#39;attention_normalization&#39;</span> <span class="ow">in</span> <span class="n">block</span><span class="p">:</span>
</span><span id="FTTransformerBackbone.forward-795"><a href="#FTTransformerBackbone.forward-795"><span class="linenos">795</span></a>                <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention_normalization&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-796"><a href="#FTTransformerBackbone.forward-796"><span class="linenos">796</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i_block</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-797"><a href="#FTTransformerBackbone.forward-797"><span class="linenos">797</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;attention_residual_dropout&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-798"><a href="#FTTransformerBackbone.forward-798"><span class="linenos">798</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x_identity</span> <span class="o">+</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone.forward-799"><a href="#FTTransformerBackbone.forward-799"><span class="linenos">799</span></a>
</span><span id="FTTransformerBackbone.forward-800"><a href="#FTTransformerBackbone.forward-800"><span class="linenos">800</span></a>            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone.forward-801"><a href="#FTTransformerBackbone.forward-801"><span class="linenos">801</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn_normalization&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-802"><a href="#FTTransformerBackbone.forward-802"><span class="linenos">802</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-803"><a href="#FTTransformerBackbone.forward-803"><span class="linenos">803</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;ffn_residual_dropout&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-804"><a href="#FTTransformerBackbone.forward-804"><span class="linenos">804</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">x_identity</span> <span class="o">+</span> <span class="n">x</span>
</span><span id="FTTransformerBackbone.forward-805"><a href="#FTTransformerBackbone.forward-805"><span class="linenos">805</span></a>
</span><span id="FTTransformerBackbone.forward-806"><a href="#FTTransformerBackbone.forward-806"><span class="linenos">806</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-807"><a href="#FTTransformerBackbone.forward-807"><span class="linenos">807</span></a>
</span><span id="FTTransformerBackbone.forward-808"><a href="#FTTransformerBackbone.forward-808"><span class="linenos">808</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># The representation of [CLS]-token.</span>
</span><span id="FTTransformerBackbone.forward-809"><a href="#FTTransformerBackbone.forward-809"><span class="linenos">809</span></a>
</span><span id="FTTransformerBackbone.forward-810"><a href="#FTTransformerBackbone.forward-810"><span class="linenos">810</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformerBackbone.forward-811"><a href="#FTTransformerBackbone.forward-811"><span class="linenos">811</span></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformerBackbone.forward-812"><a href="#FTTransformerBackbone.forward-812"><span class="linenos">812</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="FTTransformerBackbone.dump_patches" class="variable">dump_patches</dd>
                <dd id="FTTransformerBackbone.training" class="variable">training</dd>
                <dd id="FTTransformerBackbone.register_buffer" class="function">register_buffer</dd>
                <dd id="FTTransformerBackbone.register_parameter" class="function">register_parameter</dd>
                <dd id="FTTransformerBackbone.add_module" class="function">add_module</dd>
                <dd id="FTTransformerBackbone.apply" class="function">apply</dd>
                <dd id="FTTransformerBackbone.cuda" class="function">cuda</dd>
                <dd id="FTTransformerBackbone.xpu" class="function">xpu</dd>
                <dd id="FTTransformerBackbone.cpu" class="function">cpu</dd>
                <dd id="FTTransformerBackbone.type" class="function">type</dd>
                <dd id="FTTransformerBackbone.float" class="function">float</dd>
                <dd id="FTTransformerBackbone.double" class="function">double</dd>
                <dd id="FTTransformerBackbone.half" class="function">half</dd>
                <dd id="FTTransformerBackbone.bfloat16" class="function">bfloat16</dd>
                <dd id="FTTransformerBackbone.to" class="function">to</dd>
                <dd id="FTTransformerBackbone.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="FTTransformerBackbone.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="FTTransformerBackbone.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="FTTransformerBackbone.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="FTTransformerBackbone.state_dict" class="function">state_dict</dd>
                <dd id="FTTransformerBackbone.load_state_dict" class="function">load_state_dict</dd>
                <dd id="FTTransformerBackbone.parameters" class="function">parameters</dd>
                <dd id="FTTransformerBackbone.named_parameters" class="function">named_parameters</dd>
                <dd id="FTTransformerBackbone.buffers" class="function">buffers</dd>
                <dd id="FTTransformerBackbone.named_buffers" class="function">named_buffers</dd>
                <dd id="FTTransformerBackbone.children" class="function">children</dd>
                <dd id="FTTransformerBackbone.named_children" class="function">named_children</dd>
                <dd id="FTTransformerBackbone.modules" class="function">modules</dd>
                <dd id="FTTransformerBackbone.named_modules" class="function">named_modules</dd>
                <dd id="FTTransformerBackbone.train" class="function">train</dd>
                <dd id="FTTransformerBackbone.eval" class="function">eval</dd>
                <dd id="FTTransformerBackbone.requires_grad_" class="function">requires_grad_</dd>
                <dd id="FTTransformerBackbone.zero_grad" class="function">zero_grad</dd>
                <dd id="FTTransformerBackbone.share_memory" class="function">share_memory</dd>
                <dd id="FTTransformerBackbone.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="FTTransformer">
                            <input id="FTTransformer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">FTTransformer</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="FTTransformer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformer-815"><a href="#FTTransformer-815"><span class="linenos"> 815</span></a><span class="k">class</span> <span class="nc">FTTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="FTTransformer-816"><a href="#FTTransformer-816"><span class="linenos"> 816</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;The FT-Transformer model from Section 3.3 in the paper.</span>
</span><span id="FTTransformer-817"><a href="#FTTransformer-817"><span class="linenos"> 817</span></a>
</span><span id="FTTransformer-818"><a href="#FTTransformer-818"><span class="linenos"> 818</span></a><span class="sd">    &lt;img src=&quot;ft-transformer-overview.png&quot; width=100%&gt;</span>
</span><span id="FTTransformer-819"><a href="#FTTransformer-819"><span class="linenos"> 819</span></a>
</span><span id="FTTransformer-820"><a href="#FTTransformer-820"><span class="linenos"> 820</span></a><span class="sd">    We should admit that &quot;Feature Tokenizer&quot; is a bad and misleading name,</span>
</span><span id="FTTransformer-821"><a href="#FTTransformer-821"><span class="linenos"> 821</span></a><span class="sd">    which misuses the term &quot;token&quot;. A better name would be &quot;Feature Embeddings&quot;.</span>
</span><span id="FTTransformer-822"><a href="#FTTransformer-822"><span class="linenos"> 822</span></a>
</span><span id="FTTransformer-823"><a href="#FTTransformer-823"><span class="linenos"> 823</span></a><span class="sd">    &lt;img src=&quot;ft-transformer-details.png&quot; width=100%&gt;</span>
</span><span id="FTTransformer-824"><a href="#FTTransformer-824"><span class="linenos"> 824</span></a>
</span><span id="FTTransformer-825"><a href="#FTTransformer-825"><span class="linenos"> 825</span></a><span class="sd">    The default hyperparameters can be obtained with `FTTransformer.get_default_kwargs`.</span>
</span><span id="FTTransformer-826"><a href="#FTTransformer-826"><span class="linenos"> 826</span></a>
</span><span id="FTTransformer-827"><a href="#FTTransformer-827"><span class="linenos"> 827</span></a><span class="sd">    **Shape**</span>
</span><span id="FTTransformer-828"><a href="#FTTransformer-828"><span class="linenos"> 828</span></a>
</span><span id="FTTransformer-829"><a href="#FTTransformer-829"><span class="linenos"> 829</span></a><span class="sd">    - Input:</span>
</span><span id="FTTransformer-830"><a href="#FTTransformer-830"><span class="linenos"> 830</span></a><span class="sd">        - continuous features: `x_cont ~ (batch_size, n_cont_features)`</span>
</span><span id="FTTransformer-831"><a href="#FTTransformer-831"><span class="linenos"> 831</span></a><span class="sd">        - categorical features: `x_cat ~ (batch_size, len(cat_cardinalities))`</span>
</span><span id="FTTransformer-832"><a href="#FTTransformer-832"><span class="linenos"> 832</span></a><span class="sd">    - Output: `(batch_size, d_out or d_block)`</span>
</span><span id="FTTransformer-833"><a href="#FTTransformer-833"><span class="linenos"> 833</span></a>
</span><span id="FTTransformer-834"><a href="#FTTransformer-834"><span class="linenos"> 834</span></a><span class="sd">    **Examples**</span>
</span><span id="FTTransformer-835"><a href="#FTTransformer-835"><span class="linenos"> 835</span></a>
</span><span id="FTTransformer-836"><a href="#FTTransformer-836"><span class="linenos"> 836</span></a><span class="sd">    &gt;&gt;&gt; batch_size = 2</span>
</span><span id="FTTransformer-837"><a href="#FTTransformer-837"><span class="linenos"> 837</span></a><span class="sd">    &gt;&gt;&gt; n_cont_feaatures = 3</span>
</span><span id="FTTransformer-838"><a href="#FTTransformer-838"><span class="linenos"> 838</span></a><span class="sd">    &gt;&gt;&gt; cardinalities = [3, 4]</span>
</span><span id="FTTransformer-839"><a href="#FTTransformer-839"><span class="linenos"> 839</span></a><span class="sd">    &gt;&gt;&gt; x_cont = torch.randn(batch_size, n_cont_feaatures)</span>
</span><span id="FTTransformer-840"><a href="#FTTransformer-840"><span class="linenos"> 840</span></a><span class="sd">    &gt;&gt;&gt; x_cat = torch.column_stack([</span>
</span><span id="FTTransformer-841"><a href="#FTTransformer-841"><span class="linenos"> 841</span></a><span class="sd">    ...     torch.randint(0, c, (batch_size,))</span>
</span><span id="FTTransformer-842"><a href="#FTTransformer-842"><span class="linenos"> 842</span></a><span class="sd">    ...     for c in cardinalities</span>
</span><span id="FTTransformer-843"><a href="#FTTransformer-843"><span class="linenos"> 843</span></a><span class="sd">    ... ])</span>
</span><span id="FTTransformer-844"><a href="#FTTransformer-844"><span class="linenos"> 844</span></a><span class="sd">    &gt;&gt;&gt; d_out = 1</span>
</span><span id="FTTransformer-845"><a href="#FTTransformer-845"><span class="linenos"> 845</span></a><span class="sd">    &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer-846"><a href="#FTTransformer-846"><span class="linenos"> 846</span></a><span class="sd">    ...     n_cont_features=n_cont_feaatures,</span>
</span><span id="FTTransformer-847"><a href="#FTTransformer-847"><span class="linenos"> 847</span></a><span class="sd">    ...     cat_cardinalities=cardinalities,</span>
</span><span id="FTTransformer-848"><a href="#FTTransformer-848"><span class="linenos"> 848</span></a><span class="sd">    ...     d_out=d_out,</span>
</span><span id="FTTransformer-849"><a href="#FTTransformer-849"><span class="linenos"> 849</span></a><span class="sd">    ...     n_blocks=2,</span>
</span><span id="FTTransformer-850"><a href="#FTTransformer-850"><span class="linenos"> 850</span></a><span class="sd">    ...     d_block=16,</span>
</span><span id="FTTransformer-851"><a href="#FTTransformer-851"><span class="linenos"> 851</span></a><span class="sd">    ...     attention_n_heads=8,</span>
</span><span id="FTTransformer-852"><a href="#FTTransformer-852"><span class="linenos"> 852</span></a><span class="sd">    ...     attention_dropout=0.2,</span>
</span><span id="FTTransformer-853"><a href="#FTTransformer-853"><span class="linenos"> 853</span></a><span class="sd">    ...     ffn_d_hidden=None,</span>
</span><span id="FTTransformer-854"><a href="#FTTransformer-854"><span class="linenos"> 854</span></a><span class="sd">    ...     ffn_d_hidden_multiplier=4 / 3,</span>
</span><span id="FTTransformer-855"><a href="#FTTransformer-855"><span class="linenos"> 855</span></a><span class="sd">    ...     ffn_dropout=0.1,</span>
</span><span id="FTTransformer-856"><a href="#FTTransformer-856"><span class="linenos"> 856</span></a><span class="sd">    ...     residual_dropout=0.0,</span>
</span><span id="FTTransformer-857"><a href="#FTTransformer-857"><span class="linenos"> 857</span></a><span class="sd">    ... )</span>
</span><span id="FTTransformer-858"><a href="#FTTransformer-858"><span class="linenos"> 858</span></a><span class="sd">    &gt;&gt;&gt; assert m(x_cont, x_cat).shape == (batch_size, d_out)</span>
</span><span id="FTTransformer-859"><a href="#FTTransformer-859"><span class="linenos"> 859</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="FTTransformer-860"><a href="#FTTransformer-860"><span class="linenos"> 860</span></a>
</span><span id="FTTransformer-861"><a href="#FTTransformer-861"><span class="linenos"> 861</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="FTTransformer-862"><a href="#FTTransformer-862"><span class="linenos"> 862</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FTTransformer-863"><a href="#FTTransformer-863"><span class="linenos"> 863</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="FTTransformer-864"><a href="#FTTransformer-864"><span class="linenos"> 864</span></a>        <span class="n">n_cont_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformer-865"><a href="#FTTransformer-865"><span class="linenos"> 865</span></a>        <span class="n">cat_cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="FTTransformer-866"><a href="#FTTransformer-866"><span class="linenos"> 866</span></a>        <span class="n">_is_default</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FTTransformer-867"><a href="#FTTransformer-867"><span class="linenos"> 867</span></a>        <span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">,</span>
</span><span id="FTTransformer-868"><a href="#FTTransformer-868"><span class="linenos"> 868</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformer-869"><a href="#FTTransformer-869"><span class="linenos"> 869</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FTTransformer-870"><a href="#FTTransformer-870"><span class="linenos"> 870</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformer-871"><a href="#FTTransformer-871"><span class="linenos"> 871</span></a><span class="sd">            n_cont_features: the number of continuous features</span>
</span><span id="FTTransformer-872"><a href="#FTTransformer-872"><span class="linenos"> 872</span></a><span class="sd">            cat_cardinalities: the cardinalities of categorical features (see</span>
</span><span id="FTTransformer-873"><a href="#FTTransformer-873"><span class="linenos"> 873</span></a><span class="sd">                `CategoricalFeatureEmbeddings` for details). Pass en empty list</span>
</span><span id="FTTransformer-874"><a href="#FTTransformer-874"><span class="linenos"> 874</span></a><span class="sd">                if there are no categorical features.</span>
</span><span id="FTTransformer-875"><a href="#FTTransformer-875"><span class="linenos"> 875</span></a><span class="sd">            _is_default: this is a technical argument, don&#39;t set it manually.</span>
</span><span id="FTTransformer-876"><a href="#FTTransformer-876"><span class="linenos"> 876</span></a><span class="sd">            transformer_kwargs: the keyword arguments for the `Transformer` backbone.</span>
</span><span id="FTTransformer-877"><a href="#FTTransformer-877"><span class="linenos"> 877</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer-878"><a href="#FTTransformer-878"><span class="linenos"> 878</span></a>        <span class="k">assert</span> <span class="n">n_cont_features</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</span><span id="FTTransformer-879"><a href="#FTTransformer-879"><span class="linenos"> 879</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cat_cardinalities</span><span class="p">)</span>
</span><span id="FTTransformer-880"><a href="#FTTransformer-880"><span class="linenos"> 880</span></a>        <span class="k">assert</span> <span class="n">n_cont_features</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">cat_cardinalities</span>
</span><span id="FTTransformer-881"><a href="#FTTransformer-881"><span class="linenos"> 881</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="FTTransformer-882"><a href="#FTTransformer-882"><span class="linenos"> 882</span></a>
</span><span id="FTTransformer-883"><a href="#FTTransformer-883"><span class="linenos"> 883</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">transformer_kwargs</span><span class="p">[</span><span class="s1">&#39;d_block&#39;</span><span class="p">]</span>
</span><span id="FTTransformer-884"><a href="#FTTransformer-884"><span class="linenos"> 884</span></a>        <span class="c1"># &gt;&gt;&gt; Feature embeddings (see Figure 2a in the paper).</span>
</span><span id="FTTransformer-885"><a href="#FTTransformer-885"><span class="linenos"> 885</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cont_embeddings</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformer-886"><a href="#FTTransformer-886"><span class="linenos"> 886</span></a>            <span class="n">LinearEmbeddings</span><span class="p">(</span><span class="n">n_cont_features</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span> <span class="k">if</span> <span class="n">n_cont_features</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FTTransformer-887"><a href="#FTTransformer-887"><span class="linenos"> 887</span></a>        <span class="p">)</span>
</span><span id="FTTransformer-888"><a href="#FTTransformer-888"><span class="linenos"> 888</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings for continuous features.&quot;&quot;&quot;</span>
</span><span id="FTTransformer-889"><a href="#FTTransformer-889"><span class="linenos"> 889</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cat_embeddings</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformer-890"><a href="#FTTransformer-890"><span class="linenos"> 890</span></a>            <span class="n">CategoricalFeatureEmbeddings</span><span class="p">(</span><span class="n">cat_cardinalities</span><span class="p">,</span> <span class="n">d_block</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="FTTransformer-891"><a href="#FTTransformer-891"><span class="linenos"> 891</span></a>            <span class="k">if</span> <span class="n">cat_cardinalities</span>
</span><span id="FTTransformer-892"><a href="#FTTransformer-892"><span class="linenos"> 892</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="FTTransformer-893"><a href="#FTTransformer-893"><span class="linenos"> 893</span></a>        <span class="p">)</span>
</span><span id="FTTransformer-894"><a href="#FTTransformer-894"><span class="linenos"> 894</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings for categorical features.&quot;&quot;&quot;</span>
</span><span id="FTTransformer-895"><a href="#FTTransformer-895"><span class="linenos"> 895</span></a>        <span class="c1"># &gt;&gt;&gt;</span>
</span><span id="FTTransformer-896"><a href="#FTTransformer-896"><span class="linenos"> 896</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">FTTransformerBackbone</span><span class="p">(</span><span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">)</span>
</span><span id="FTTransformer-897"><a href="#FTTransformer-897"><span class="linenos"> 897</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The transformer backbone.&quot;&quot;&quot;</span>
</span><span id="FTTransformer-898"><a href="#FTTransformer-898"><span class="linenos"> 898</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_is_default</span> <span class="o">=</span> <span class="n">_is_default</span>
</span><span id="FTTransformer-899"><a href="#FTTransformer-899"><span class="linenos"> 899</span></a>
</span><span id="FTTransformer-900"><a href="#FTTransformer-900"><span class="linenos"> 900</span></a>    <span class="nd">@classmethod</span>
</span><span id="FTTransformer-901"><a href="#FTTransformer-901"><span class="linenos"> 901</span></a>    <span class="k">def</span> <span class="nf">get_default_kwargs</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
</span><span id="FTTransformer-902"><a href="#FTTransformer-902"><span class="linenos"> 902</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the default hyperparameters.</span>
</span><span id="FTTransformer-903"><a href="#FTTransformer-903"><span class="linenos"> 903</span></a>
</span><span id="FTTransformer-904"><a href="#FTTransformer-904"><span class="linenos"> 904</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformer-905"><a href="#FTTransformer-905"><span class="linenos"> 905</span></a><span class="sd">            n_blocks: the number of blocks. The supported values are in `range(1, 7)`.</span>
</span><span id="FTTransformer-906"><a href="#FTTransformer-906"><span class="linenos"> 906</span></a><span class="sd">        Returns:</span>
</span><span id="FTTransformer-907"><a href="#FTTransformer-907"><span class="linenos"> 907</span></a><span class="sd">            the default keyword arguments for the constructor</span>
</span><span id="FTTransformer-908"><a href="#FTTransformer-908"><span class="linenos"> 908</span></a>
</span><span id="FTTransformer-909"><a href="#FTTransformer-909"><span class="linenos"> 909</span></a><span class="sd">        **Examples**</span>
</span><span id="FTTransformer-910"><a href="#FTTransformer-910"><span class="linenos"> 910</span></a>
</span><span id="FTTransformer-911"><a href="#FTTransformer-911"><span class="linenos"> 911</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer-912"><a href="#FTTransformer-912"><span class="linenos"> 912</span></a><span class="sd">        ...     n_cont_features=3,</span>
</span><span id="FTTransformer-913"><a href="#FTTransformer-913"><span class="linenos"> 913</span></a><span class="sd">        ...     cat_cardinalities=[4, 5],</span>
</span><span id="FTTransformer-914"><a href="#FTTransformer-914"><span class="linenos"> 914</span></a><span class="sd">        ...     d_out=1,</span>
</span><span id="FTTransformer-915"><a href="#FTTransformer-915"><span class="linenos"> 915</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs()</span>
</span><span id="FTTransformer-916"><a href="#FTTransformer-916"><span class="linenos"> 916</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer-917"><a href="#FTTransformer-917"><span class="linenos"> 917</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer-918"><a href="#FTTransformer-918"><span class="linenos"> 918</span></a>        <span class="k">assert</span> <span class="p">(</span>
</span><span id="FTTransformer-919"><a href="#FTTransformer-919"><span class="linenos"> 919</span></a>            <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">n_blocks</span> <span class="o">&lt;=</span> <span class="mi">6</span>
</span><span id="FTTransformer-920"><a href="#FTTransformer-920"><span class="linenos"> 920</span></a>        <span class="p">),</span> <span class="s1">&#39;We offer default configurations only for `n_blocks in range(1, 7)`&#39;</span>
</span><span id="FTTransformer-921"><a href="#FTTransformer-921"><span class="linenos"> 921</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="FTTransformer-922"><a href="#FTTransformer-922"><span class="linenos"> 922</span></a>            <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="n">n_blocks</span><span class="p">,</span>
</span><span id="FTTransformer-923"><a href="#FTTransformer-923"><span class="linenos"> 923</span></a>            <span class="s1">&#39;d_block&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">384</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="FTTransformer-924"><a href="#FTTransformer-924"><span class="linenos"> 924</span></a>            <span class="s1">&#39;attention_n_heads&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="FTTransformer-925"><a href="#FTTransformer-925"><span class="linenos"> 925</span></a>            <span class="s1">&#39;attention_dropout&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="FTTransformer-926"><a href="#FTTransformer-926"><span class="linenos"> 926</span></a>            <span class="c1"># Because of the ReGLU activation used by FT-Transformer,</span>
</span><span id="FTTransformer-927"><a href="#FTTransformer-927"><span class="linenos"> 927</span></a>            <span class="c1"># 4 / 3 results in roughly the same number of parameters as 2.0</span>
</span><span id="FTTransformer-928"><a href="#FTTransformer-928"><span class="linenos"> 928</span></a>            <span class="c1"># would with simple element-wise activations (e.g. ReLU).</span>
</span><span id="FTTransformer-929"><a href="#FTTransformer-929"><span class="linenos"> 929</span></a>            <span class="s1">&#39;ffn_d_hidden&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformer-930"><a href="#FTTransformer-930"><span class="linenos"> 930</span></a>            <span class="s1">&#39;ffn_d_hidden_multiplier&#39;</span><span class="p">:</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="FTTransformer-931"><a href="#FTTransformer-931"><span class="linenos"> 931</span></a>            <span class="s1">&#39;ffn_dropout&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="FTTransformer-932"><a href="#FTTransformer-932"><span class="linenos"> 932</span></a>            <span class="s1">&#39;residual_dropout&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="FTTransformer-933"><a href="#FTTransformer-933"><span class="linenos"> 933</span></a>            <span class="s1">&#39;_is_default&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FTTransformer-934"><a href="#FTTransformer-934"><span class="linenos"> 934</span></a>        <span class="p">}</span>
</span><span id="FTTransformer-935"><a href="#FTTransformer-935"><span class="linenos"> 935</span></a>
</span><span id="FTTransformer-936"><a href="#FTTransformer-936"><span class="linenos"> 936</span></a>    <span class="k">def</span> <span class="nf">make_parameter_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
</span><span id="FTTransformer-937"><a href="#FTTransformer-937"><span class="linenos"> 937</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Make parameter groups for optimizers.</span>
</span><span id="FTTransformer-938"><a href="#FTTransformer-938"><span class="linenos"> 938</span></a>
</span><span id="FTTransformer-939"><a href="#FTTransformer-939"><span class="linenos"> 939</span></a><span class="sd">        The difference with calling this method instead of</span>
</span><span id="FTTransformer-940"><a href="#FTTransformer-940"><span class="linenos"> 940</span></a><span class="sd">        `.parameters()` is that this method always sets `weight_decay=0.0`</span>
</span><span id="FTTransformer-941"><a href="#FTTransformer-941"><span class="linenos"> 941</span></a><span class="sd">        for some of the parameters.</span>
</span><span id="FTTransformer-942"><a href="#FTTransformer-942"><span class="linenos"> 942</span></a>
</span><span id="FTTransformer-943"><a href="#FTTransformer-943"><span class="linenos"> 943</span></a><span class="sd">        **Examples**</span>
</span><span id="FTTransformer-944"><a href="#FTTransformer-944"><span class="linenos"> 944</span></a>
</span><span id="FTTransformer-945"><a href="#FTTransformer-945"><span class="linenos"> 945</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer-946"><a href="#FTTransformer-946"><span class="linenos"> 946</span></a><span class="sd">        ...     n_cont_features=2,</span>
</span><span id="FTTransformer-947"><a href="#FTTransformer-947"><span class="linenos"> 947</span></a><span class="sd">        ...     cat_cardinalities=[3, 4],</span>
</span><span id="FTTransformer-948"><a href="#FTTransformer-948"><span class="linenos"> 948</span></a><span class="sd">        ...     d_out=5,</span>
</span><span id="FTTransformer-949"><a href="#FTTransformer-949"><span class="linenos"> 949</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs(),</span>
</span><span id="FTTransformer-950"><a href="#FTTransformer-950"><span class="linenos"> 950</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer-951"><a href="#FTTransformer-951"><span class="linenos"> 951</span></a><span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.AdamW(</span>
</span><span id="FTTransformer-952"><a href="#FTTransformer-952"><span class="linenos"> 952</span></a><span class="sd">        ...     m.make_parameter_groups(),</span>
</span><span id="FTTransformer-953"><a href="#FTTransformer-953"><span class="linenos"> 953</span></a><span class="sd">        ...     lr=1e-4,</span>
</span><span id="FTTransformer-954"><a href="#FTTransformer-954"><span class="linenos"> 954</span></a><span class="sd">        ...     weight_decay=1e-5,</span>
</span><span id="FTTransformer-955"><a href="#FTTransformer-955"><span class="linenos"> 955</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer-956"><a href="#FTTransformer-956"><span class="linenos"> 956</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer-957"><a href="#FTTransformer-957"><span class="linenos"> 957</span></a>        <span class="n">main_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[]}</span>
</span><span id="FTTransformer-958"><a href="#FTTransformer-958"><span class="linenos"> 958</span></a>        <span class="n">zero_wd_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
</span><span id="FTTransformer-959"><a href="#FTTransformer-959"><span class="linenos"> 959</span></a>
</span><span id="FTTransformer-960"><a href="#FTTransformer-960"><span class="linenos"> 960</span></a>        <span class="n">zero_wd_subnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="s1">&#39;.bias&#39;</span><span class="p">]</span>
</span><span id="FTTransformer-961"><a href="#FTTransformer-961"><span class="linenos"> 961</span></a>        <span class="k">for</span> <span class="n">modulename</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;cont_embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;cat_embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;cls_embedding&#39;</span><span class="p">]:</span>
</span><span id="FTTransformer-962"><a href="#FTTransformer-962"><span class="linenos"> 962</span></a>            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modulename</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformer-963"><a href="#FTTransformer-963"><span class="linenos"> 963</span></a>                <span class="n">zero_wd_subnames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modulename</span><span class="p">)</span>
</span><span id="FTTransformer-964"><a href="#FTTransformer-964"><span class="linenos"> 964</span></a>        <span class="c1"># Check that there are no typos in the above list.</span>
</span><span id="FTTransformer-965"><a href="#FTTransformer-965"><span class="linenos"> 965</span></a>        <span class="k">for</span> <span class="n">subname</span> <span class="ow">in</span> <span class="n">zero_wd_subnames</span><span class="p">:</span>
</span><span id="FTTransformer-966"><a href="#FTTransformer-966"><span class="linenos"> 966</span></a>            <span class="k">assert</span> <span class="nb">any</span><span class="p">(</span>
</span><span id="FTTransformer-967"><a href="#FTTransformer-967"><span class="linenos"> 967</span></a>                <span class="n">subname</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span id="FTTransformer-968"><a href="#FTTransformer-968"><span class="linenos"> 968</span></a>            <span class="p">),</span> <span class="n">_INTERNAL_ERROR_MESSAGE</span>
</span><span id="FTTransformer-969"><a href="#FTTransformer-969"><span class="linenos"> 969</span></a>
</span><span id="FTTransformer-970"><a href="#FTTransformer-970"><span class="linenos"> 970</span></a>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span><span id="FTTransformer-971"><a href="#FTTransformer-971"><span class="linenos"> 971</span></a>            <span class="n">zero_wd_condition</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">subname</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">subname</span> <span class="ow">in</span> <span class="n">zero_wd_subnames</span><span class="p">)</span>
</span><span id="FTTransformer-972"><a href="#FTTransformer-972"><span class="linenos"> 972</span></a>            <span class="p">(</span><span class="n">zero_wd_group</span> <span class="k">if</span> <span class="n">zero_wd_condition</span> <span class="k">else</span> <span class="n">main_group</span><span class="p">)[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="FTTransformer-973"><a href="#FTTransformer-973"><span class="linenos"> 973</span></a>                <span class="n">parameter</span>
</span><span id="FTTransformer-974"><a href="#FTTransformer-974"><span class="linenos"> 974</span></a>            <span class="p">)</span>
</span><span id="FTTransformer-975"><a href="#FTTransformer-975"><span class="linenos"> 975</span></a>        <span class="k">return</span> <span class="p">[</span><span class="n">main_group</span><span class="p">,</span> <span class="n">zero_wd_group</span><span class="p">]</span>
</span><span id="FTTransformer-976"><a href="#FTTransformer-976"><span class="linenos"> 976</span></a>
</span><span id="FTTransformer-977"><a href="#FTTransformer-977"><span class="linenos"> 977</span></a>    <span class="k">def</span> <span class="nf">make_default_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">:</span>
</span><span id="FTTransformer-978"><a href="#FTTransformer-978"><span class="linenos"> 978</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the &quot;default&quot; `torch.nn.AdamW` suitable for the *default* FT-Transformer.</span>
</span><span id="FTTransformer-979"><a href="#FTTransformer-979"><span class="linenos"> 979</span></a>
</span><span id="FTTransformer-980"><a href="#FTTransformer-980"><span class="linenos"> 980</span></a><span class="sd">        Returns:</span>
</span><span id="FTTransformer-981"><a href="#FTTransformer-981"><span class="linenos"> 981</span></a><span class="sd">            optimizer</span>
</span><span id="FTTransformer-982"><a href="#FTTransformer-982"><span class="linenos"> 982</span></a>
</span><span id="FTTransformer-983"><a href="#FTTransformer-983"><span class="linenos"> 983</span></a><span class="sd">        **Examples**</span>
</span><span id="FTTransformer-984"><a href="#FTTransformer-984"><span class="linenos"> 984</span></a>
</span><span id="FTTransformer-985"><a href="#FTTransformer-985"><span class="linenos"> 985</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer-986"><a href="#FTTransformer-986"><span class="linenos"> 986</span></a><span class="sd">        ...     n_cont_features=2,</span>
</span><span id="FTTransformer-987"><a href="#FTTransformer-987"><span class="linenos"> 987</span></a><span class="sd">        ...     cat_cardinalities=[3, 4],</span>
</span><span id="FTTransformer-988"><a href="#FTTransformer-988"><span class="linenos"> 988</span></a><span class="sd">        ...     d_out=5,</span>
</span><span id="FTTransformer-989"><a href="#FTTransformer-989"><span class="linenos"> 989</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs(),</span>
</span><span id="FTTransformer-990"><a href="#FTTransformer-990"><span class="linenos"> 990</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer-991"><a href="#FTTransformer-991"><span class="linenos"> 991</span></a><span class="sd">        &gt;&gt;&gt; optimizer = m.make_default_optimizer()</span>
</span><span id="FTTransformer-992"><a href="#FTTransformer-992"><span class="linenos"> 992</span></a><span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
</span><span id="FTTransformer-993"><a href="#FTTransformer-993"><span class="linenos"> 993</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default</span><span class="p">:</span>
</span><span id="FTTransformer-994"><a href="#FTTransformer-994"><span class="linenos"> 994</span></a>            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
</span><span id="FTTransformer-995"><a href="#FTTransformer-995"><span class="linenos"> 995</span></a>                <span class="s1">&#39;The default opimizer is supposed to be used in a combination&#39;</span>
</span><span id="FTTransformer-996"><a href="#FTTransformer-996"><span class="linenos"> 996</span></a>                <span class="s1">&#39; with the default FT-Transformer.&#39;</span>
</span><span id="FTTransformer-997"><a href="#FTTransformer-997"><span class="linenos"> 997</span></a>            <span class="p">)</span>
</span><span id="FTTransformer-998"><a href="#FTTransformer-998"><span class="linenos"> 998</span></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
</span><span id="FTTransformer-999"><a href="#FTTransformer-999"><span class="linenos"> 999</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">make_parameter_groups</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span>
</span><span id="FTTransformer-1000"><a href="#FTTransformer-1000"><span class="linenos">1000</span></a>        <span class="p">)</span>
</span><span id="FTTransformer-1001"><a href="#FTTransformer-1001"><span class="linenos">1001</span></a>
</span><span id="FTTransformer-1002"><a href="#FTTransformer-1002"><span class="linenos">1002</span></a>    <span class="n">_FORWARD_BAD_ARGS_MESSAGE</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformer-1003"><a href="#FTTransformer-1003"><span class="linenos">1003</span></a>        <span class="s1">&#39;Based on the arguments passed to the constructor of FT-Transformer, </span><span class="si">{}</span><span class="s1">&#39;</span>
</span><span id="FTTransformer-1004"><a href="#FTTransformer-1004"><span class="linenos">1004</span></a>    <span class="p">)</span>
</span><span id="FTTransformer-1005"><a href="#FTTransformer-1005"><span class="linenos">1005</span></a>
</span><span id="FTTransformer-1006"><a href="#FTTransformer-1006"><span class="linenos">1006</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_cont</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">x_cat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FTTransformer-1007"><a href="#FTTransformer-1007"><span class="linenos">1007</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.</span>
</span><span id="FTTransformer-1008"><a href="#FTTransformer-1008"><span class="linenos">1008</span></a>
</span><span id="FTTransformer-1009"><a href="#FTTransformer-1009"><span class="linenos">1009</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformer-1010"><a href="#FTTransformer-1010"><span class="linenos">1010</span></a><span class="sd">            x_cont: the continuous features.</span>
</span><span id="FTTransformer-1011"><a href="#FTTransformer-1011"><span class="linenos">1011</span></a><span class="sd">            x_cat: the categorical features.</span>
</span><span id="FTTransformer-1012"><a href="#FTTransformer-1012"><span class="linenos">1012</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer-1013"><a href="#FTTransformer-1013"><span class="linenos">1013</span></a>        <span class="k">assert</span> <span class="n">x_cont</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">x_cat</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="FTTransformer-1014"><a href="#FTTransformer-1014"><span class="linenos">1014</span></a>
</span><span id="FTTransformer-1015"><a href="#FTTransformer-1015"><span class="linenos">1015</span></a>        <span class="n">x_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="FTTransformer-1016"><a href="#FTTransformer-1016"><span class="linenos">1016</span></a>        <span class="k">for</span> <span class="n">argname</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FTTransformer-1017"><a href="#FTTransformer-1017"><span class="linenos">1017</span></a>            <span class="p">(</span><span class="s1">&#39;x_cont&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_embeddings</span><span class="p">),</span>
</span><span id="FTTransformer-1018"><a href="#FTTransformer-1018"><span class="linenos">1018</span></a>            <span class="p">(</span><span class="s1">&#39;x_cat&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embeddings</span><span class="p">),</span>
</span><span id="FTTransformer-1019"><a href="#FTTransformer-1019"><span class="linenos">1019</span></a>        <span class="p">]:</span>
</span><span id="FTTransformer-1020"><a href="#FTTransformer-1020"><span class="linenos">1020</span></a>            <span class="n">argvalue</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">argname</span><span class="p">]</span>
</span><span id="FTTransformer-1021"><a href="#FTTransformer-1021"><span class="linenos">1021</span></a>            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformer-1022"><a href="#FTTransformer-1022"><span class="linenos">1022</span></a>                <span class="k">assert</span> <span class="n">argvalue</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">FTTransformer</span><span class="o">.</span><span class="n">_FORWARD_BAD_ARGS_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="FTTransformer-1023"><a href="#FTTransformer-1023"><span class="linenos">1023</span></a>                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">argname</span><span class="si">}</span><span class="s1"> must be None&#39;</span>
</span><span id="FTTransformer-1024"><a href="#FTTransformer-1024"><span class="linenos">1024</span></a>                <span class="p">)</span>
</span><span id="FTTransformer-1025"><a href="#FTTransformer-1025"><span class="linenos">1025</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FTTransformer-1026"><a href="#FTTransformer-1026"><span class="linenos">1026</span></a>                <span class="k">assert</span> <span class="p">(</span>
</span><span id="FTTransformer-1027"><a href="#FTTransformer-1027"><span class="linenos">1027</span></a>                    <span class="n">argvalue</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="FTTransformer-1028"><a href="#FTTransformer-1028"><span class="linenos">1028</span></a>                <span class="p">),</span> <span class="n">FTTransformer</span><span class="o">.</span><span class="n">_FORWARD_BAD_ARGS_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="FTTransformer-1029"><a href="#FTTransformer-1029"><span class="linenos">1029</span></a>                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">argname</span><span class="si">}</span><span class="s1"> must not be None&#39;</span>
</span><span id="FTTransformer-1030"><a href="#FTTransformer-1030"><span class="linenos">1030</span></a>                <span class="p">)</span>
</span><span id="FTTransformer-1031"><a href="#FTTransformer-1031"><span class="linenos">1031</span></a>                <span class="n">x_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="p">(</span><span class="n">argvalue</span><span class="p">))</span>
</span><span id="FTTransformer-1032"><a href="#FTTransformer-1032"><span class="linenos">1032</span></a>        <span class="k">assert</span> <span class="n">x_embeddings</span>
</span><span id="FTTransformer-1033"><a href="#FTTransformer-1033"><span class="linenos">1033</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FTTransformer-1034"><a href="#FTTransformer-1034"><span class="linenos">1034</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformer-1035"><a href="#FTTransformer-1035"><span class="linenos">1035</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>The FT-Transformer model from Section 3.3 in the paper.</p>

<p><img src="ft-transformer-overview.png" width=100%&gt;</p>

<p>We should admit that "Feature Tokenizer" is a bad and misleading name,
which misuses the term "token". A better name would be "Feature Embeddings".</p>

<p><img src="ft-transformer-details.png" width=100%&gt;</p>

<p>The default hyperparameters can be obtained with <code><a href="#FTTransformer.get_default_kwargs">FTTransformer.get_default_kwargs</a></code>.</p>

<p><strong>Shape</strong></p>

<ul>
<li>Input:
<ul>
<li>continuous features: <code>x_cont ~ (batch_size, n_cont_features)</code></li>
<li>categorical features: <code>x_cat ~ (batch_size, len(cat_cardinalities))</code></li>
</ul></li>
<li>Output: <code>(batch_size, d_out or d_block)</code></li>
</ul>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_cont_feaatures</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cardinalities</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_cont</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_cont_feaatures</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_cat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">cardinalities</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_out</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_cont_features</span><span class="o">=</span><span class="n">n_cont_feaatures</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">cat_cardinalities</span><span class="o">=</span><span class="n">cardinalities</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_out</span><span class="o">=</span><span class="n">d_out</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_block</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">attention_n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ffn_d_hidden</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ffn_d_hidden_multiplier</span><span class="o">=</span><span class="mi">4</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ffn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">residual_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">x_cont</span><span class="p">,</span> <span class="n">x_cat</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
</code></pre>
</div>
</div>


                            <div id="FTTransformer.__init__" class="classattr">
                                        <input id="FTTransformer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">FTTransformer</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="o">*</span>,</span><span class="param">	<span class="n">n_cont_features</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">cat_cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>,</span><span class="param">	<span class="n">_is_default</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="o">**</span><span class="n">transformer_kwargs</span></span>)</span>

                <label class="view-source-button" for="FTTransformer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformer.__init__-861"><a href="#FTTransformer.__init__-861"><span class="linenos">861</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="FTTransformer.__init__-862"><a href="#FTTransformer.__init__-862"><span class="linenos">862</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="FTTransformer.__init__-863"><a href="#FTTransformer.__init__-863"><span class="linenos">863</span></a>        <span class="o">*</span><span class="p">,</span>
</span><span id="FTTransformer.__init__-864"><a href="#FTTransformer.__init__-864"><span class="linenos">864</span></a>        <span class="n">n_cont_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FTTransformer.__init__-865"><a href="#FTTransformer.__init__-865"><span class="linenos">865</span></a>        <span class="n">cat_cardinalities</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span><span id="FTTransformer.__init__-866"><a href="#FTTransformer.__init__-866"><span class="linenos">866</span></a>        <span class="n">_is_default</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FTTransformer.__init__-867"><a href="#FTTransformer.__init__-867"><span class="linenos">867</span></a>        <span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">,</span>
</span><span id="FTTransformer.__init__-868"><a href="#FTTransformer.__init__-868"><span class="linenos">868</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformer.__init__-869"><a href="#FTTransformer.__init__-869"><span class="linenos">869</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FTTransformer.__init__-870"><a href="#FTTransformer.__init__-870"><span class="linenos">870</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformer.__init__-871"><a href="#FTTransformer.__init__-871"><span class="linenos">871</span></a><span class="sd">            n_cont_features: the number of continuous features</span>
</span><span id="FTTransformer.__init__-872"><a href="#FTTransformer.__init__-872"><span class="linenos">872</span></a><span class="sd">            cat_cardinalities: the cardinalities of categorical features (see</span>
</span><span id="FTTransformer.__init__-873"><a href="#FTTransformer.__init__-873"><span class="linenos">873</span></a><span class="sd">                `CategoricalFeatureEmbeddings` for details). Pass en empty list</span>
</span><span id="FTTransformer.__init__-874"><a href="#FTTransformer.__init__-874"><span class="linenos">874</span></a><span class="sd">                if there are no categorical features.</span>
</span><span id="FTTransformer.__init__-875"><a href="#FTTransformer.__init__-875"><span class="linenos">875</span></a><span class="sd">            _is_default: this is a technical argument, don&#39;t set it manually.</span>
</span><span id="FTTransformer.__init__-876"><a href="#FTTransformer.__init__-876"><span class="linenos">876</span></a><span class="sd">            transformer_kwargs: the keyword arguments for the `Transformer` backbone.</span>
</span><span id="FTTransformer.__init__-877"><a href="#FTTransformer.__init__-877"><span class="linenos">877</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer.__init__-878"><a href="#FTTransformer.__init__-878"><span class="linenos">878</span></a>        <span class="k">assert</span> <span class="n">n_cont_features</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</span><span id="FTTransformer.__init__-879"><a href="#FTTransformer.__init__-879"><span class="linenos">879</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cat_cardinalities</span><span class="p">)</span>
</span><span id="FTTransformer.__init__-880"><a href="#FTTransformer.__init__-880"><span class="linenos">880</span></a>        <span class="k">assert</span> <span class="n">n_cont_features</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">cat_cardinalities</span>
</span><span id="FTTransformer.__init__-881"><a href="#FTTransformer.__init__-881"><span class="linenos">881</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="FTTransformer.__init__-882"><a href="#FTTransformer.__init__-882"><span class="linenos">882</span></a>
</span><span id="FTTransformer.__init__-883"><a href="#FTTransformer.__init__-883"><span class="linenos">883</span></a>        <span class="n">d_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">transformer_kwargs</span><span class="p">[</span><span class="s1">&#39;d_block&#39;</span><span class="p">]</span>
</span><span id="FTTransformer.__init__-884"><a href="#FTTransformer.__init__-884"><span class="linenos">884</span></a>        <span class="c1"># &gt;&gt;&gt; Feature embeddings (see Figure 2a in the paper).</span>
</span><span id="FTTransformer.__init__-885"><a href="#FTTransformer.__init__-885"><span class="linenos">885</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cont_embeddings</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformer.__init__-886"><a href="#FTTransformer.__init__-886"><span class="linenos">886</span></a>            <span class="n">LinearEmbeddings</span><span class="p">(</span><span class="n">n_cont_features</span><span class="p">,</span> <span class="n">d_block</span><span class="p">)</span> <span class="k">if</span> <span class="n">n_cont_features</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FTTransformer.__init__-887"><a href="#FTTransformer.__init__-887"><span class="linenos">887</span></a>        <span class="p">)</span>
</span><span id="FTTransformer.__init__-888"><a href="#FTTransformer.__init__-888"><span class="linenos">888</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings for continuous features.&quot;&quot;&quot;</span>
</span><span id="FTTransformer.__init__-889"><a href="#FTTransformer.__init__-889"><span class="linenos">889</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cat_embeddings</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="FTTransformer.__init__-890"><a href="#FTTransformer.__init__-890"><span class="linenos">890</span></a>            <span class="n">CategoricalFeatureEmbeddings</span><span class="p">(</span><span class="n">cat_cardinalities</span><span class="p">,</span> <span class="n">d_block</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="FTTransformer.__init__-891"><a href="#FTTransformer.__init__-891"><span class="linenos">891</span></a>            <span class="k">if</span> <span class="n">cat_cardinalities</span>
</span><span id="FTTransformer.__init__-892"><a href="#FTTransformer.__init__-892"><span class="linenos">892</span></a>            <span class="k">else</span> <span class="kc">None</span>
</span><span id="FTTransformer.__init__-893"><a href="#FTTransformer.__init__-893"><span class="linenos">893</span></a>        <span class="p">)</span>
</span><span id="FTTransformer.__init__-894"><a href="#FTTransformer.__init__-894"><span class="linenos">894</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The embeddings for categorical features.&quot;&quot;&quot;</span>
</span><span id="FTTransformer.__init__-895"><a href="#FTTransformer.__init__-895"><span class="linenos">895</span></a>        <span class="c1"># &gt;&gt;&gt;</span>
</span><span id="FTTransformer.__init__-896"><a href="#FTTransformer.__init__-896"><span class="linenos">896</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">FTTransformerBackbone</span><span class="p">(</span><span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">)</span>
</span><span id="FTTransformer.__init__-897"><a href="#FTTransformer.__init__-897"><span class="linenos">897</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The transformer backbone.&quot;&quot;&quot;</span>
</span><span id="FTTransformer.__init__-898"><a href="#FTTransformer.__init__-898"><span class="linenos">898</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_is_default</span> <span class="o">=</span> <span class="n">_is_default</span>
</span></pre></div>


            <div class="docstring"><h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>n_cont_features:</strong>  the number of continuous features</li>
<li><strong>cat_cardinalities:</strong>  the cardinalities of categorical features (see
<code><a href="#CategoricalFeatureEmbeddings">CategoricalFeatureEmbeddings</a></code> for details). Pass en empty list
if there are no categorical features.</li>
<li><strong>_is_default:</strong>  this is a technical argument, don't set it manually.</li>
<li><strong>transformer_kwargs:</strong>  the keyword arguments for the <code>Transformer</code> backbone.</li>
</ul>
</div>


                            </div>
                            <div id="FTTransformer.cont_embeddings" class="classattr">
                                <div class="attr variable">
            <span class="name">cont_embeddings</span>

        
    </div>
    <a class="headerlink" href="#FTTransformer.cont_embeddings"></a>
    
            <div class="docstring"><p>The embeddings for continuous features.</p>
</div>


                            </div>
                            <div id="FTTransformer.cat_embeddings" class="classattr">
                                <div class="attr variable">
            <span class="name">cat_embeddings</span>

        
    </div>
    <a class="headerlink" href="#FTTransformer.cat_embeddings"></a>
    
            <div class="docstring"><p>The embeddings for categorical features.</p>
</div>


                            </div>
                            <div id="FTTransformer.backbone" class="classattr">
                                <div class="attr variable">
            <span class="name">backbone</span>

        
    </div>
    <a class="headerlink" href="#FTTransformer.backbone"></a>
    
            <div class="docstring"><p>The transformer backbone.</p>
</div>


                            </div>
                            <div id="FTTransformer.get_default_kwargs" class="classattr">
                                        <input id="FTTransformer.get_default_kwargs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator">@classmethod</div>

        <span class="def">def</span>
        <span class="name">get_default_kwargs</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">cls</span>, </span><span class="param"><span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span></span><span class="return-annotation">) -> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="FTTransformer.get_default_kwargs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformer.get_default_kwargs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformer.get_default_kwargs-900"><a href="#FTTransformer.get_default_kwargs-900"><span class="linenos">900</span></a>    <span class="nd">@classmethod</span>
</span><span id="FTTransformer.get_default_kwargs-901"><a href="#FTTransformer.get_default_kwargs-901"><span class="linenos">901</span></a>    <span class="k">def</span> <span class="nf">get_default_kwargs</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
</span><span id="FTTransformer.get_default_kwargs-902"><a href="#FTTransformer.get_default_kwargs-902"><span class="linenos">902</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the default hyperparameters.</span>
</span><span id="FTTransformer.get_default_kwargs-903"><a href="#FTTransformer.get_default_kwargs-903"><span class="linenos">903</span></a>
</span><span id="FTTransformer.get_default_kwargs-904"><a href="#FTTransformer.get_default_kwargs-904"><span class="linenos">904</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformer.get_default_kwargs-905"><a href="#FTTransformer.get_default_kwargs-905"><span class="linenos">905</span></a><span class="sd">            n_blocks: the number of blocks. The supported values are in `range(1, 7)`.</span>
</span><span id="FTTransformer.get_default_kwargs-906"><a href="#FTTransformer.get_default_kwargs-906"><span class="linenos">906</span></a><span class="sd">        Returns:</span>
</span><span id="FTTransformer.get_default_kwargs-907"><a href="#FTTransformer.get_default_kwargs-907"><span class="linenos">907</span></a><span class="sd">            the default keyword arguments for the constructor</span>
</span><span id="FTTransformer.get_default_kwargs-908"><a href="#FTTransformer.get_default_kwargs-908"><span class="linenos">908</span></a>
</span><span id="FTTransformer.get_default_kwargs-909"><a href="#FTTransformer.get_default_kwargs-909"><span class="linenos">909</span></a><span class="sd">        **Examples**</span>
</span><span id="FTTransformer.get_default_kwargs-910"><a href="#FTTransformer.get_default_kwargs-910"><span class="linenos">910</span></a>
</span><span id="FTTransformer.get_default_kwargs-911"><a href="#FTTransformer.get_default_kwargs-911"><span class="linenos">911</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer.get_default_kwargs-912"><a href="#FTTransformer.get_default_kwargs-912"><span class="linenos">912</span></a><span class="sd">        ...     n_cont_features=3,</span>
</span><span id="FTTransformer.get_default_kwargs-913"><a href="#FTTransformer.get_default_kwargs-913"><span class="linenos">913</span></a><span class="sd">        ...     cat_cardinalities=[4, 5],</span>
</span><span id="FTTransformer.get_default_kwargs-914"><a href="#FTTransformer.get_default_kwargs-914"><span class="linenos">914</span></a><span class="sd">        ...     d_out=1,</span>
</span><span id="FTTransformer.get_default_kwargs-915"><a href="#FTTransformer.get_default_kwargs-915"><span class="linenos">915</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs()</span>
</span><span id="FTTransformer.get_default_kwargs-916"><a href="#FTTransformer.get_default_kwargs-916"><span class="linenos">916</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer.get_default_kwargs-917"><a href="#FTTransformer.get_default_kwargs-917"><span class="linenos">917</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer.get_default_kwargs-918"><a href="#FTTransformer.get_default_kwargs-918"><span class="linenos">918</span></a>        <span class="k">assert</span> <span class="p">(</span>
</span><span id="FTTransformer.get_default_kwargs-919"><a href="#FTTransformer.get_default_kwargs-919"><span class="linenos">919</span></a>            <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">n_blocks</span> <span class="o">&lt;=</span> <span class="mi">6</span>
</span><span id="FTTransformer.get_default_kwargs-920"><a href="#FTTransformer.get_default_kwargs-920"><span class="linenos">920</span></a>        <span class="p">),</span> <span class="s1">&#39;We offer default configurations only for `n_blocks in range(1, 7)`&#39;</span>
</span><span id="FTTransformer.get_default_kwargs-921"><a href="#FTTransformer.get_default_kwargs-921"><span class="linenos">921</span></a>        <span class="k">return</span> <span class="p">{</span>
</span><span id="FTTransformer.get_default_kwargs-922"><a href="#FTTransformer.get_default_kwargs-922"><span class="linenos">922</span></a>            <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="n">n_blocks</span><span class="p">,</span>
</span><span id="FTTransformer.get_default_kwargs-923"><a href="#FTTransformer.get_default_kwargs-923"><span class="linenos">923</span></a>            <span class="s1">&#39;d_block&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">384</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="FTTransformer.get_default_kwargs-924"><a href="#FTTransformer.get_default_kwargs-924"><span class="linenos">924</span></a>            <span class="s1">&#39;attention_n_heads&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="FTTransformer.get_default_kwargs-925"><a href="#FTTransformer.get_default_kwargs-925"><span class="linenos">925</span></a>            <span class="s1">&#39;attention_dropout&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="FTTransformer.get_default_kwargs-926"><a href="#FTTransformer.get_default_kwargs-926"><span class="linenos">926</span></a>            <span class="c1"># Because of the ReGLU activation used by FT-Transformer,</span>
</span><span id="FTTransformer.get_default_kwargs-927"><a href="#FTTransformer.get_default_kwargs-927"><span class="linenos">927</span></a>            <span class="c1"># 4 / 3 results in roughly the same number of parameters as 2.0</span>
</span><span id="FTTransformer.get_default_kwargs-928"><a href="#FTTransformer.get_default_kwargs-928"><span class="linenos">928</span></a>            <span class="c1"># would with simple element-wise activations (e.g. ReLU).</span>
</span><span id="FTTransformer.get_default_kwargs-929"><a href="#FTTransformer.get_default_kwargs-929"><span class="linenos">929</span></a>            <span class="s1">&#39;ffn_d_hidden&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FTTransformer.get_default_kwargs-930"><a href="#FTTransformer.get_default_kwargs-930"><span class="linenos">930</span></a>            <span class="s1">&#39;ffn_d_hidden_multiplier&#39;</span><span class="p">:</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="FTTransformer.get_default_kwargs-931"><a href="#FTTransformer.get_default_kwargs-931"><span class="linenos">931</span></a>            <span class="s1">&#39;ffn_dropout&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">][</span><span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
</span><span id="FTTransformer.get_default_kwargs-932"><a href="#FTTransformer.get_default_kwargs-932"><span class="linenos">932</span></a>            <span class="s1">&#39;residual_dropout&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="FTTransformer.get_default_kwargs-933"><a href="#FTTransformer.get_default_kwargs-933"><span class="linenos">933</span></a>            <span class="s1">&#39;_is_default&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FTTransformer.get_default_kwargs-934"><a href="#FTTransformer.get_default_kwargs-934"><span class="linenos">934</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Get the default hyperparameters.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>n_blocks:</strong>  the number of blocks. The supported values are in <code>range(1, 7)</code>.</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>the default keyword arguments for the constructor</p>
</blockquote>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_cont_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">cat_cardinalities</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">d_out</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="o">**</span><span class="n"><a href="#FTTransformer.get_default_kwargs">FTTransformer.get_default_kwargs</a></span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span>
</code></pre>
</div>
</div>


                            </div>
                            <div id="FTTransformer.make_parameter_groups" class="classattr">
                                        <input id="FTTransformer.make_parameter_groups-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">make_parameter_groups</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span>:</span></span>

                <label class="view-source-button" for="FTTransformer.make_parameter_groups-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformer.make_parameter_groups"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformer.make_parameter_groups-936"><a href="#FTTransformer.make_parameter_groups-936"><span class="linenos">936</span></a>    <span class="k">def</span> <span class="nf">make_parameter_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
</span><span id="FTTransformer.make_parameter_groups-937"><a href="#FTTransformer.make_parameter_groups-937"><span class="linenos">937</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Make parameter groups for optimizers.</span>
</span><span id="FTTransformer.make_parameter_groups-938"><a href="#FTTransformer.make_parameter_groups-938"><span class="linenos">938</span></a>
</span><span id="FTTransformer.make_parameter_groups-939"><a href="#FTTransformer.make_parameter_groups-939"><span class="linenos">939</span></a><span class="sd">        The difference with calling this method instead of</span>
</span><span id="FTTransformer.make_parameter_groups-940"><a href="#FTTransformer.make_parameter_groups-940"><span class="linenos">940</span></a><span class="sd">        `.parameters()` is that this method always sets `weight_decay=0.0`</span>
</span><span id="FTTransformer.make_parameter_groups-941"><a href="#FTTransformer.make_parameter_groups-941"><span class="linenos">941</span></a><span class="sd">        for some of the parameters.</span>
</span><span id="FTTransformer.make_parameter_groups-942"><a href="#FTTransformer.make_parameter_groups-942"><span class="linenos">942</span></a>
</span><span id="FTTransformer.make_parameter_groups-943"><a href="#FTTransformer.make_parameter_groups-943"><span class="linenos">943</span></a><span class="sd">        **Examples**</span>
</span><span id="FTTransformer.make_parameter_groups-944"><a href="#FTTransformer.make_parameter_groups-944"><span class="linenos">944</span></a>
</span><span id="FTTransformer.make_parameter_groups-945"><a href="#FTTransformer.make_parameter_groups-945"><span class="linenos">945</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer.make_parameter_groups-946"><a href="#FTTransformer.make_parameter_groups-946"><span class="linenos">946</span></a><span class="sd">        ...     n_cont_features=2,</span>
</span><span id="FTTransformer.make_parameter_groups-947"><a href="#FTTransformer.make_parameter_groups-947"><span class="linenos">947</span></a><span class="sd">        ...     cat_cardinalities=[3, 4],</span>
</span><span id="FTTransformer.make_parameter_groups-948"><a href="#FTTransformer.make_parameter_groups-948"><span class="linenos">948</span></a><span class="sd">        ...     d_out=5,</span>
</span><span id="FTTransformer.make_parameter_groups-949"><a href="#FTTransformer.make_parameter_groups-949"><span class="linenos">949</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs(),</span>
</span><span id="FTTransformer.make_parameter_groups-950"><a href="#FTTransformer.make_parameter_groups-950"><span class="linenos">950</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer.make_parameter_groups-951"><a href="#FTTransformer.make_parameter_groups-951"><span class="linenos">951</span></a><span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.AdamW(</span>
</span><span id="FTTransformer.make_parameter_groups-952"><a href="#FTTransformer.make_parameter_groups-952"><span class="linenos">952</span></a><span class="sd">        ...     m.make_parameter_groups(),</span>
</span><span id="FTTransformer.make_parameter_groups-953"><a href="#FTTransformer.make_parameter_groups-953"><span class="linenos">953</span></a><span class="sd">        ...     lr=1e-4,</span>
</span><span id="FTTransformer.make_parameter_groups-954"><a href="#FTTransformer.make_parameter_groups-954"><span class="linenos">954</span></a><span class="sd">        ...     weight_decay=1e-5,</span>
</span><span id="FTTransformer.make_parameter_groups-955"><a href="#FTTransformer.make_parameter_groups-955"><span class="linenos">955</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer.make_parameter_groups-956"><a href="#FTTransformer.make_parameter_groups-956"><span class="linenos">956</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer.make_parameter_groups-957"><a href="#FTTransformer.make_parameter_groups-957"><span class="linenos">957</span></a>        <span class="n">main_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[]}</span>
</span><span id="FTTransformer.make_parameter_groups-958"><a href="#FTTransformer.make_parameter_groups-958"><span class="linenos">958</span></a>        <span class="n">zero_wd_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
</span><span id="FTTransformer.make_parameter_groups-959"><a href="#FTTransformer.make_parameter_groups-959"><span class="linenos">959</span></a>
</span><span id="FTTransformer.make_parameter_groups-960"><a href="#FTTransformer.make_parameter_groups-960"><span class="linenos">960</span></a>        <span class="n">zero_wd_subnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;normalization&#39;</span><span class="p">,</span> <span class="s1">&#39;.bias&#39;</span><span class="p">]</span>
</span><span id="FTTransformer.make_parameter_groups-961"><a href="#FTTransformer.make_parameter_groups-961"><span class="linenos">961</span></a>        <span class="k">for</span> <span class="n">modulename</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;cont_embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;cat_embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;cls_embedding&#39;</span><span class="p">]:</span>
</span><span id="FTTransformer.make_parameter_groups-962"><a href="#FTTransformer.make_parameter_groups-962"><span class="linenos">962</span></a>            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modulename</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformer.make_parameter_groups-963"><a href="#FTTransformer.make_parameter_groups-963"><span class="linenos">963</span></a>                <span class="n">zero_wd_subnames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modulename</span><span class="p">)</span>
</span><span id="FTTransformer.make_parameter_groups-964"><a href="#FTTransformer.make_parameter_groups-964"><span class="linenos">964</span></a>        <span class="c1"># Check that there are no typos in the above list.</span>
</span><span id="FTTransformer.make_parameter_groups-965"><a href="#FTTransformer.make_parameter_groups-965"><span class="linenos">965</span></a>        <span class="k">for</span> <span class="n">subname</span> <span class="ow">in</span> <span class="n">zero_wd_subnames</span><span class="p">:</span>
</span><span id="FTTransformer.make_parameter_groups-966"><a href="#FTTransformer.make_parameter_groups-966"><span class="linenos">966</span></a>            <span class="k">assert</span> <span class="nb">any</span><span class="p">(</span>
</span><span id="FTTransformer.make_parameter_groups-967"><a href="#FTTransformer.make_parameter_groups-967"><span class="linenos">967</span></a>                <span class="n">subname</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
</span><span id="FTTransformer.make_parameter_groups-968"><a href="#FTTransformer.make_parameter_groups-968"><span class="linenos">968</span></a>            <span class="p">),</span> <span class="n">_INTERNAL_ERROR_MESSAGE</span>
</span><span id="FTTransformer.make_parameter_groups-969"><a href="#FTTransformer.make_parameter_groups-969"><span class="linenos">969</span></a>
</span><span id="FTTransformer.make_parameter_groups-970"><a href="#FTTransformer.make_parameter_groups-970"><span class="linenos">970</span></a>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span><span id="FTTransformer.make_parameter_groups-971"><a href="#FTTransformer.make_parameter_groups-971"><span class="linenos">971</span></a>            <span class="n">zero_wd_condition</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">subname</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">subname</span> <span class="ow">in</span> <span class="n">zero_wd_subnames</span><span class="p">)</span>
</span><span id="FTTransformer.make_parameter_groups-972"><a href="#FTTransformer.make_parameter_groups-972"><span class="linenos">972</span></a>            <span class="p">(</span><span class="n">zero_wd_group</span> <span class="k">if</span> <span class="n">zero_wd_condition</span> <span class="k">else</span> <span class="n">main_group</span><span class="p">)[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="FTTransformer.make_parameter_groups-973"><a href="#FTTransformer.make_parameter_groups-973"><span class="linenos">973</span></a>                <span class="n">parameter</span>
</span><span id="FTTransformer.make_parameter_groups-974"><a href="#FTTransformer.make_parameter_groups-974"><span class="linenos">974</span></a>            <span class="p">)</span>
</span><span id="FTTransformer.make_parameter_groups-975"><a href="#FTTransformer.make_parameter_groups-975"><span class="linenos">975</span></a>        <span class="k">return</span> <span class="p">[</span><span class="n">main_group</span><span class="p">,</span> <span class="n">zero_wd_group</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Make parameter groups for optimizers.</p>

<p>The difference with calling this method instead of
<code>.parameters()</code> is that this method always sets <code>weight_decay=0.0</code>
for some of the parameters.</p>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_cont_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">cat_cardinalities</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">d_out</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="o">**</span><span class="n"><a href="#FTTransformer.get_default_kwargs">FTTransformer.get_default_kwargs</a></span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">m</span><span class="o">.</span><span class="n">make_parameter_groups</span><span class="p">(),</span>
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</code></pre>
</div>
</div>


                            </div>
                            <div id="FTTransformer.make_default_optimizer" class="classattr">
                                        <input id="FTTransformer.make_default_optimizer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">make_default_optimizer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">adamw</span><span class="o">.</span><span class="n">AdamW</span>:</span></span>

                <label class="view-source-button" for="FTTransformer.make_default_optimizer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformer.make_default_optimizer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformer.make_default_optimizer-977"><a href="#FTTransformer.make_default_optimizer-977"><span class="linenos"> 977</span></a>    <span class="k">def</span> <span class="nf">make_default_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">:</span>
</span><span id="FTTransformer.make_default_optimizer-978"><a href="#FTTransformer.make_default_optimizer-978"><span class="linenos"> 978</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the &quot;default&quot; `torch.nn.AdamW` suitable for the *default* FT-Transformer.</span>
</span><span id="FTTransformer.make_default_optimizer-979"><a href="#FTTransformer.make_default_optimizer-979"><span class="linenos"> 979</span></a>
</span><span id="FTTransformer.make_default_optimizer-980"><a href="#FTTransformer.make_default_optimizer-980"><span class="linenos"> 980</span></a><span class="sd">        Returns:</span>
</span><span id="FTTransformer.make_default_optimizer-981"><a href="#FTTransformer.make_default_optimizer-981"><span class="linenos"> 981</span></a><span class="sd">            optimizer</span>
</span><span id="FTTransformer.make_default_optimizer-982"><a href="#FTTransformer.make_default_optimizer-982"><span class="linenos"> 982</span></a>
</span><span id="FTTransformer.make_default_optimizer-983"><a href="#FTTransformer.make_default_optimizer-983"><span class="linenos"> 983</span></a><span class="sd">        **Examples**</span>
</span><span id="FTTransformer.make_default_optimizer-984"><a href="#FTTransformer.make_default_optimizer-984"><span class="linenos"> 984</span></a>
</span><span id="FTTransformer.make_default_optimizer-985"><a href="#FTTransformer.make_default_optimizer-985"><span class="linenos"> 985</span></a><span class="sd">        &gt;&gt;&gt; m = FTTransformer(</span>
</span><span id="FTTransformer.make_default_optimizer-986"><a href="#FTTransformer.make_default_optimizer-986"><span class="linenos"> 986</span></a><span class="sd">        ...     n_cont_features=2,</span>
</span><span id="FTTransformer.make_default_optimizer-987"><a href="#FTTransformer.make_default_optimizer-987"><span class="linenos"> 987</span></a><span class="sd">        ...     cat_cardinalities=[3, 4],</span>
</span><span id="FTTransformer.make_default_optimizer-988"><a href="#FTTransformer.make_default_optimizer-988"><span class="linenos"> 988</span></a><span class="sd">        ...     d_out=5,</span>
</span><span id="FTTransformer.make_default_optimizer-989"><a href="#FTTransformer.make_default_optimizer-989"><span class="linenos"> 989</span></a><span class="sd">        ...     **FTTransformer.get_default_kwargs(),</span>
</span><span id="FTTransformer.make_default_optimizer-990"><a href="#FTTransformer.make_default_optimizer-990"><span class="linenos"> 990</span></a><span class="sd">        ... )</span>
</span><span id="FTTransformer.make_default_optimizer-991"><a href="#FTTransformer.make_default_optimizer-991"><span class="linenos"> 991</span></a><span class="sd">        &gt;&gt;&gt; optimizer = m.make_default_optimizer()</span>
</span><span id="FTTransformer.make_default_optimizer-992"><a href="#FTTransformer.make_default_optimizer-992"><span class="linenos"> 992</span></a><span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
</span><span id="FTTransformer.make_default_optimizer-993"><a href="#FTTransformer.make_default_optimizer-993"><span class="linenos"> 993</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default</span><span class="p">:</span>
</span><span id="FTTransformer.make_default_optimizer-994"><a href="#FTTransformer.make_default_optimizer-994"><span class="linenos"> 994</span></a>            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
</span><span id="FTTransformer.make_default_optimizer-995"><a href="#FTTransformer.make_default_optimizer-995"><span class="linenos"> 995</span></a>                <span class="s1">&#39;The default opimizer is supposed to be used in a combination&#39;</span>
</span><span id="FTTransformer.make_default_optimizer-996"><a href="#FTTransformer.make_default_optimizer-996"><span class="linenos"> 996</span></a>                <span class="s1">&#39; with the default FT-Transformer.&#39;</span>
</span><span id="FTTransformer.make_default_optimizer-997"><a href="#FTTransformer.make_default_optimizer-997"><span class="linenos"> 997</span></a>            <span class="p">)</span>
</span><span id="FTTransformer.make_default_optimizer-998"><a href="#FTTransformer.make_default_optimizer-998"><span class="linenos"> 998</span></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
</span><span id="FTTransformer.make_default_optimizer-999"><a href="#FTTransformer.make_default_optimizer-999"><span class="linenos"> 999</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">make_parameter_groups</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span>
</span><span id="FTTransformer.make_default_optimizer-1000"><a href="#FTTransformer.make_default_optimizer-1000"><span class="linenos">1000</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Create the "default" <code>torch.nn.AdamW</code> suitable for the <em>default</em> FT-Transformer.</p>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>optimizer</p>
</blockquote>

<p><strong>Examples</strong></p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_cont_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">cat_cardinalities</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">d_out</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="o">**</span><span class="n"><a href="#FTTransformer.get_default_kwargs">FTTransformer.get_default_kwargs</a></span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">make_default_optimizer</span><span class="p">()</span>
</code></pre>
</div>
</div>


                            </div>
                            <div id="FTTransformer.forward" class="classattr">
                                        <input id="FTTransformer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x_cont</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>,</span><span class="param">	<span class="n">x_cat</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="FTTransformer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FTTransformer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FTTransformer.forward-1006"><a href="#FTTransformer.forward-1006"><span class="linenos">1006</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_cont</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">x_cat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="FTTransformer.forward-1007"><a href="#FTTransformer.forward-1007"><span class="linenos">1007</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Do the forward pass.</span>
</span><span id="FTTransformer.forward-1008"><a href="#FTTransformer.forward-1008"><span class="linenos">1008</span></a>
</span><span id="FTTransformer.forward-1009"><a href="#FTTransformer.forward-1009"><span class="linenos">1009</span></a><span class="sd">        Args:</span>
</span><span id="FTTransformer.forward-1010"><a href="#FTTransformer.forward-1010"><span class="linenos">1010</span></a><span class="sd">            x_cont: the continuous features.</span>
</span><span id="FTTransformer.forward-1011"><a href="#FTTransformer.forward-1011"><span class="linenos">1011</span></a><span class="sd">            x_cat: the categorical features.</span>
</span><span id="FTTransformer.forward-1012"><a href="#FTTransformer.forward-1012"><span class="linenos">1012</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FTTransformer.forward-1013"><a href="#FTTransformer.forward-1013"><span class="linenos">1013</span></a>        <span class="k">assert</span> <span class="n">x_cont</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">x_cat</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="FTTransformer.forward-1014"><a href="#FTTransformer.forward-1014"><span class="linenos">1014</span></a>
</span><span id="FTTransformer.forward-1015"><a href="#FTTransformer.forward-1015"><span class="linenos">1015</span></a>        <span class="n">x_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="FTTransformer.forward-1016"><a href="#FTTransformer.forward-1016"><span class="linenos">1016</span></a>        <span class="k">for</span> <span class="n">argname</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="FTTransformer.forward-1017"><a href="#FTTransformer.forward-1017"><span class="linenos">1017</span></a>            <span class="p">(</span><span class="s1">&#39;x_cont&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_embeddings</span><span class="p">),</span>
</span><span id="FTTransformer.forward-1018"><a href="#FTTransformer.forward-1018"><span class="linenos">1018</span></a>            <span class="p">(</span><span class="s1">&#39;x_cat&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embeddings</span><span class="p">),</span>
</span><span id="FTTransformer.forward-1019"><a href="#FTTransformer.forward-1019"><span class="linenos">1019</span></a>        <span class="p">]:</span>
</span><span id="FTTransformer.forward-1020"><a href="#FTTransformer.forward-1020"><span class="linenos">1020</span></a>            <span class="n">argvalue</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()[</span><span class="n">argname</span><span class="p">]</span>
</span><span id="FTTransformer.forward-1021"><a href="#FTTransformer.forward-1021"><span class="linenos">1021</span></a>            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FTTransformer.forward-1022"><a href="#FTTransformer.forward-1022"><span class="linenos">1022</span></a>                <span class="k">assert</span> <span class="n">argvalue</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">FTTransformer</span><span class="o">.</span><span class="n">_FORWARD_BAD_ARGS_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="FTTransformer.forward-1023"><a href="#FTTransformer.forward-1023"><span class="linenos">1023</span></a>                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">argname</span><span class="si">}</span><span class="s1"> must be None&#39;</span>
</span><span id="FTTransformer.forward-1024"><a href="#FTTransformer.forward-1024"><span class="linenos">1024</span></a>                <span class="p">)</span>
</span><span id="FTTransformer.forward-1025"><a href="#FTTransformer.forward-1025"><span class="linenos">1025</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FTTransformer.forward-1026"><a href="#FTTransformer.forward-1026"><span class="linenos">1026</span></a>                <span class="k">assert</span> <span class="p">(</span>
</span><span id="FTTransformer.forward-1027"><a href="#FTTransformer.forward-1027"><span class="linenos">1027</span></a>                    <span class="n">argvalue</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="FTTransformer.forward-1028"><a href="#FTTransformer.forward-1028"><span class="linenos">1028</span></a>                <span class="p">),</span> <span class="n">FTTransformer</span><span class="o">.</span><span class="n">_FORWARD_BAD_ARGS_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="FTTransformer.forward-1029"><a href="#FTTransformer.forward-1029"><span class="linenos">1029</span></a>                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">argname</span><span class="si">}</span><span class="s1"> must not be None&#39;</span>
</span><span id="FTTransformer.forward-1030"><a href="#FTTransformer.forward-1030"><span class="linenos">1030</span></a>                <span class="p">)</span>
</span><span id="FTTransformer.forward-1031"><a href="#FTTransformer.forward-1031"><span class="linenos">1031</span></a>                <span class="n">x_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="p">(</span><span class="n">argvalue</span><span class="p">))</span>
</span><span id="FTTransformer.forward-1032"><a href="#FTTransformer.forward-1032"><span class="linenos">1032</span></a>        <span class="k">assert</span> <span class="n">x_embeddings</span>
</span><span id="FTTransformer.forward-1033"><a href="#FTTransformer.forward-1033"><span class="linenos">1033</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FTTransformer.forward-1034"><a href="#FTTransformer.forward-1034"><span class="linenos">1034</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="FTTransformer.forward-1035"><a href="#FTTransformer.forward-1035"><span class="linenos">1035</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Do the forward pass.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>x_cont:</strong>  the continuous features.</li>
<li><strong>x_cat:</strong>  the categorical features.</li>
</ul>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="FTTransformer.dump_patches" class="variable">dump_patches</dd>
                <dd id="FTTransformer.training" class="variable">training</dd>
                <dd id="FTTransformer.register_buffer" class="function">register_buffer</dd>
                <dd id="FTTransformer.register_parameter" class="function">register_parameter</dd>
                <dd id="FTTransformer.add_module" class="function">add_module</dd>
                <dd id="FTTransformer.apply" class="function">apply</dd>
                <dd id="FTTransformer.cuda" class="function">cuda</dd>
                <dd id="FTTransformer.xpu" class="function">xpu</dd>
                <dd id="FTTransformer.cpu" class="function">cpu</dd>
                <dd id="FTTransformer.type" class="function">type</dd>
                <dd id="FTTransformer.float" class="function">float</dd>
                <dd id="FTTransformer.double" class="function">double</dd>
                <dd id="FTTransformer.half" class="function">half</dd>
                <dd id="FTTransformer.bfloat16" class="function">bfloat16</dd>
                <dd id="FTTransformer.to" class="function">to</dd>
                <dd id="FTTransformer.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="FTTransformer.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="FTTransformer.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="FTTransformer.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="FTTransformer.state_dict" class="function">state_dict</dd>
                <dd id="FTTransformer.load_state_dict" class="function">load_state_dict</dd>
                <dd id="FTTransformer.parameters" class="function">parameters</dd>
                <dd id="FTTransformer.named_parameters" class="function">named_parameters</dd>
                <dd id="FTTransformer.buffers" class="function">buffers</dd>
                <dd id="FTTransformer.named_buffers" class="function">named_buffers</dd>
                <dd id="FTTransformer.children" class="function">children</dd>
                <dd id="FTTransformer.named_children" class="function">named_children</dd>
                <dd id="FTTransformer.modules" class="function">modules</dd>
                <dd id="FTTransformer.named_modules" class="function">named_modules</dd>
                <dd id="FTTransformer.train" class="function">train</dd>
                <dd id="FTTransformer.eval" class="function">eval</dd>
                <dd id="FTTransformer.requires_grad_" class="function">requires_grad_</dd>
                <dd id="FTTransformer.zero_grad" class="function">zero_grad</dd>
                <dd id="FTTransformer.share_memory" class="function">share_memory</dd>
                <dd id="FTTransformer.extra_repr" class="function">extra_repr</dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
</body>
</html>